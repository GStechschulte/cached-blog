{"title":"Inference - Hamiltonian Monte Carlo from Scratch","markdown":{"yaml":{"aliases":["/probability/sampling/inference/optimization/pytorch/2022/10/14/inference-hmc"],"badges":false,"categories":["probability","sampling","inference","optimization","pytorch"],"date":"2022-10-14","output-file":"2022-10-14-inference-hmc.html","title":"Inference - Hamiltonian Monte Carlo from Scratch","jupyter":"python3","toc":false,"image":"hmc_output.png"},"headingText":"Hamiltonian Mechanics in a Statistical Setting","containsRefs":false,"markdown":"\n\nMany MCMC algorithms perform poorly in high dimensions as they rely on a form of random searches based on local perturbations. Hamiltonian Monte Carlo (HMC), however, leverages gradient information to guide the local moves and propose new states. The gradients of the log probability of the posterior evaluated at some state provides information of the geometry of the posterior density function. HMC attempts to avoid the random walk behavior typical of Metropolis-Hastings by using the gradient to propose new positions far from the current one with high acceptance probability. This allows HMC to scale to higher dimensions and in principle more complex posterior geometries. \n\n\nHMC gets its name from Hamiltonian mechanics. The field of mechanics can be used to describe simple systems such as a bouncing ball, a pendulum or an oscillating spring in which energy changes from kinetic to potential and back again over time. Consider a skateboarder riding in an empty pool. We can characterize the skateboarder in terms of their position $\\theta \\in \\mathbb{R}^D$ denoted $q$ and momentum $v \\in \\mathbb{R}^D$, denoted $p$. The set of all possible values for ($q, p$) the skateboarder can take on is called the **phase space**. The Hamiltonian function is a description of the total energy of a physical system and is defined as:\n\n$$\\mathcal{H}(q, p) = \\mathcal{E}(q) + \\mathcal{K}(p)$$\n\nwhere $\\mathcal{E}(q)$ is the **potential energy**, and $\\mathcal{K}(p)$ is the **kinetic energy**. $\\mathcal{H}(q, p)$ is the **total energy**. However, since we are operating in a statistical, not a physical, setting, the potential energy is a log probability density function (logpdf) such as $p(q, D)$:\n\n$$\\mathcal{E}(q) = -log\\tilde{p}(q)$$\n\nand kinetic energy is:\n\n$$\\mathcal{K} = \\frac{1}{2}p^T\\sum^{-1}p$$\n\nwhere $\\sum$ is a positive-definite matrix, known as the inverse mass matrix. Why is kinetic energy this way? We are free to choose the kinetic energy, and if we choose it to be Gaussian, and drop the normalization constant, we get the $\\mathcal{K}$ above.\n\nTo run the physics simulation, one must solve the continuous time differential equations, known as Hamilton's equations:\n\n$$\\frac{dq}{dt} = \\frac{\\partial \\mathcal{H}}{\\partial p} = \\frac{\\partial \\mathcal{K}}{\\partial p}$$\n\n$$\\frac{dp}{dt} = -\\frac{\\partial \\mathcal{H}}{\\partial q} = -\\frac{\\partial \\mathcal{E}}{\\partial q}$$\n\n#### Conservation of Energy\n\nSince we are running a physics simulation, total energy must be conserved. Intuitively, a satellite in orbit around a planet will \"want\"  to continue in a straight line due to its momentum, but will get pulled in towards the planet due to gravity, and if these forces cancel out, the orbit is stable. If the satellite beings spiraling towards the planet, its kinetic energy will increase but its potential energy will decrease. In our statistical setting, if total energy is not conserved, then this means there were _divergences_ and our numerical approximation went bad. \n\n### Leapfrog Integrator\n\nTo simulate the differential equations above, we must first discretize $t$, and go back and forth updating $q$ and $p$. However, this \"back and forth\" is not so straightforward. It turns out, one way to simulate Hamiltonian dynamics, is with a method called the **leapfrog integrator**. \n\nThis integrator first performs a half update of the momentum $p$, then a full update of the position $q$, and then finally another half update of the momentum. \n\n$$\\underbrace{v_{t+1/2} = v_t - \\frac{\\eta}{2} \\frac{\\partial \\mathcal{E}(q_t)}{\\partial q}}_{\\text{half update}}$$\n\n$$\\underbrace{q_{t+1} = q_t + \\eta \\frac{\\partial \\mathcal{K}(p_{t+1/2})}{\\partial p}}_{\\text{full update}}$$\n\n$$\\underbrace{v_{t+1} = v_{t+1/2} - \\frac{\\eta}{2} \\frac{\\partial \\mathcal{E}(q_{t+1})}{\\partial q}}_{\\text{half update}}$$\n\n\nThe leapfrog integrator has two important parameters: (1) path length, and (2) step size $\\eta$. In the simululation, the path length represents how \"long\" you travel before collecting another sample. The step size indicates the size each step is in the path length and determines how fine grained the simulation is. For example, in the drawing below, imagine path length $=1$ for both simulations. However, the left simulation has a step size $=4$ whereas the right simulation has a step size $=2$. These parameters are important in determining how the simulator collects samples of the geometry of the posterior.\n\n![leapfrog](leapfrog_params.png)\n\n### Main Idea\n\nHMC says the log posterior is like a \"bowl\" (the empty pool in the figure below), with the highest posterior probability at the center of the valley. If we give the skateboarder a flick, this momentum will simulate its path. It must obey physics, gliding along until we stop the clock and take a sample. If the log posterior is flat, then not much information is in the likelihood & prior and the skateboarder will glide before the gradient makes it turn around. However, if the geometry of the log posterior is not flat, like the pool below, the gradient will make the skateboarder turn around and back into the valley. Since HMC runs a physics simulation, certain things must be conserved, i.e., the total energy of the system. \n\n![skateboarder](skateboarder.jpeg)\n\nHMC needs a few things to run:\n1. A function or callable that returns the negative log probability of the data at the current position $q$\n2. A means of returning the gradient of the negative log probability at the current position $q$\n3. An integrator for simulating the Hamiltonian equations in discrete time with two parameters:\n    - step size \n    - path length\n\nThe algorithm for running HMC is to: set the initial position $q$ to $q'_0 = q_{t-1}$, and sample a new random momentum $p' \\sim \\mathcal{N}(0, \\sum)$. Then, initialize a random trajectory in the phase space, starting at ($q'_0, p'_0$), followed for $L$ leapfrog steps until we get to the new proposed state ($q^*, p^*$) $=$ ($q'_L, p'_L$). With the new proposed state, we compute the MH acceptance probability. This process is ran $n$ times, according the number of samples the user wants to collect.\n\n### Hamiltonian Monte Carlo - Multivariate Normal\n\nThe HMC implementation below is heavily inspired by Colin Carroll's [implementation](https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/). I recommend reading his blog plosts on HMC for an in-depth, but intuitive, explanation.\n","srcMarkdownNoYaml":"\n\nMany MCMC algorithms perform poorly in high dimensions as they rely on a form of random searches based on local perturbations. Hamiltonian Monte Carlo (HMC), however, leverages gradient information to guide the local moves and propose new states. The gradients of the log probability of the posterior evaluated at some state provides information of the geometry of the posterior density function. HMC attempts to avoid the random walk behavior typical of Metropolis-Hastings by using the gradient to propose new positions far from the current one with high acceptance probability. This allows HMC to scale to higher dimensions and in principle more complex posterior geometries. \n\n### Hamiltonian Mechanics in a Statistical Setting\n\nHMC gets its name from Hamiltonian mechanics. The field of mechanics can be used to describe simple systems such as a bouncing ball, a pendulum or an oscillating spring in which energy changes from kinetic to potential and back again over time. Consider a skateboarder riding in an empty pool. We can characterize the skateboarder in terms of their position $\\theta \\in \\mathbb{R}^D$ denoted $q$ and momentum $v \\in \\mathbb{R}^D$, denoted $p$. The set of all possible values for ($q, p$) the skateboarder can take on is called the **phase space**. The Hamiltonian function is a description of the total energy of a physical system and is defined as:\n\n$$\\mathcal{H}(q, p) = \\mathcal{E}(q) + \\mathcal{K}(p)$$\n\nwhere $\\mathcal{E}(q)$ is the **potential energy**, and $\\mathcal{K}(p)$ is the **kinetic energy**. $\\mathcal{H}(q, p)$ is the **total energy**. However, since we are operating in a statistical, not a physical, setting, the potential energy is a log probability density function (logpdf) such as $p(q, D)$:\n\n$$\\mathcal{E}(q) = -log\\tilde{p}(q)$$\n\nand kinetic energy is:\n\n$$\\mathcal{K} = \\frac{1}{2}p^T\\sum^{-1}p$$\n\nwhere $\\sum$ is a positive-definite matrix, known as the inverse mass matrix. Why is kinetic energy this way? We are free to choose the kinetic energy, and if we choose it to be Gaussian, and drop the normalization constant, we get the $\\mathcal{K}$ above.\n\nTo run the physics simulation, one must solve the continuous time differential equations, known as Hamilton's equations:\n\n$$\\frac{dq}{dt} = \\frac{\\partial \\mathcal{H}}{\\partial p} = \\frac{\\partial \\mathcal{K}}{\\partial p}$$\n\n$$\\frac{dp}{dt} = -\\frac{\\partial \\mathcal{H}}{\\partial q} = -\\frac{\\partial \\mathcal{E}}{\\partial q}$$\n\n#### Conservation of Energy\n\nSince we are running a physics simulation, total energy must be conserved. Intuitively, a satellite in orbit around a planet will \"want\"  to continue in a straight line due to its momentum, but will get pulled in towards the planet due to gravity, and if these forces cancel out, the orbit is stable. If the satellite beings spiraling towards the planet, its kinetic energy will increase but its potential energy will decrease. In our statistical setting, if total energy is not conserved, then this means there were _divergences_ and our numerical approximation went bad. \n\n### Leapfrog Integrator\n\nTo simulate the differential equations above, we must first discretize $t$, and go back and forth updating $q$ and $p$. However, this \"back and forth\" is not so straightforward. It turns out, one way to simulate Hamiltonian dynamics, is with a method called the **leapfrog integrator**. \n\nThis integrator first performs a half update of the momentum $p$, then a full update of the position $q$, and then finally another half update of the momentum. \n\n$$\\underbrace{v_{t+1/2} = v_t - \\frac{\\eta}{2} \\frac{\\partial \\mathcal{E}(q_t)}{\\partial q}}_{\\text{half update}}$$\n\n$$\\underbrace{q_{t+1} = q_t + \\eta \\frac{\\partial \\mathcal{K}(p_{t+1/2})}{\\partial p}}_{\\text{full update}}$$\n\n$$\\underbrace{v_{t+1} = v_{t+1/2} - \\frac{\\eta}{2} \\frac{\\partial \\mathcal{E}(q_{t+1})}{\\partial q}}_{\\text{half update}}$$\n\n\nThe leapfrog integrator has two important parameters: (1) path length, and (2) step size $\\eta$. In the simululation, the path length represents how \"long\" you travel before collecting another sample. The step size indicates the size each step is in the path length and determines how fine grained the simulation is. For example, in the drawing below, imagine path length $=1$ for both simulations. However, the left simulation has a step size $=4$ whereas the right simulation has a step size $=2$. These parameters are important in determining how the simulator collects samples of the geometry of the posterior.\n\n![leapfrog](leapfrog_params.png)\n\n### Main Idea\n\nHMC says the log posterior is like a \"bowl\" (the empty pool in the figure below), with the highest posterior probability at the center of the valley. If we give the skateboarder a flick, this momentum will simulate its path. It must obey physics, gliding along until we stop the clock and take a sample. If the log posterior is flat, then not much information is in the likelihood & prior and the skateboarder will glide before the gradient makes it turn around. However, if the geometry of the log posterior is not flat, like the pool below, the gradient will make the skateboarder turn around and back into the valley. Since HMC runs a physics simulation, certain things must be conserved, i.e., the total energy of the system. \n\n![skateboarder](skateboarder.jpeg)\n\nHMC needs a few things to run:\n1. A function or callable that returns the negative log probability of the data at the current position $q$\n2. A means of returning the gradient of the negative log probability at the current position $q$\n3. An integrator for simulating the Hamiltonian equations in discrete time with two parameters:\n    - step size \n    - path length\n\nThe algorithm for running HMC is to: set the initial position $q$ to $q'_0 = q_{t-1}$, and sample a new random momentum $p' \\sim \\mathcal{N}(0, \\sum)$. Then, initialize a random trajectory in the phase space, starting at ($q'_0, p'_0$), followed for $L$ leapfrog steps until we get to the new proposed state ($q^*, p^*$) $=$ ($q'_L, p'_L$). With the new proposed state, we compute the MH acceptance probability. This process is ran $n$ times, according the number of samples the user wants to collect.\n\n### Hamiltonian Monte Carlo - Multivariate Normal\n\nThe HMC implementation below is heavily inspired by Colin Carroll's [implementation](https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/). I recommend reading his blog plosts on HMC for an in-depth, but intuitive, explanation.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2022-10-14-inference-hmc.html","toc":false},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.306","theme":"cosmo","title-block-banner":true,"aliases":["/probability/sampling/inference/optimization/pytorch/2022/10/14/inference-hmc"],"badges":false,"categories":["probability","sampling","inference","optimization","pytorch"],"date":"2022-10-14","title":"Inference - Hamiltonian Monte Carlo from Scratch","jupyter":"python3","image":"hmc_output.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}