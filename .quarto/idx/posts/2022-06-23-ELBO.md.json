{"title":"Variational Inference - ELBO","markdown":{"yaml":{"aliases":["/probability/optimization/2022/06/23/ELBO"],"categories":["probability","optimization"],"date":"2022-06-23","layout":"post","title":"Variational Inference - ELBO"},"containsRefs":false,"markdown":"\n\nWe don't know the real posterior so we are going to choose a distribution $Q(\\theta)$ from a family of distributions $Q^*$ that are **easy to work with** and parameterized by $\\theta$. The approximate distribution should be *as close as possible* to the true posterior. This closeness is measured using KL-Divergence. If we have the joint $p(x, z)$ where $x$ is some observed data, the goal is to perform inference: given what we have observed, what can we infer about the latent states?, i.e , we want the posterior. \n\nRecall Bayes theorem:\n\n$$p(z | x) = \\frac{p(x|z)p(z)}{p(x)}$$\n\nThe problem is the marginal $p(x = D)$ as this could require a hundred, thousand, . . .dimensional integral:\n\n$$p(x) = \\int_{z_0},...,\\int_{z_{D-1}}p(x, z)dz_0,...,d_{z_D{-1}}$$\n\nIf we want the full posterior and can't compute the marginal, then what's the solution? **Surrogate posterior**. We want to approximate the true posterior using some known distribution: \n\n$$q(z) \\approx p(z|X=D)$$\n\nwhere $\\approx$ can mean you want the approximated posterior to be \"as good as possible\". Using variational inference, the objective is to minimize the distance between the surrogate $q(z)$ and the true posterior $p(x)$ using KL-Divergence:\n\n$$q^*(z) = argmin_{q(z) \\in Q} (KL(q(z) || p(z|x=D)))$$\n\nwhere $Q$ is a more \"simple\" distribution. We can restate the KL-divergence as the expectation:\n\n$$KL(q(z) || p(z|D)) = \\mathbb{E_{z \\sim q(z)}}[log \\frac{q(z)}{p(z|D)}]$$\n\nwhich, taking the expectation over $z$, is equivalent to integration:\n\n$$\\int_{z_0}, . . .,\\int_{z_{D-1}}q(z)log\\frac{q(z)}{p(z|D)}d_{z_0},...,d_{z_{D-1}}$$\n\nBut, sadly we don't have $p(z \\vert D)$ as this is the posterior! We only have the joint. Solution? Recall our KL-divergence:\n\n$$KL(q(z) || p(z|D))$$\n\nWe can rearrange the terms inside the $log$ so that we can **actually** compute something:\n\n$$\\int_{z}q(z)log(\\frac{q(z)p(D)}{p(z, D)})dz$$\n\nWe only have the **joint**. Not the posterior; nor the marginal. We know from Bayes rule that we can express the posterior in terms of the joint $p(z, D)$ divided by the marginal $p(x=D)$:\n\n$$p(z|D) = \\frac{p(Z, D)}{p(D)}$$\n\nWe plug this inside of the $log$:\n\n$$\\int_{z}q(z)log(\\frac{q(z)p(D)}{p(z, D)})dz$$\n\nHowever, the problem now is that we have reformulated our problem into **another** quantity that we don't have, i.e., the marginal $p(D)$. But we can put the quantity that we don't have outside of the $log$ to form two separate integrals.\n\n$$\\int_z q(z)log(\\frac{q(z)}{p(z, D)})dz + \\int_zq(z)log(p(D)dz$$\n\nThis is a valid _rearrangement_ because of the properties of logarithms. In this case, the numerator is a product, so this turns into a sum of the second integral. What do we see in these two terms? We see an expectation over the quantity $\\frac{q(z)}{p(z, D)}$ and another expectation over $p(D)$. Rewriting in terms of expectation:\n\n$$\\mathbb{E_{z{\\sim q(z)}}}[log(\\frac{q(z)}{p(z, D)})] + \\mathbb{E_{z \\sim q(z)}}[log(p(D))]$$\n\nThe right term contains **information we know**—the functional form of the surrogate $q(z)$ and the joint $p(z, D)$ (in the form of a directed graphical model). We still don't have access to $p(D)$ on the right side, but this is a **constant quantity**. The expectation of a quantity that does not contain $z$ is just whatever the expectation was taken over. Because of this, we can again rearrange:\n\n$$-\\mathbb{E_{z \\sim q(z)}}[log \\frac{p(z, D)}{q(z)}]+log (p(D))$$\n\nThe minus sign is a result of the \"swapping\" of the numerator and denominator and is required to make it a _valid_ change. Looking at this, the left side is a function dependent on $q$. In shorthand form, we can call this $\\mathcal{L(q)}$. Our KL-divergence is:\n\n$$KL = \\mathcal{-L(q)} + \\underbrace{log(p(D))}_\\textrm{evidence}$$\n\nwhere $p(D)$ is a value between $[0, 1]$ and this value is called the **evidence** which is the **log probability of the data**. If we apply the $log$ to something between $[0, 1]$ then this value will be negative. This value is also **constant** since we have observed the dataset and thus does not change. \n\n$KL$ is the **distance** (between the posterior and the surrogate) so it must be something positive. If the $KL$ is positive and the evidence is negative, then in order to fulfill this equation, $\\mathcal{L}$ must also be negative (negative times a negative is a positive). The $\\mathcal{L}$ should be *smaller* than the evidence, and thus it is called the **lower bound** of the evidence $\\rightarrow$ **Evidence Lower Bound** (ELBO). \n\nAgain, ELBO is defined as: $\\mathcal{L} = \\mathbb{E_{z \\sim q(z)}}[log(\\frac{p(z, D)}{q(z)})]$ and is important to note that the ELBO is equal to the evidence if and only if the KL-divergence between the surrogate and the true posterior is $0$:\n\n$$\\mathcal{L(q)} = log(p(D)) \\textrm{ i.f.f. } KL(q(z)||p(z|D))=0$$ \n","srcMarkdownNoYaml":"\n\nWe don't know the real posterior so we are going to choose a distribution $Q(\\theta)$ from a family of distributions $Q^*$ that are **easy to work with** and parameterized by $\\theta$. The approximate distribution should be *as close as possible* to the true posterior. This closeness is measured using KL-Divergence. If we have the joint $p(x, z)$ where $x$ is some observed data, the goal is to perform inference: given what we have observed, what can we infer about the latent states?, i.e , we want the posterior. \n\nRecall Bayes theorem:\n\n$$p(z | x) = \\frac{p(x|z)p(z)}{p(x)}$$\n\nThe problem is the marginal $p(x = D)$ as this could require a hundred, thousand, . . .dimensional integral:\n\n$$p(x) = \\int_{z_0},...,\\int_{z_{D-1}}p(x, z)dz_0,...,d_{z_D{-1}}$$\n\nIf we want the full posterior and can't compute the marginal, then what's the solution? **Surrogate posterior**. We want to approximate the true posterior using some known distribution: \n\n$$q(z) \\approx p(z|X=D)$$\n\nwhere $\\approx$ can mean you want the approximated posterior to be \"as good as possible\". Using variational inference, the objective is to minimize the distance between the surrogate $q(z)$ and the true posterior $p(x)$ using KL-Divergence:\n\n$$q^*(z) = argmin_{q(z) \\in Q} (KL(q(z) || p(z|x=D)))$$\n\nwhere $Q$ is a more \"simple\" distribution. We can restate the KL-divergence as the expectation:\n\n$$KL(q(z) || p(z|D)) = \\mathbb{E_{z \\sim q(z)}}[log \\frac{q(z)}{p(z|D)}]$$\n\nwhich, taking the expectation over $z$, is equivalent to integration:\n\n$$\\int_{z_0}, . . .,\\int_{z_{D-1}}q(z)log\\frac{q(z)}{p(z|D)}d_{z_0},...,d_{z_{D-1}}$$\n\nBut, sadly we don't have $p(z \\vert D)$ as this is the posterior! We only have the joint. Solution? Recall our KL-divergence:\n\n$$KL(q(z) || p(z|D))$$\n\nWe can rearrange the terms inside the $log$ so that we can **actually** compute something:\n\n$$\\int_{z}q(z)log(\\frac{q(z)p(D)}{p(z, D)})dz$$\n\nWe only have the **joint**. Not the posterior; nor the marginal. We know from Bayes rule that we can express the posterior in terms of the joint $p(z, D)$ divided by the marginal $p(x=D)$:\n\n$$p(z|D) = \\frac{p(Z, D)}{p(D)}$$\n\nWe plug this inside of the $log$:\n\n$$\\int_{z}q(z)log(\\frac{q(z)p(D)}{p(z, D)})dz$$\n\nHowever, the problem now is that we have reformulated our problem into **another** quantity that we don't have, i.e., the marginal $p(D)$. But we can put the quantity that we don't have outside of the $log$ to form two separate integrals.\n\n$$\\int_z q(z)log(\\frac{q(z)}{p(z, D)})dz + \\int_zq(z)log(p(D)dz$$\n\nThis is a valid _rearrangement_ because of the properties of logarithms. In this case, the numerator is a product, so this turns into a sum of the second integral. What do we see in these two terms? We see an expectation over the quantity $\\frac{q(z)}{p(z, D)}$ and another expectation over $p(D)$. Rewriting in terms of expectation:\n\n$$\\mathbb{E_{z{\\sim q(z)}}}[log(\\frac{q(z)}{p(z, D)})] + \\mathbb{E_{z \\sim q(z)}}[log(p(D))]$$\n\nThe right term contains **information we know**—the functional form of the surrogate $q(z)$ and the joint $p(z, D)$ (in the form of a directed graphical model). We still don't have access to $p(D)$ on the right side, but this is a **constant quantity**. The expectation of a quantity that does not contain $z$ is just whatever the expectation was taken over. Because of this, we can again rearrange:\n\n$$-\\mathbb{E_{z \\sim q(z)}}[log \\frac{p(z, D)}{q(z)}]+log (p(D))$$\n\nThe minus sign is a result of the \"swapping\" of the numerator and denominator and is required to make it a _valid_ change. Looking at this, the left side is a function dependent on $q$. In shorthand form, we can call this $\\mathcal{L(q)}$. Our KL-divergence is:\n\n$$KL = \\mathcal{-L(q)} + \\underbrace{log(p(D))}_\\textrm{evidence}$$\n\nwhere $p(D)$ is a value between $[0, 1]$ and this value is called the **evidence** which is the **log probability of the data**. If we apply the $log$ to something between $[0, 1]$ then this value will be negative. This value is also **constant** since we have observed the dataset and thus does not change. \n\n$KL$ is the **distance** (between the posterior and the surrogate) so it must be something positive. If the $KL$ is positive and the evidence is negative, then in order to fulfill this equation, $\\mathcal{L}$ must also be negative (negative times a negative is a positive). The $\\mathcal{L}$ should be *smaller* than the evidence, and thus it is called the **lower bound** of the evidence $\\rightarrow$ **Evidence Lower Bound** (ELBO). \n\nAgain, ELBO is defined as: $\\mathcal{L} = \\mathbb{E_{z \\sim q(z)}}[log(\\frac{p(z, D)}{q(z)})]$ and is important to note that the ELBO is equal to the evidence if and only if the KL-divergence between the surrogate and the true posterior is $0$:\n\n$$\\mathcal{L(q)} = log(p(D)) \\textrm{ i.f.f. } KL(q(z)||p(z|D))=0$$ \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2022-06-23-ELBO.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.306","theme":"cosmo","title-block-banner":true,"aliases":["/probability/optimization/2022/06/23/ELBO"],"categories":["probability","optimization"],"date":"2022-06-23","layout":"post","title":"Variational Inference - ELBO"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}