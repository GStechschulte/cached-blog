{"title":"Inference - Gibbs Sampling from Scratch","markdown":{"yaml":{"aliases":["/probability/sampling/inference/pytorch/2022/10/12/inference-gibbs"],"badges":false,"categories":["probability","sampling","inference","pytorch"],"date":"2022-10-12","output-file":"2022-10-12-inference-gibbs.html","title":"Inference - Gibbs Sampling from Scratch","toc":false},"headingText":"Main Idea","containsRefs":false,"markdown":"\n\n\n\nA variant of the Metropolis-Hastings (MH) algorithm that uses clever proposals and is therefore more efficient (you can get a good approximate of the posterior with far fewer samples) is Gibbs sampling. A problem with MH is the need to choose the proposal distribution, and the fact that the acceptance rate may be low.\n\nThe improvement arises from adaptive proposals in which the distribution of proposed parameter values adjusts itself intelligently, **depending upon the parameter values at the moment**. This dependence upon the parameters at that moment is an exploitation of conditional independence properties of a graphical model to automatically create a good proposal, with acceptance probability equal to one.\n\n\nSuppose we have a 3-dimensional joint distribution. Estimating this joint distribution is much harder than a 1-dimensional distribution. Subsequently, sampling is also harder in $\\mathbb{R}^3$. In Gibbs sampling, you condition each variable on the values of all the other variables in the distribution. For example, if we have $D=3$ variables:\n\n$$x_{1}^{s+1} \\sim p(x_1 | x_2^s,x_3^s)$$\n\n$$x_{2}^{s+1} \\sim p(x_2 | x_1^{s+1},x_3^s)$$\n\n$$x_{3}^{s+1} \\sim p(x_3 | x_1^{s+1},x_2^{s+1})$$\n\nwhere $x_1, x_2,...,x_n$ are variable $1, 2,...,n$, respectively. By conditioning on the values of the other variables, sampling from the conditional distribution is much easier than the joint. Because of the exploitation of conditional independence properties of graphical models, the Gibbs algorithm can readily generalize to $D$ variables. \n\n### Gibbs Sampling - Bayesian Gaussian Mixture Model\n\nHere, the Gibbs sampling algorithm is implemented in PyTorch for a 2-dimensional Bayesian Gaussian Mixture Model (GMM). The Bayesian GMM is given by:\n\n$$p(z = k, x | \\theta) = \\pi_k \\mathcal{N}(x|\\mu_k, \\sum_k)$$\n\nwhere the parameters $\\theta$ are known and implemented using PyTorch's `MixtureSameFamily` distribution class. This class implements a batch of mixture distributions where all components are from different parameterizations of the same distribution type (Normal distributions in this example). It is then parameterized by a `Categorical` distribution over $k$ components. \n\nFor a GMM, the full conditional distributions are:\n\n$$p(x|z = k, \\theta) = \\mathcal{N}(x|\\mu_k, \\sum_k)$$\n\nThis conditional distribution reads; \"the probability of data $x$ given component $k$ parameterized by $\\theta$ is distributed according to a Normal distribution with a mean vector $\\mu$ and covariance $\\sum$ according to component $k$\".  \n\n$$p(z = k | x) = \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\sum_k)}{\\sum_{k'} \\mathcal{N}(x|\\mu_{k'}, \\sum_{k'})}$$\n\nThis conditional distribution is given by Bayes rule and reads; \"the probability of component $k$ given we observe some data $x$ is equal to the prior probability $\\pi$ of component $k$ times the likelihood of data $x$ being distributed according to a Normal distribution with a mean vector $\\mu$ and covariance $\\sum$ according to component $k$\" over the total probability of components $k$. \n\nWith the conditional distributions defined, the Gibbs sampling algorithm can be implemented. Below is the full code to reproduce the results and each main step is outlined below the code block.\n\n### Explanation of Code\n\nThe main steps to implement the Gibbs sampling algorithm:\n\n1.  The `main()` function defines the initial values for the sample, component, and mixture distribution. \n\n2. The `gibbs_sampler()` function first sets the values of `x_current` and `z_current` for current data point $x$ and component $z$, respectively. To analyze the trace history of the sampler, `x_samples` and `z_samples` are empty lists. \n\n3. In the for loop, the conditional $p(z = k \\vert x)$ is computed using `x_current`, i.e., given we have observed datum $x$, what is the log probability of component $Z = k$. \n\n4. `probs_z` is then multiplied by the prior probability of the mixture components. \n\n5. Then, the denominator for the conditional $p(z = k \\vert x)$ is computed by dividing `probs_z` by the total probability.\n\n6. Since there are only two components in this GMM, we use logic to determine which component is _most likely_ with the `x_current`. As zero-based indexing is used, the components are $k = 0, 1$. Therefore, if the probability of $k=1 > k=0$, then use index$=1$, else index$=0$. \n\n7. `z_current` defines the component using zero-based indexing. Thus, indexing `mu` and `scale` by the current, most likely component, new samples are drawn according to the conditional Normal distribution.\n\n8. Data and component samples are appended for analyzing the trace history.\n\n### Limitations\n\nAlthough Gibbs sampling can generalize to $D$ variables, the algorithm becomes inefficient as it tends to get stuck in small regions of the posterior for, potentially, a long number of iterations. This isn't because of the large number of variables, but rather, because models with many parameters tend to have regions of high correlation in the posterior. High correlation between parameters means a narrow ridge of probability combinations, resulting in the sampler getting \"stuck\". \n","srcMarkdownNoYaml":"\n\n\n\nA variant of the Metropolis-Hastings (MH) algorithm that uses clever proposals and is therefore more efficient (you can get a good approximate of the posterior with far fewer samples) is Gibbs sampling. A problem with MH is the need to choose the proposal distribution, and the fact that the acceptance rate may be low.\n\nThe improvement arises from adaptive proposals in which the distribution of proposed parameter values adjusts itself intelligently, **depending upon the parameter values at the moment**. This dependence upon the parameters at that moment is an exploitation of conditional independence properties of a graphical model to automatically create a good proposal, with acceptance probability equal to one.\n\n### Main Idea\n\nSuppose we have a 3-dimensional joint distribution. Estimating this joint distribution is much harder than a 1-dimensional distribution. Subsequently, sampling is also harder in $\\mathbb{R}^3$. In Gibbs sampling, you condition each variable on the values of all the other variables in the distribution. For example, if we have $D=3$ variables:\n\n$$x_{1}^{s+1} \\sim p(x_1 | x_2^s,x_3^s)$$\n\n$$x_{2}^{s+1} \\sim p(x_2 | x_1^{s+1},x_3^s)$$\n\n$$x_{3}^{s+1} \\sim p(x_3 | x_1^{s+1},x_2^{s+1})$$\n\nwhere $x_1, x_2,...,x_n$ are variable $1, 2,...,n$, respectively. By conditioning on the values of the other variables, sampling from the conditional distribution is much easier than the joint. Because of the exploitation of conditional independence properties of graphical models, the Gibbs algorithm can readily generalize to $D$ variables. \n\n### Gibbs Sampling - Bayesian Gaussian Mixture Model\n\nHere, the Gibbs sampling algorithm is implemented in PyTorch for a 2-dimensional Bayesian Gaussian Mixture Model (GMM). The Bayesian GMM is given by:\n\n$$p(z = k, x | \\theta) = \\pi_k \\mathcal{N}(x|\\mu_k, \\sum_k)$$\n\nwhere the parameters $\\theta$ are known and implemented using PyTorch's `MixtureSameFamily` distribution class. This class implements a batch of mixture distributions where all components are from different parameterizations of the same distribution type (Normal distributions in this example). It is then parameterized by a `Categorical` distribution over $k$ components. \n\nFor a GMM, the full conditional distributions are:\n\n$$p(x|z = k, \\theta) = \\mathcal{N}(x|\\mu_k, \\sum_k)$$\n\nThis conditional distribution reads; \"the probability of data $x$ given component $k$ parameterized by $\\theta$ is distributed according to a Normal distribution with a mean vector $\\mu$ and covariance $\\sum$ according to component $k$\".  \n\n$$p(z = k | x) = \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\sum_k)}{\\sum_{k'} \\mathcal{N}(x|\\mu_{k'}, \\sum_{k'})}$$\n\nThis conditional distribution is given by Bayes rule and reads; \"the probability of component $k$ given we observe some data $x$ is equal to the prior probability $\\pi$ of component $k$ times the likelihood of data $x$ being distributed according to a Normal distribution with a mean vector $\\mu$ and covariance $\\sum$ according to component $k$\" over the total probability of components $k$. \n\nWith the conditional distributions defined, the Gibbs sampling algorithm can be implemented. Below is the full code to reproduce the results and each main step is outlined below the code block.\n\n### Explanation of Code\n\nThe main steps to implement the Gibbs sampling algorithm:\n\n1.  The `main()` function defines the initial values for the sample, component, and mixture distribution. \n\n2. The `gibbs_sampler()` function first sets the values of `x_current` and `z_current` for current data point $x$ and component $z$, respectively. To analyze the trace history of the sampler, `x_samples` and `z_samples` are empty lists. \n\n3. In the for loop, the conditional $p(z = k \\vert x)$ is computed using `x_current`, i.e., given we have observed datum $x$, what is the log probability of component $Z = k$. \n\n4. `probs_z` is then multiplied by the prior probability of the mixture components. \n\n5. Then, the denominator for the conditional $p(z = k \\vert x)$ is computed by dividing `probs_z` by the total probability.\n\n6. Since there are only two components in this GMM, we use logic to determine which component is _most likely_ with the `x_current`. As zero-based indexing is used, the components are $k = 0, 1$. Therefore, if the probability of $k=1 > k=0$, then use index$=1$, else index$=0$. \n\n7. `z_current` defines the component using zero-based indexing. Thus, indexing `mu` and `scale` by the current, most likely component, new samples are drawn according to the conditional Normal distribution.\n\n8. Data and component samples are appended for analyzing the trace history.\n\n### Limitations\n\nAlthough Gibbs sampling can generalize to $D$ variables, the algorithm becomes inefficient as it tends to get stuck in small regions of the posterior for, potentially, a long number of iterations. This isn't because of the large number of variables, but rather, because models with many parameters tend to have regions of high correlation in the posterior. High correlation between parameters means a narrow ridge of probability combinations, resulting in the sampler getting \"stuck\". \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2022-10-12-inference-gibbs.html","toc":false},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.306","theme":"cosmo","title-block-banner":true,"aliases":["/probability/sampling/inference/pytorch/2022/10/12/inference-gibbs"],"badges":false,"categories":["probability","sampling","inference","pytorch"],"date":"2022-10-12","title":"Inference - Gibbs Sampling from Scratch"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}