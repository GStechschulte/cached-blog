[
  {
    "objectID": "posts/2022-10-14-inference-hmc.html",
    "href": "posts/2022-10-14-inference-hmc.html",
    "title": "Inference - Hamiltonian Monte Carlo from Scratch",
    "section": "",
    "text": "Many MCMC algorithms perform poorly in high dimensions as they rely on a form of random searches based on local perturbations. Hamiltonian Monte Carlo (HMC), however, leverages gradient information to guide the local moves and propose new states. The gradients of the log probability of the posterior evaluated at some state provides information of the geometry of the posterior density function. HMC attempts to avoid the random walk behavior typical of Metropolis-Hastings by using the gradient to propose new positions far from the current one with high acceptance probability. This allows HMC to scale to higher dimensions and in principle more complex posterior geometries.\n\nHamiltonian Mechanics in a Statistical Setting\nHMC gets its name from Hamiltonian mechanis. This field of mechanics can be used to describe simple systems such as a bouncing ball, a pendulum or an oscillating spring in which energy changes from kinetic to potential and back again over time. Consider a skateboarder riding in an empty pool. We can characterize the skateboarder in terms of their position \\(\\theta \\in \\mathbb{R}^D\\) denoted \\(q\\) and momentum \\(v \\in \\mathbb{R}^D\\), denoted \\(p\\). The set of all possible values for (\\(q, p\\)) the skateboarder can take on is called the phase space. The Hamiltonian function is a description of the total energy of a physical system and is defined as:\n\\[\\mathcal{H}(q, p) = \\mathcal{E}(q) + \\mathcal{K}(p)\\]\nwhere \\(\\mathcal{E}(q)\\) is the potential energy, and \\(\\mathcal{K}(p)\\) is the kinetic energy. \\(\\mathcal{H}(q, p)\\) is the total energy. However, since we are operating in a statistical, not a physical, setting, the potential energy is a log probability density function (logpdf) such as \\(p(q, D)\\):\n\\[\\mathcal{E}(q) = -log\\tilde{p}(q)\\]\nand kinetic energy is:\n\\[\\mathcal{K} = \\frac{1}{2}p^T\\sum^{-1}p\\]\nwhere \\(\\sum\\) is a positive-definite matrix, known as the inverse mass matrix. Why is kinetic energy this way? We are free to choose the kinetic energy, and if we choose it to be Gaussian, and drop the normalization constant, we get the \\(\\mathcal{K}\\) above.\nTo run the physics simulation, one must solve the continuous time differential equations, known as Hamilton’s equations:\n\\[\\frac{dq}{dt} = \\frac{\\partial \\mathcal{H}}{\\partial p} = \\frac{\\partial \\mathcal{K}}{\\partial p}\\]\n\\[\\frac{dp}{dt} = -\\frac{\\partial \\mathcal{H}}{\\partial q} = -\\frac{\\partial \\mathcal{E}}{\\partial q}\\]\n\nConservation of Energy\nSince we are running a physics simulation, total energy must be conserved. Intuitively, a satellite in orbit around a planet will “want” to continue in a straight line due to its momentum, but will get pulled in towards the planet due to gravity, and if these forces cancel out, the orbit is stable. If the satellite beings spiraling towards the planet, its kinetic energy will increase but its potential energy will decrease. In our statistical setting, if total energy is not conserved, then this means there were divergences and our numerical approximation went bad.\n\n\n\nLeapfrog Integrator\nTo simulate the differential equations above, we must first discretize \\(t\\), and go back and forth updating \\(q\\) and \\(p\\). However, this “back and forth” is not so straightforward. It turns out, one way to simulate Hamiltonian dynamics, is with a method called the leapfrog integrator.\nThis integrator first performs a half update of the momentum \\(p\\), then a full update of the position \\(q\\), and then finally another half update of the momentum.\n\\[\\underbrace{v_{t+1/2} = v_t - \\frac{\\eta}{2} \\frac{\\partial \\mathcal{E}(q_t)}{\\partial q}}_{\\text{half update}}\\]\n\\[\\underbrace{q_{t+1} = q_t + \\eta \\frac{\\partial \\mathcal{K}(p_{t+1/2})}{\\partial p}}_{\\text{full update}}\\]\n\\[\\underbrace{v_{t+1} = v_{t+1/2} - \\frac{\\eta}{2} \\frac{\\partial \\mathcal{E}(q_{t+1})}{\\partial q}}_{\\text{half update}}\\]\nThe leapfrog integrator has two important parameters: (1) path length, and (2) step size \\(\\eta\\). In the simululation, the path length represents how “long” you travel before collecting another sample. The step size indicates the size each step is in the path length and determines how fine grained the simulation is. For example, in the drawing below, imagine path length\\(=1\\) for both simulations. However, the left simulation has a step size \\(=4\\) whereas the right simulation has a step size \\(=2\\). These parameters are important in determining how the simulator collects samples of the geometry of the posterior.\n\n\n\nleapfrog\n\n\n\n\nMain Idea\nHMC says the log posterior is like a “bowl” (the empty pool in the figure below), with the highest posterior probability at the center of the valley. If we give the skateboarder a flick, this momentum will simulate its path. It must obey physics, gliding along until we stop the clock and take a sample. If the log posterior is flat, then not much information is in the likelihood & prior and the skateboarder will glide before the gradient makes it turn around. However, if the geometry of the log posterior is not flat, like the pool below, the gradient will make the skateboarder turn around and back into the valley. Since HMC runs a physics simulation, certain things must be conserved, i.e., the total energy of the system.\n\n\n\nskateboarder\n\n\nHMC needs a few things to run: 1. A function or callable that returns the negative log probability of the data at the current position \\(q\\) 2. A means of returning the gradient of the negative log probability at the current position \\(q\\) 3. An integrator for simulating the Hamiltonian equations in discrete time with two parameters: - step size - path length\nThe algorithm for running HMC is to: set the initial position \\(q\\) to \\(q'_0 = q_{t-1}\\), and sample a new random momentum \\(p' \\sim \\mathcal{N}(0, \\sum)\\). Then, initialize a random trajectory in the phase space, starting at (\\(q'_0, p'_0\\)), followed for \\(L\\) leapfrog steps until we get to the new proposed state (\\(q^*, p^*\\)) \\(=\\) (\\(q'_L, p'_L\\)). With the new proposed state, we compute the MH acceptance probability. This process is ran \\(n\\) times, according the number of samples the user wants to collect.\n\n\nHamiltonian Monte Carlo - Multivariate Normal\nNote: The HMC implementation below is inspired by Colin Carroll’s implementation.\n\n\nCode\ndef log_probs_to_img(dist, extent=None, num=100):\n\n    if extent is None:\n        extent = (-3, 3, -3, 3)\n\n    X, Y = torch.meshgrid(\n        torch.linspace(*extent[:2], num), torch.linspace(*extent[2:], num)\n        )\n\n    Z = torch.dstack((X, Y))\n    log_probs = torch.exp(dist.log_prob(Z))\n    \n    return X, Y, log_probs, extent\n\n\ndef plot_hmc(X, Y, Z, samples, positions, momentum, extent):\n\n    collected_samples = np.vstack([tens.detach().numpy() for tens in samples])\n    leap_q = [[tens.detach().numpy() for tens in q] for q in positions]\n    leap_p = [[tens.detach().numpy() for tens in p] for p in momentum]\n\n    final_q = np.concatenate(leap_q)\n    final_p = np.concatenate(leap_p)\n\n    steps = slice(None, None, 20)\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(X, Y, Z, extent=extent, levels=20)\n    plt.quiver(\n        final_q[steps, 0], final_q[steps, 1], final_p[steps, 0], final_p[steps, 1], \n        headwidth=5, scale=80, headlength=7\n        )\n    plt.plot(final_q[:, 0], final_q[:, 1], linestyle='-', lw=2.5, color='black')\n    plt.scatter(\n        collected_samples[:, 0], collected_samples[:, 1],\n        color='red', alpha=0.75\n        )\n\n    plt.title('Hamiltonian Monte Carlo')\n    plt.show()\n\n\n\ndef leapfrog(q, p, dist, path_len, step_size):\n    \n    output = -dist.log_prob(q)\n    output.backward()\n    p -= step_size * q.grad / 2\n    q.grad.zero_()\n\n    leap_q = []\n    leap_p = []\n    for _ in range(int(path_len / step_size) - 1):\n\n        q.grad.zero_()\n        with torch.no_grad():\n            q += step_size * p\n        output = -dist.log_prob(q)\n        output.backward()\n        p -= step_size * q.grad\n\n        leap_q.append(q.clone())\n        leap_p.append(p.clone())\n        \n    output = -dist.log_prob(q)\n    output.backward()\n    \n    with torch.no_grad():\n        q += step_size * p\n    \n    p -= step_size * q.grad / 2\n\n    return q, -p, leap_q, leap_p\n\n\ndef hamiltonian_monte_carlo(\n    n_samples, \n    dist, \n    initial_position,\n    path_len=1, \n    step_size=0.1\n    ):\n    \"\"\"\n\n    This HMC implementation is inspired by Colin Carroll's blog post:\n    https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/\n\n    Parameters:\n    ----------\n    n_samples : int\n        Number of samples to return\n    dist : object\n        PyTorch distribution object that can be called\n    initial_position : np.array\n        A place to start sampling from.\n    path_len : float\n        How long each integration path is. Smaller is faster and more correlated.\n    step_size : float\n        How long each integration step is. Smaller is slower and more accurate.\n\n    Returns:\n    -------\n    param_samples: list\n        list of parameter samples (position q)\n    all_leap_q: list\n        list of all the positions (q) when leapfrog integrator is ran\n    all_leap_p: list\n        list of all the momentum (p) values when leapfrog integrator is ran\n    \"\"\"    \n\n    samples = [initial_position]\n    param_samples = []\n    all_leap_q = []\n    all_leap_p = []\n\n    momentum = torch.distributions.Normal(0, 1)\n\n    size = (n_samples,) + initial_position.shape[:1]\n    for idx, p0 in tqdm(enumerate(momentum.sample(size)), total=size[0]):\n\n        q0 = samples[-1]\n        q_new, p_new, leap_q, leap_p = leapfrog(\n            q=q0,\n            p=p0,\n            dist=dist,\n            path_len=2 * np.random.rand() * path_len,\n            step_size=step_size\n            )\n\n        all_leap_q.append(leap_q)\n        all_leap_p.append(leap_p)\n\n        # Metropolis acceptance criterion\n        start_log_p = torch.sum(momentum.log_prob(p0)) - dist.log_prob(samples[-1])\n        new_log_p = torch.sum(momentum.log_prob(p_new)) - dist.log_prob(q_new)\n        p_accept = min(1, torch.exp(new_log_p - start_log_p))\n\n        if torch.rand(1) &lt; p_accept:\n            param_samples.append(q_new.clone())\n        else:\n            param_samples.append(q0.clone())\n        \n    return param_samples, all_leap_q, all_leap_p\n\n\ndef main(args):\n\n    mvn = dist.MultivariateNormal(torch.zeros(2), torch.eye(2))\n    init_pos = torch.randn(2, requires_grad=True)\n\n    X, Y, Z, extent = log_probs_to_img(mvn, (-3, 3, -3, 3), num=200)\n\n    samples, leap_q, leap_p = hamiltonian_monte_carlo(\n        n_samples=args.n_samples,\n        dist=mvn,\n        initial_position=init_pos,\n        path_len=args.path_len,\n        step_size=args.step_size\n        )\n\n    plot_hmc(X, Y, Z, samples, leap_q, leap_p, extent)\n\n\nparser = argparse.ArgumentParser(description='HMC')\nparser.add_argument('--n_samples', type=int, default=10)\nparser.add_argument('--path_len', type=int, default=1)\nparser.add_argument('--step_size', type=float, default=0.01)\nargs = parser.parse_args(\"\")\n\nmain(args)\n\n100%|██████████| 10/10 [00:00&lt;00:00, 62.54it/s]"
  },
  {
    "objectID": "posts/2021-05-31-Economics-of-Open-Source.html",
    "href": "posts/2021-05-31-Economics-of-Open-Source.html",
    "title": "Economics of Open Source",
    "section": "",
    "text": "Why does open-source work?\nHighly skilled people devoting their valuable time for little to no monetary benefit.\n. . ."
  },
  {
    "objectID": "posts/2022-07-12-bmcp-ch-3.html",
    "href": "posts/2022-07-12-bmcp-ch-3.html",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport arviz as az\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions import constraints\nfrom pyro.infer import Predictive, TracePredictive, NUTS, MCMC\nfrom pyro.infer.autoguide import AutoLaplaceApproximation\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\nfrom pyro.infer.mcmc.util import summary\nfrom palmerpenguins import load_penguins\nplt.style.use('ggplot')\nplt.rcParams[\"figure.figsize\"] = (7, 4)"
  },
  {
    "objectID": "posts/2022-07-12-bmcp-ch-3.html#linear-penguins",
    "href": "posts/2022-07-12-bmcp-ch-3.html#linear-penguins",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
    "section": "3.2.1 Linear Penguins",
    "text": "3.2.1 Linear Penguins\n\nadelie_flipper_length = torch.from_numpy(penguins.loc[adelie_mask, 'flipper_length_mm'].values)\nadelie_flipper_length -= adelie_flipper_length.mean()\nadelie_mass = torch.from_numpy(penguins.loc[adelie_mask, 'body_mass_g'].values)\n\n\ndef linear_model(flipper_length, mass=None):\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(2000.))\n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 4000.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 4000.))\n    mu = pyro.deterministic('mu', beta_0 + beta_1 * flipper_length)\n\n    with pyro.plate('plate'):   \n        preds = pyro.sample('mass', dist.Normal(mu, sigma), obs=mass)  \n\n\npyro.render_model(\n    linear_model, \n    model_args=(adelie_flipper_length, adelie_mass),\n    render_distributions=True\n    )\n\n\n\n\n\nMCMC\n\nkernel = NUTS(linear_model, adapt_step_size=True)\nmcmc_simple = MCMC(kernel, num_samples=500, warmup_steps=300)\nmcmc_simple.run(flipper_length=adelie_flipper_length, mass=adelie_mass)\n\nSample: 100%|██████████| 800/800 [00:37, 21.16it/s, step size=9.30e-01, acc. prob=0.911] \n\n\n\nmcmc_simple.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n    beta_0   3706.50     33.96   3706.28   3650.73   3755.13    421.93      1.01\n    beta_1     32.37      5.06     32.27     25.29     42.32    401.68      1.00\n     sigma    408.44     23.59    407.82    366.63    443.52    353.65      1.00\n\nNumber of divergences: 0\n\n\n\n# trace plots are interesting for only only chain\naz.plot_trace(az.from_pyro(mcmc_simple))\nplt.tight_layout()\n\n\n\n\n\n\nPosterior Predictive Distribution\n\n# posterior samples\nmcmc_samples = mcmc_simple.get_samples(num_samples=1000)\n# pred. dist. conditioned on posterior samples\npredictive = Predictive(linear_model, mcmc_samples)\npredictive_samples = predictive(flipper_length=adelie_flipper_length, mass=None) \n\nfor k, v in predictive_samples.items():\n    print(f'{k}: {tuple(v.shape)}')\n\nmass: (1000, 146)\nmu: (1000, 1, 146)\n\n\n\ndef mcmc_fit(predictive):\n    mass = predictive['mass']\n    mass_mu = mass.mean(axis=0)\n    mass_std = mass.std(axis=0)\n\n    mass_df = pd.DataFrame({\n        'feat': adelie_flipper_length,\n        'mean': mass_mu,\n        'high': mass_mu + mass_std,\n        'low': mass_mu - mass_std}\n    )\n\n    return mass_df.sort_values(by=['feat'])\n\n\nmass_df = mcmc_fit(predictive=predictive_samples)\n\n\nplt.scatter(adelie_flipper_length.numpy(), adelie_mass.numpy(), alpha=0.5)\nplt.plot(mass_df['feat'], mass_df['mean'], color='black')\nplt.fill_between(\n    mass_df['feat'], mass_df['high'], mass_df['low'], alpha=0.2, color='grey')\nplt.xlabel('adelie_flipper_length')\nplt.ylabel('mass')\nplt.title('$\\mu =  \\\\beta_0 + \\\\beta_1X_1$')\nplt.show()\n\n\n\n\n\n\nSVI\nNOT WORKING: I believe it has to do with the autoguide. Solution could be to implement the guide by hand\npyro.get_param_store() is comprised of learned parameters that will be used in the Predictive stage. Instead of providing samples, the guide parameter is used to construct the posterior predictive distribution"
  },
  {
    "objectID": "posts/2022-07-12-bmcp-ch-3.html#multiple-linear-regression",
    "href": "posts/2022-07-12-bmcp-ch-3.html#multiple-linear-regression",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
    "section": "3.3 Multiple Linear Regression",
    "text": "3.3 Multiple Linear Regression\n\nsex_obs = torch.from_numpy(penguins.loc[adelie_mask, 'sex'].replace({'male': 0, 'female': 1}).values)\n\nsns.scatterplot(\n    x=adelie_flipper_length, y=adelie_mass_obs, hue=sex_obs, alpha=0.5)\nplt.xlabel('adelie_flipper_length')\nplt.ylabel('mass')\nplt.show()\n\n\n\n\n\ndef linear_model(flipper_length, sex, mass=None):\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(2000.))\n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 3000.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 3000.))\n    beta_2 = pyro.sample('beta_2', dist.Normal(0., 3000.))\n    mu = pyro.deterministic('mu', beta_0 + beta_1 * flipper_length + beta_2 * sex)\n\n    with pyro.plate('plate'):   \n        preds = pyro.sample('mass', dist.Normal(mu, sigma), obs=mass)  \n\n\npyro.render_model(\n    linear_model, \n    model_args=(adelie_flipper_length, sex_obs, adelie_mass),\n    render_distributions=True\n    )\n\n\n\n\n\nMCMC\n\nkernel = NUTS(linear_model, adapt_step_size=True)\nmcmc = MCMC(kernel, num_samples=500, warmup_steps=300, num_chains=1)\nmcmc.run(flipper_length=adelie_flipper_length, sex=sex_obs, mass=adelie_mass)\nmcmc.summary()\n\nSample: 100%|██████████| 800/800 [00:53, 15.07it/s, step size=5.06e-01, acc. prob=0.926] \n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n    beta_0   4004.68     37.17   4003.62   3947.60   4064.92    329.83      1.00\n    beta_1     16.42      3.98     16.41      9.50     22.23    234.91      1.00\n    beta_2   -597.92     54.41   -594.84   -684.72   -514.38    313.84      1.00\n     sigma    297.48     18.46    296.21    271.08    332.22    352.02      1.00\n\nNumber of divergences: 0\n\n\n\naz.plot_trace(az.from_pyro(mcmc))\nplt.tight_layout()\n\n\n\n\n\nmcmc_samples = mcmc.get_samples(num_samples=1000)\npredictive = Predictive(linear_model, mcmc_samples)\npredictive_samples = predictive(flipper_length=adelie_flipper_length, sex=sex_obs, mass=None) \n\n\nmass_mu = predictive_samples['mass'].numpy().mean(axis=0)\nmass_std = predictive_samples['mass'].numpy().std(axis=0)\n\npredictions = pd.DataFrame({\n    'sex': sex_obs,\n    'flipper': adelie_flipper_length,\n    'mass_mu': mass_mu,\n    'mass_std': mass_std,\n    'high': mass_mu + mass_std,\n    'low': mass_mu - mass_std\n})\n\npredictions = predictions.sort_values(by=['flipper'])\n\n\nmale = predictions[predictions['sex'] == 0]\nfemale = predictions[predictions['sex'] == 1]\n\n\nsns.scatterplot(\n    x=adelie_flipper_length, y=adelie_mass_obs, hue=sex_obs, alpha=0.5)\nplt.plot(male['flipper'], male['mass_mu'])\nplt.plot(female['flipper'], female['mass_mu'])\nplt.fill_between(\n    male['flipper'], male['high'], male['low'], alpha=0.2, color='grey')\nplt.fill_between(\n    female['flipper'], female['high'], female['low'], alpha=0.2, color='grey')\nplt.xlabel('adelie_flipper_length')\nplt.ylabel('mass')\nplt.title('Predictions $\\pm 1 \\sigma$ ')\nplt.show()\n\n\n\n\n\nmcmc_multiple_az = az.from_pyro(mcmc)\nmcmc_simple_az = az.from_pyro(mcmc_simple)\n\naz.plot_forest([mcmc_simple_az, mcmc_multiple_az], var_names=['sigma'])\n# manually specify to avoid confusion\nplt.legend(['flipper_x_sex $\\sigma$', 'flipper_only $\\sigma$'])\nplt.show()\n\n\n\n\n\n\nCounterfactuals\n\nbill_length_obs = torch.from_numpy(penguins.loc[adelie_mask, 'bill_length_mm'].values)\nbill_length_obs -= bill_length_obs.mean()\n\n\ndef linear_model_counterfactual(flipper_length, sex, bill_length, mass=None):\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(2000.))\n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 3000.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 3000.))\n    beta_2 = pyro.sample('beta_2', dist.Normal(0., 3000.))\n    beta_3 = pyro.sample('beta_3', dist.Normal(0., 3000.))\n    mu = pyro.deterministic(\n        'mu', beta_0 + beta_1 * flipper_length + beta_2 * sex + beta_3 * bill_length)\n\n    with pyro.plate('plate'):   \n        preds = pyro.sample('mass', dist.Normal(mu, sigma), obs=mass)  \n\n\npyro.render_model(\n    linear_model_counterfactual, \n    model_args=(adelie_flipper_length, bill_length_obs, sex_obs, adelie_mass),\n    render_distributions=True\n    )\n\n\nkernel = NUTS(linear_model_counterfactual, adapt_step_size=True)\nmcmc = MCMC(kernel, num_samples=500, warmup_steps=300, num_chains=1)\nmcmc.run(adelie_flipper_length, bill_length_obs, sex_obs, adelie_mass)\nmcmc.summary()\n\nSample: 100%|██████████| 800/800 [01:08, 11.60it/s, step size=4.50e-01, acc. prob=0.913] \n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n    beta_0   3978.38     41.39   3978.53   3900.43   4039.73    201.51      1.00\n    beta_1     14.99      4.32     15.03      7.73     20.89    326.37      1.01\n    beta_2     21.17     12.07     21.02      1.69     40.13    258.26      1.00\n    beta_3   -541.23     64.61   -540.35   -646.93   -431.72    196.03      1.00\n     sigma    295.87     16.45    294.49    269.92    323.25    452.90      1.00\n\nNumber of divergences: 0\n\n\n\nmcmc_samples = mcmc.get_samples(num_samples=1000)\n\n\nmean_flipper_length = penguins.loc[adelie_mask, 'flipper_length_mm'].mean()\ncounterfactual_flipper_lengths = torch.linspace(mean_flipper_length - 20, mean_flipper_length + 20, 21)\ncounterfactual_flipper_lengths -= counterfactual_flipper_lengths.mean()\n\nsex_indicator = torch.zeros_like(counterfactual_flipper_lengths)\nmean_bill_length = torch.ones_like(counterfactual_flipper_lengths) * bill_length_obs.mean()\n\n\ncounterfactual_samples = Predictive(\n    linear_model_counterfactual,\n    mcmc_samples)(counterfactual_flipper_lengths, sex_indicator, mean_bill_length, None)\n\nmass_mu = counterfactual_samples['mass'].numpy().mean(axis=0)\nmass_std = counterfactual_samples['mass'].numpy().std(axis=0)\n\n\nplt.plot(counterfactual_flipper_lengths, mass_mu, color='blue')\nplt.fill_between(\n    x=counterfactual_flipper_lengths , \n    y1=mass_mu + mass_std, \n    y2=mass_mu - mass_std,\n    color='grey',\n    alpha=0.5)\nplt.ylabel('counterfactual expected mass')\nplt.xlabel('manipulated flipper length (centered)')\nplt.title('Counterfactual plot');"
  },
  {
    "objectID": "posts/2022-07-12-bmcp-ch-3.html#generalized-linear-models",
    "href": "posts/2022-07-12-bmcp-ch-3.html#generalized-linear-models",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nLogistic Regression\n\nspecies_filter = penguins['species'].isin(['Adelie', 'Chinstrap'])\n\n\nspecies_filter = penguins['species'].isin(['Adelie', 'Chinstrap'])\nbill_length_obs = torch.from_numpy(penguins.loc[species_filter, 'bill_length_mm'].values.reshape(-1, 1))\nbill_length_obs = torch.tensor(bill_length_obs, dtype=torch.float)\n\nspecies = pd.Categorical(penguins.loc[species_filter, 'species'])\nspecies_codes = torch.from_numpy(species.codes).to(torch.float64)\nspecies_codes = torch.tensor(species_codes, dtype=torch.float)\n\n\ndef logistic_model(bill_length, species=None):\n\n    N, P = bill_length.shape\n\n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 10.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 10.).expand([P]))#.unsqueeze(-1)\n    mu = beta_0 + torch.matmul(beta_1, bill_length.T) \n    theta = pyro.deterministic('theta', torch.sigmoid(mu))\n    db = pyro.deterministic('db', -beta_0 / beta_1)\n\n    with pyro.plate('plate'):\n        y1 = pyro.sample('y1', dist.Bernoulli(theta), obs=species)\n\n\n# we don't observe theta or db\npyro.render_model(\n    logistic_model, \n    model_args=(bill_length_obs, species_codes),\n    render_distributions=True)\n\n\n\n\n\n\nMCMC\n\nkernel = NUTS(logistic_model, adapt_step_size=True)\nmcmc_logistic = MCMC(kernel, num_samples=500, warmup_steps=300)\nmcmc_logistic.run(bill_length=bill_length_obs, species=species_codes)\nmcmc_logistic.summary()\n\nSample: 100%|██████████| 800/800 [00:11, 70.35it/s, step size=9.29e-02, acc. prob=0.857]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n    beta_0    -33.38      4.12    -33.01    -41.20    -27.87     55.78      1.00\n beta_1[0]      0.75      0.10      0.75      0.61      0.91     55.19      1.00\n\nNumber of divergences: 0\n\n\n\naz.plot_trace(az.from_pyro(mcmc_logistic))\nplt.tight_layout()\n\n\n\n\n\n# latent variables\nmcmc_samples = mcmc_logistic.get_samples(num_samples=1000)\n# posterior samples \npredictive = Predictive(logistic_model, mcmc_samples)\n# posterior predictive\npredictive_samples = predictive(bill_length_obs, None) \n\n\nprob_mu = predictive_samples['theta'].numpy().mean(axis=0).flatten()\nprob_std = predictive_samples['theta'].numpy().std(axis=0).flatten()\ndb_mu = predictive_samples['db'].numpy().mean()\ndb_std = predictive_samples['db'].numpy().std()\n\npredictions = pd.DataFrame({\n    'bill_length': bill_length_obs.flatten(),\n    'prob_mu': prob_mu,\n    'prob_std': prob_std,\n    'high': prob_mu + prob_std,\n    'low': prob_mu - prob_std\n})\n\npredictions = predictions.sort_values(by=['bill_length'])\n\n\nfor i, (label, marker) in enumerate(zip(species.categories, (\".\", \"s\"))):\n    _filter = (species.codes == i) ## size\n    x = bill_length_obs[_filter] ## x_obs\n    y = np.random.normal(i, 0.02, size=_filter.sum()) ## small amount of noise (jitter)\n    plt.scatter(bill_length_obs[_filter], y, marker=marker, label=label, alpha=.8)\n\nplt.plot(predictions['bill_length'], predictions['prob_mu'], color='black')\nplt.fill_between(\n    predictions['bill_length'], predictions['high'], predictions['low'],\n    alpha=0.25, color='grey')\nplt.axvline(\n    x=predictive_samples['db'].numpy().mean(), linestyle='--', color='black')\nplt.xlabel('bill length')\nplt.ylabel('Probability')\nplt.title('Logistic Model - MCMC');\n\n\n\n\n\n\nSVI\n\ndef logistic_guide(bill_length, species=None):\n\n    N, P = bill_length.shape\n    \n    beta_0_loc = pyro.param('beta_0_loc', torch.tensor(0.))\n    beta_0_scale = pyro.param('beta_0_scale', torch.tensor(0.1), constraint=constraints.positive)\n    beta_0 = pyro.sample('beta_0', dist.Normal(beta_0_loc, beta_0_scale))\n\n    beta_1_loc = pyro.param('beta_1_loc', torch.tensor(0.1))\n    beta_1_scale = pyro.param('beta_1_scale', torch.tensor(0.1), constraint=constraints.positive)\n    beta_1 = pyro.sample('beta_1', dist.Normal(beta_1_loc, beta_1_scale).expand([P]))\n\n\npyro.render_model(\n    logistic_guide, \n    model_args=(bill_length_obs, species_codes), \n    render_params=True\n    )\n\n\n\n\n\npyro.clear_param_store()\n\noptim = Adam({\"lr\": 0.2})\nsvi = SVI(logistic_model, logistic_guide, optim, Trace_ELBO())\n\nelbo_loss = []\nfor i in range(500):\n    loss = svi.step(bill_length_obs, species_codes)\n    elbo_loss.append(loss)\n\nplt.plot(np.arange(1, 501), elbo_loss)\nplt.ylabel('ELBO Loss')\nplt.xlabel('Iterations')\nplt.title(f'iter: 1000, loss: {elbo_loss[-1]:.4f}');\n\n\n\n\n\nfor name, value in pyro.get_param_store().items():\n    print(name, pyro.param(name))\n\nbeta_0_loc tensor(-13.7759, requires_grad=True)\nbeta_0_scale tensor(0.1191, grad_fn=&lt;AddBackward0&gt;)\nbeta_1_loc tensor(0.3193, requires_grad=True)\nbeta_1_scale tensor(0.0044, grad_fn=&lt;AddBackward0&gt;)\n\n\n\npredictive = Predictive(logistic_model, guide=logistic_guide, num_samples=1000)\nposterior_svi_samples = predictive(bill_length_obs, None)\n\n\nprob_mu = posterior_svi_samples['theta'].numpy().mean(axis=0).flatten()\nprob_std = posterior_svi_samples['theta'].numpy().std(axis=0).flatten()\n\npredictions = pd.DataFrame({\n    'bill_length': bill_length_obs.flatten(),\n    'prob_mu': prob_mu,\n    'prob_std': prob_std,\n    'high': prob_mu + prob_std,\n    'low': prob_mu - prob_std\n})\n\npredictions = predictions.sort_values(by=['bill_length'])\n\n\nfor i, (label, marker) in enumerate(zip(species.categories, (\".\", \"s\"))):\n    _filter = (species.codes == i) ## size\n    x = bill_length_obs[_filter] ## x_obs\n    y = np.random.normal(i, 0.02, size=_filter.sum()) ## add small amount of noise for plotting\n    plt.scatter(bill_length_obs[_filter], y, marker=marker, label=label, alpha=.8)\n\nplt.plot(predictions['bill_length'], predictions['prob_mu'], color='black')\nplt.fill_between(\n    predictions['bill_length'], predictions['high'], predictions['low'],\n    alpha=0.25, color='grey')\n# plt.axvline(\n#     x=predictive_samples['db'].numpy().mean(), linestyle='--', color='black')\nplt.xlabel('bill length')\nplt.ylabel('Probability')\nplt.title('Logistic Model - SVI');\n\n\n\n\n\n\nCode 3.24\nUsing body mass and flipper length as covariates\nWhen creating a multidimensional distribution in pyro, there is the added functionality of .to_event(1). This method implies that “these dimensions should be treated as a single event”. - see discussion here\n\nX = penguins.loc[species_filter, ['bill_length_mm', 'body_mass_g']]\n\nbill_length_mu = X['bill_length_mm'].mean()\nbill_length_std = X['bill_length_mm'].std()\nbody_mass_g_mu = X['body_mass_g'].mean()\nbody_mass_g_std = X['body_mass_g'].std()\n\nX['bill_length_mm'] = (X['bill_length_mm'] - bill_length_mu) / bill_length_std\nX['body_mass_g'] = (X['body_mass_g'] - body_mass_g_mu) / body_mass_g_std\nX = torch.from_numpy(X.values).to(torch.float)\n\nintercept = torch.ones_like(X[:, 0][..., None])\nX = torch.hstack((intercept, X))\n\nspecies_codes = species_codes.to(torch.float)\n\n\ndef multiple_logistic_model(data_matrix, species=None):\n\n    N, K = data_matrix.size()\n\n    # w = pyro.sample('coef', dist.MultivariateNormal(\n    #     loc=torch.ones(K), covariance_matrix=torch.eye(K)\n    # ))\n\n    w = pyro.sample('coef', dist.Normal(0., 10.).expand([K]))    \n    mu = torch.matmul(w, data_matrix.T)\n    theta = pyro.deterministic('theta', torch.sigmoid(mu))\n    db = pyro.deterministic('db', -w[0] / w[2] - w[1] / w[2] * data_matrix[:, 1])\n    \n    with pyro.plate('output', N):\n        y1 = pyro.sample('obs', dist.Bernoulli(logits=theta), obs=species)\n\n\npyro.render_model(\n    multiple_logistic_model, \n    model_args=(X, species_codes),\n    render_distributions=True\n    )\n\n\n\n\n\nkernel = NUTS(model=multiple_logistic_model, adapt_step_size=True)\nmcmc_mult_logistic = MCMC(kernel, num_samples=800, warmup_steps=500)\nmcmc_mult_logistic.run(X, species_codes)\n\nSample: 100%|██████████| 1300/1300 [00:05, 222.11it/s, step size=2.76e-01, acc. prob=0.935]\n\n\n\nmcmc_mult_logistic.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n   coef[0]     -6.54      3.13     -6.00    -11.19     -1.35    154.09      1.00\n   coef[1]     15.50      5.52     15.11      6.80     23.63    151.94      1.01\n   coef[2]     -4.98      2.82     -4.58     -9.15     -0.30    203.16      1.00\n\nNumber of divergences: 0\n\n\n\nmcmc_mult_samples = mcmc_mult_logistic.get_samples(num_samples=1000)\npost_predictive = Predictive(multiple_logistic_model, mcmc_mult_samples)\npredictive_samples = post_predictive(X, None)\n\nfor k, v in predictive_samples.items():\n    print(f'{k}: {tuple(v.shape)}')\n\nobs: (1000, 214)\ntheta: (1000, 1, 214)\ndb: (1000, 1, 214)\n\n\n\ninf_data = az.from_pyro(\n    mcmc_mult_logistic,\n    posterior_predictive=mcmc_mult_samples\n)\n\n\naz.plot_trace(inf_data, compact=False, var_names=['coef'])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nprob_mu = predictive_samples['theta'].mean(axis=0)[0]\nprob_std = predictive_samples['theta'].std(axis=0)[0]\ndb_mu = predictive_samples['db'].mean(axis=0)[0]\ndb_std = predictive_samples['db'].std(axis=0)[0]\n\npredictions = pd.DataFrame({\n    'bill_length': X[:, 1].numpy(),\n    'mass': X[:, 2].numpy(),\n    'species': species_codes.numpy(),\n    'prob_mu': prob_mu,\n    'prob_std': prob_std,\n    'high': prob_mu + prob_std,\n    'low': prob_mu - prob_std,\n    'db_mu': db_mu,\n    'db_high': db_mu + db_std,\n    'db_low': db_mu - db_std\n})\n\npredictions = predictions.sort_values(by=['bill_length', 'mass'])\n\n\nsns.scatterplot(data=predictions, x='bill_length', y='mass', hue='species')\nsns.lineplot(data=predictions, x='bill_length', y='db_mu', color='black');\nplt.fill_between(\n    x=predictions['bill_length'], \n    y1=predictions['db_high'], \n    y2=predictions['db_low'],\n    color='grey', alpha=0.5\n    )\nplt.ylim(bottom=-10, top=10)\n\n(-10.0, 10.0)\n\n\n\n\n\n\n# not working\naz.plot_separation(inf_data, y='obs')\n\n\n\nInterpreting log odds\n\ncounts = penguins['species'].value_counts()\nadelie_count = counts['Adelie']\nchinstrap_count = counts['Chinstrap']\nadelie_probs = adelie_count / (adelie_count + chinstrap_count)\n\nprint(f'prior probability of adelie     = {adelie_probs:.4f}')\nprint(f'odds of adelie                  = {(adelie_probs / (1 - adelie_probs)):.4f}')\nprint(f'log odds (logit) of adelie      = {np.log(adelie_probs / (1 - adelie_probs)):.4f}')\n\nprior probability of adelie     = 0.6822\nodds of adelie                  = 2.1471\nlog odds (logit) of adelie      = 0.7641\n\n\n\nbeta_0 = inf_data['posterior']['coef'].to_numpy()[0][:, 0].mean()\nbeta_1 = inf_data['posterior']['coef'].to_numpy()[0][:, 1].mean()\nbeta_2 = inf_data['posterior']['coef'].to_numpy()[0][:, 2].mean()\n\nbill_length = 0\nval_1 = beta_0 + beta_1*bill_length + beta_2*0\nval_2 = beta_0 + beta_1*(bill_length+0.5) + beta_2*0\n\nval_1_probs = 1 / (1 + np.exp(-val_1))\nval_2_probs = 1 / (1 + np.exp(-val_2))\n\nprint(f'''\nincreasing bill length by 0.5 stddev (while holding body mass constant) \nincreases class probability from {val_1_probs:.2f} to {val_2_probs:.2f}\n''')\n\n\nincreasing bill length by 0.5 stddev (while holding body mass constant) \nincreases class probability from 0.19 to 0.55"
  },
  {
    "objectID": "posts/2022-07-12-bmcp-ch-3.html#picking-priors-in-regression-models",
    "href": "posts/2022-07-12-bmcp-ch-3.html#picking-priors-in-regression-models",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
    "section": "Picking priors in regression models",
    "text": "Picking priors in regression models\n\nx = torch.arange(-2, 3, 1)\ny = torch.tensor([50, 44, 50, 47, 56])\n\nplt.scatter(x, y)\nplt.title('Attractiveness of parent and sex ratio');\n\n\n\n\n\ndef model_uninformative_prior_sex_ratio(x, obs=None):\n\n    sigma = pyro.sample('sigma', dist.Exponential(0.5))\n    beta_0 = pyro.sample('beta_0', dist.Normal(50., 20.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 20.))\n\n    mu = pyro.deterministic('mu', beta_0 + beta_1 * x)\n    ratio = pyro.sample('ratio', dist.Normal(mu, sigma), obs=obs)\n\ndef model_informative_prior_sex_ratio(x, obs=None):\n\n    sigma = pyro.sample('sigma', dist.Exponential(0.5))\n    beta_0 = pyro.sample('beta_0', dist.Normal(50., 0.5))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 0.5))\n\n    mu = pyro.deterministic('mu', beta_0 + beta_1 * x)\n    ratio = pyro.sample('ratio', dist.Normal(mu, sigma), obs=obs)\n\n\nsex_ratio_uninform_prior_mcmc = MCMC(NUTS(\n    model_uninformative_prior_sex_ratio), num_samples=500, warmup_steps=300)\nsex_ratio_uninform_prior_mcmc.run(x, y)\n\nsex_ratio_inform_prior_mcmc = MCMC(NUTS(\n    model_informative_prior_sex_ratio), num_samples=500, warmup_steps=300)\nsex_ratio_inform_prior_mcmc.run(x, y)\n\nSample: 100%|██████████| 800/800 [00:04, 185.18it/s, step size=5.95e-01, acc. prob=0.923]\nSample: 100%|██████████| 800/800 [00:02, 298.60it/s, step size=1.04e+00, acc. prob=0.879]\n\n\n\n# uninformative prior\nuninform_prior_predictive = Predictive(\n    model_uninformative_prior_sex_ratio, \n    None, \n    num_samples=500)(x, None)\n\nuninform_post_samples = sex_ratio_uninform_prior_mcmc.get_samples(500)\n\nuninform_post_predictive = Predictive(\n    model_uninformative_prior_sex_ratio, \n    posterior_samples=uninform_post_samples)(x, None)\n\n# informative prior\ninform_prior_predictive = Predictive(\n    model_informative_prior_sex_ratio, \n    None, \n    num_samples=500)(x, None)\n\ninform_post_samples = sex_ratio_inform_prior_mcmc.get_samples(500)\n\ninform_post_predictive = Predictive(\n    model_informative_prior_sex_ratio, \n    posterior_samples=inform_post_samples)(x, None)\n\n\nfig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(10, 7))\n\nax[0].scatter(x, y, color='black')\nax[0].plot(x, uninform_prior_predictive['ratio'].T, color='grey', alpha=0.1)\nax[0].plot(x, uninform_prior_predictive['ratio'].mean(axis=0), color='blue', alpha=0.75, label='mean prior sample')\nax[0].set_ylabel('ratio (in %)')\nax[0].set_ylim(bottom=0, top=100)\nax[0].legend()\nax[0].set_title('Uninformative prior samples')\n\nax[1].scatter(x, y, color='black')\nax[1].plot(x, uninform_post_predictive['ratio'].T, color='grey', alpha=0.1)\nax[1].plot(x, uninform_post_predictive['ratio'].mean(axis=0), color='blue', alpha=0.75, label='mean post. sample')\nax[1].set_ylabel('ratio (in %)')\nax[1].set_ylim(bottom=0, top=100)\nax[1].legend()\nax[1].set_title('Uninformative posterior samples');\n\n\n\n\n\nfig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(10, 7))\n\nax[0].scatter(x, y, color='black')\nax[0].plot(x, inform_prior_predictive['ratio'].T, color='grey', alpha=0.1)\nax[0].plot(x, inform_prior_predictive['ratio'].mean(axis=0), color='blue', alpha=0.75, label='mean prior sample')\nax[0].set_ylabel('ratio (in %)')\nax[0].set_ylim(bottom=0, top=100)\nax[0].legend()\nax[0].set_title('Informative prior samples')\n\nax[1].scatter(x, y, color='black')\nax[1].plot(x, inform_post_predictive['ratio'].T, color='grey', alpha=0.1)\nax[1].plot(x, inform_post_predictive['ratio'].mean(axis=0), color='blue', alpha=0.75, label='mean post. sample')\nax[1].set_ylabel('ratio (in %)')\nax[1].set_ylim(bottom=0, top=100)\nax[1].legend()\nax[1].set_title('Informative posterior samples');"
  },
  {
    "objectID": "posts/2022-10-12-inference-gibbs.html",
    "href": "posts/2022-10-12-inference-gibbs.html",
    "title": "Inference - Gibbs Sampling from Scratch",
    "section": "",
    "text": "A variant of the Metropolis-Hastings (MH) algorithm that uses clever proposals and is therefore more efficient (you can get a good approximate of the posterior with far fewer samples) is Gibbs sampling. A problem with MH is the need to choose the proposal distribution, and the fact that the acceptance rate may be low.\nThe improvement arises from adaptive proposals in which the distribution of proposed parameter values adjusts itself intelligently, depending upon the parameter values at the moment. This dependence upon the parameters at that moment is an exploitation of conditional independence properties of a graphical model to automatically create a good proposal, with acceptance probability equal to one.\n\nMain Idea\nSuppose we have a 3-dimensional joint distribution. Estimating this joint distribution is much harder than a 1-dimensional distribution. Subsequently, sampling is also harder in \\(\\mathbb{R}^3\\). In Gibbs sampling, you condition each variable on the values of all the other variables in the distribution. For example, if we have \\(D=3\\) variables:\n\\[x_{1}^{s+1} \\sim p(x_1 | x_2^s,x_3^s)\\]\n\\[x_{2}^{s+1} \\sim p(x_2 | x_1^{s+1},x_3^s)\\]\n\\[x_{3}^{s+1} \\sim p(x_3 | x_1^{s+1},x_2^{s+1})\\]\nwhere \\(x_1, x_2,...,x_n\\) are variable \\(1, 2,...,n\\), respectively. By conditioning on the values of the other variables, sampling from the conditional distribution is much easier than the joint. Because of the exploitation of conditional independence properties of graphical models, the Gibbs algorithm can readily generalize to \\(D\\) variables.\n\n\nGibbs Sampling - Bayesian Gaussian Mixture Model\nHere, the Gibbs sampling algorithm is implemented in PyTorch for a 2-dimensional Bayesian Gaussian Mixture Model (GMM). The Bayesian GMM is given by:\n\\[p(z = k, x | \\theta) = \\pi_k \\mathcal{N}(x|\\mu_k, \\sum_k)\\]\nwhere the parameters \\(\\theta\\) are known and implemented using PyTorch’s MixtureSameFamily distribution class. This class implements a batch of mixture distributions where all components are from different parameterizations of the same distribution type (Normal distributions in this example). It is then parameterized by a Categorical distribution over \\(k\\) components.\nFor a GMM, the full conditional distributions are:\n\\[p(x|z = k, \\theta) = \\mathcal{N}(x|\\mu_k, \\sum_k)\\]\nThis conditional distribution reads; “the probability of data \\(x\\) given component \\(k\\) parameterized by \\(\\theta\\) is distributed according to a Normal distribution with a mean vector \\(\\mu\\) and covariance \\(\\sum\\) according to component \\(k\\)”.\n\\[p(z = k | x) = \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\sum_k)}{\\sum_{k'} \\mathcal{N}(x|\\mu_{k'}, \\sum_{k'})}\\]\nThis conditional distribution is given by Bayes rule and reads; “the probability of component \\(k\\) given we observe some data \\(x\\) is equal to the prior probability \\(\\pi\\) of component \\(k\\) times the likelihood of data \\(x\\) being distributed according to a Normal distribution with a mean vector \\(\\mu\\) and covariance \\(\\sum\\) according to component \\(k\\)” over the total probability of components \\(k\\).\nWith the conditional distributions defined, the Gibbs sampling algorithm can be implemented. Below is the full code to reproduce the results and each main step is outlined below the code block.\n\n\nCode\ndef plot_gibbs(trace_hist, probs, scales, mus, n_iters, n_eval=500):\n\n    mix = dist.Categorical(probs=probs)\n    comp = dist.Independent(dist.Normal(loc=mus, scale=scales), 1)\n    norm_mixture = dist.MixtureSameFamily(mix, comp)\n    \n    x = torch.arange(-1, 2, 0.01)\n    y = torch.arange(-1, 2, 0.01) \n\n    X, Y = torch.meshgrid(x, y)\n    Z = torch.dstack((X, Y))\n    probs_z = torch.exp(norm_mixture.log_prob(Z))\n\n    fig = plt.figure(figsize=(12, 5))\n    plt.contourf(X, Y, probs_z, levels=15)\n    plt.scatter(trace_hist[:, 0], trace_hist[:, 1], alpha=0.25, color='red')\n    plt.xlim(-1, 2)\n    plt.ylim(-1, 2)\n    plt.colorbar()\n    plt.xlabel(xlabel='$X$')\n    plt.ylabel(ylabel='$Y$')\n    plt.title('Gibbs Sampling for a Mixture of 2d Gaussians')\n    plt.show()\n\n\n\ndef gibbs_sampler(x0, z0, kv, probs, mu, scale, n_iterations, rng_key=None):\n    \"\"\"\n    implements the gibbs sampling algorithm for known params. of\n    a 2d GMM\n    \"\"\"\n\n    x_current = x0\n    z_current = z0\n\n    x_samples = torch.zeros(n_iterations, 2)\n    z_samples = torch.zeros(n_iterations)\n     \n    for n in range(1, n_iterations):\n\n        # p(Z = k | X = x)\n        probs_z = torch.exp(\n            dist.Independent(\n                dist.Normal(loc=mu, scale=scale), 1).log_prob(x_current))\n        # p(Z = k) * p(Z = k | X = x)\n        probs_z *= probs\n        # denom. of Bayes\n        probs_z = probs_z / torch.sum(probs_z)\n        # indexing component Z = k\n        z_current = kv[-1] if probs_z[-1] &gt; probs[0] else kv[0]\n        # draw new sample X conditioned on Z = k\n        x_current = dist.Normal(loc=mu[z_current], scale=scale[z_current]).sample()\n\n        x_samples[n] = x_current\n        z_samples[n] = z_current\n\n    return x_samples, z_samples\n\n\ndef main(args):\n\n    # initial sample values\n    x0 = torch.randn((2,))\n    # initial component values\n    z0 = torch.randint(0, 2, (2,))\n    # for indexing\n    kv = np.arange(2)\n    # defining prior mixture probability p(Z = k)\n    mixture_probs = torch.tensor([0.4, 0.6])\n    # defining mu vector and covariance matrix\n    mus = torch.randn(2, 2)\n    scales = torch.rand(2, 2)\n\n    x_samples, z_samples = gibbs_sampler(\n        x0, z0, kv, mixture_probs, mus, scales, args.iters\n        )\n\n    plot_gibbs(x_samples, mixture_probs, scales, mus, args.iters)\n\n\nparser = argparse.ArgumentParser(description='rw-mh')\nparser.add_argument('--iters', type=int, default=1000)\nargs = parser.parse_args(\"\")\n\nmain(args)\n\n\n\n\n\n\nExplanation of Code\nThe main steps to implement the Gibbs sampling algorithm:\n\nThe main() function defines the initial values for the sample, component, and mixture distribution.\nThe gibbs_sampler() function first sets the values of x_current and z_current for current data point \\(x\\) and component \\(z\\), respectively. To analyze the trace history of the sampler, x_samples and z_samples are empty lists.\nIn the for loop, the conditional \\(p(z = k \\vert x)\\) is computed using x_current, i.e., given we have observed datum \\(x\\), what is the log probability of component \\(Z = k\\).\nprobs_z is then multiplied by the prior probability of the mixture components.\nThen, the denominator for the conditional \\(p(z = k \\vert x)\\) is computed by dividing probs_z by the total probability.\nSince there are only two components in this GMM, we use logic to determine which component is most likely with the x_current. As zero-based indexing is used, the components are \\(k = 0, 1\\). Therefore, if the probability of \\(k=1 &gt; k=0\\), then use index\\(=1\\), else index\\(=0\\).\nz_current defines the component using zero-based indexing. Thus, indexing mu and scale by the current, most likely component, new samples are drawn according to the conditional Normal distribution.\nData and component samples are appended for analyzing the trace history.\n\n\n\nLimitations\nAlthough Gibbs sampling can generalize to \\(D\\) variables, the algorithm becomes inefficient as it tends to get stuck in small regions of the posterior for, potentially, a long number of iterations. This isn’t because of the large number of variables, but rather, because models with many parameters tend to have regions of high correlation in the posterior. High correlation between parameters means a narrow ridge of probability combinations, resulting in the sampler getting “stuck”."
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html",
    "href": "posts/2023-03-26-numpyro-log-joint.html",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "",
    "text": "In probabilistic programming languages (PPLs), one needs to compute the joint probability (often unnormalized) of values and observed variables under a generative model to perform approximate inference. However, given a model in the form of a Python function, how does one translate this function (model) into a log joint probability? The objective of this blog is to better understand how modern PPLs, in particular NumPyro, performs this translation in a dynamic way, i.e., the functions for performing this translation can handle a variety of models defined by the user."
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#objective",
    "href": "posts/2023-03-26-numpyro-log-joint.html#objective",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "",
    "text": "In probabilistic programming languages (PPLs), one needs to compute the joint probability (often unnormalized) of values and observed variables under a generative model to perform approximate inference. However, given a model in the form of a Python function, how does one translate this function (model) into a log joint probability? The objective of this blog is to better understand how modern PPLs, in particular NumPyro, performs this translation in a dynamic way, i.e., the functions for performing this translation can handle a variety of models defined by the user."
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#the-model",
    "href": "posts/2023-03-26-numpyro-log-joint.html#the-model",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "The Model",
    "text": "The Model\nThe example zero_inflated_poisson.py from the NumPyro docs will be used. In this example, the authors model and predict how many fish are caught by visitors in a state park. Many groups of visitors catch zero fish, either because they did not fish at all or because they were unlucky. They explicitly model this bimodal behavior (zero versus non-zero) and ascertain which variables contribute to each behavior. The authors answer this question by fitting a zero-inflated poisson regression model. We will use NUTs as the inference method to understand the model translation.\n\nWorkflow\n\nDefine model using NumPyro primitives\nConstruct a kernel for inference and feed model into kernel\nPerform inference using MCMC\n\n\n\nCode\nimport argparse\nimport os\nimport random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\n\nimport jax.numpy as jnp\nfrom jax.random import PRNGKey\nimport jax.scipy as jsp\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS, SVI, Predictive, Trace_ELBO, autoguide\nfrom numpyro.infer import util\n\nmatplotlib.use(\"Agg\")  # noqa: E402\n\n\n\n\nCode\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n\n\n\ndef model(X, Y):\n    D_X = X.shape[1]\n    b1 = numpyro.sample(\"b1\", dist.Normal(0.0, 1.0).expand([D_X]).to_event(1))\n    b2 = numpyro.sample(\"b2\", dist.Normal(0.0, 1.0).expand([D_X]).to_event(1))\n\n    q = jsp.special.expit(jnp.dot(X, b1[:, None])).reshape(-1)\n    lam = jnp.exp(jnp.dot(X, b2[:, None]).reshape(-1))\n\n    with numpyro.plate(\"obs\", X.shape[0]):\n        numpyro.sample(\"Y\", dist.ZeroInflatedPoisson(gate=q, rate=lam), obs=Y)\n\n\ndef run_mcmc(model, args, X, Y):\n    kernel = NUTS(model)\n    mcmc = MCMC(\n        kernel,\n        num_warmup=args.num_warmup,\n        num_samples=args.num_samples,\n        num_chains=args.num_chains,\n        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n    )\n    mcmc.run(PRNGKey(1), X, Y)\n    mcmc.print_summary()\n    return mcmc.get_samples()\n\n\n\nCode\ndef main(args):\n    set_seed(args.seed)\n\n    # prepare dataset\n    df = pd.read_stata(\"http://www.stata-press.com/data/r11/fish.dta\")\n    df[\"intercept\"] = 1\n    cols = [\"livebait\", \"camper\", \"persons\", \"child\", \"intercept\"]\n\n    mask = np.random.randn(len(df)) &lt; args.train_size\n    df_train = df[mask]\n    df_test = df[~mask]\n    X_train = jnp.asarray(df_train[cols].values)\n    y_train = jnp.asarray(df_train[\"count\"].values)\n    X_test = jnp.asarray(df_test[cols].values)\n    y_test = jnp.asarray(df_test[\"count\"].values)\n\n    print(\"run MCMC.\")\n    posterior_samples = run_mcmc(model, args, X_train, y_train)\n\n    predictive = Predictive(model, posterior_samples=posterior_samples)\n    predictions = predictive(PRNGKey(1), X=X_test, Y=None)\n    mcmc_predictions = jnp.rint(predictions[\"Y\"].mean(0))\n\n    print(\n        \"MCMC RMSE: \",\n        mean_squared_error(np.asarray(y_test), np.asarray(mcmc_predictions), squared=False),\n    )\n\n\n\n\nCode\nparser = argparse.ArgumentParser(\"Zero-Inflated Poisson Regression\")\nparser.add_argument(\"--seed\", nargs=\"?\", default=42, type=int)\nparser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=2000, type=int)\nparser.add_argument(\"--num-warmup\", nargs=\"?\", default=1000, type=int)\nparser.add_argument(\"--num-chains\", nargs=\"?\", default=1, type=int)\nparser.add_argument(\"--num-data\", nargs=\"?\", default=100, type=int)\nparser.add_argument(\"--maxiter\", nargs=\"?\", default=5000, type=int)\nparser.add_argument(\"--train-size\", nargs=\"?\", default=0.8, type=float)\nparser.add_argument(\"--device\", default=\"cpu\", type=str, help='use \"cpu\" or \"gpu\".')\nargs = parser.parse_args(\"\")\n\nnumpyro.set_platform(args.device)\nnumpyro.set_host_device_count(args.num_chains)\n\nmain(args)"
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#initializing-the-kernel",
    "href": "posts/2023-03-26-numpyro-log-joint.html#initializing-the-kernel",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "Initializing the kernel",
    "text": "Initializing the kernel\nFirst, we initialize the NUTS kernel with the model. The word kernel is used in a wide range of fields ranging from probabilistic programming, statistics, and deep learning. In PPLs, the name kernel is typically used to define the interface with the sampling algorithm. In this case, we have initialized a NUTS kernel with our model, and this kernel will allow us to interface our model with the underlying HMC sampling variant NUTS.\nBut, the sampling algorithm can’t simply interface with a Python function. Our model, in the form of a Python function, needs to be translated into a joint log density function and used as input into the sampler. Here, this is where NumPyro performs a series of steps to perform this translation.\nWhen we “feed” the model into the NUTS class an initialize_model utility function is called. This function calls various helper functions such as get_potential_fn and find_valid_initial_params to return a tuple of (init_params_info, potential_fn, postprocess_fn, model_trace). Here, we are interested in initialize_model and get_potential_fn.\nThe graph of function calls looks like: initialize model \\(\\leftrightarrow\\) get potential fn \\(\\leftrightarrow\\) potential energy \\(\\leftrightarrow\\) log density where each function being called is also returning an object. Below, the sequential order of functions calls are described."
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#initialize_model",
    "href": "posts/2023-03-26-numpyro-log-joint.html#initialize_model",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "initialize_model",
    "text": "initialize_model\ninitialize_model is a function that returns a tuple of objects and values used as input into the HMC algorithm. At a high level, our model and data are passed into the initialize_model function to intialize the model to some values using the observed data and numpyro.sample statements. This initialization allows us to perform inference with NUTS. Below, the various helper functions that are called within this function are described as these helpers constitute where the majority of our interest lies regarding translating a model into a log joint probability."
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#get_potential_fn",
    "href": "posts/2023-03-26-numpyro-log-joint.html#get_potential_fn",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "get_potential_fn",
    "text": "get_potential_fn\nInside of intialize_model, the function get_potential_fn is called. Given a model with Pyro primitives, this Python function returns another function which, given unconstrained parameters, evaluates the potential energy (negative log joint density). In addition, this returns a function to transform unconstrained values at sample sites to constrained values within their respective support.\nThe interesting parts here are the evaluation of potential energy and the returns a function. First, we focus on the function potential_energy to evaluate the potential energy. Later, we then return to the potential_fn object.\n\n\nCode\ndef get_potential_fn(\n    model,\n    inv_transforms,\n    *,\n    enum=False,\n    replay_model=False,\n    dynamic_args=False,\n    model_args=(),\n    model_kwargs=None,\n):\n    \"\"\"\n    (EXPERIMENTAL INTERFACE) Given a model with Pyro primitives, returns a\n    function which, given unconstrained parameters, evaluates the potential\n    energy (negative log joint density). In addition, this returns a\n    function to transform unconstrained values at sample sites to constrained\n    values within their respective support.\n\n    :param model: Python callable containing Pyro primitives.\n    :param dict inv_transforms: dictionary of transforms keyed by names.\n    :param bool enum: whether to enumerate over discrete latent sites.\n    :param bool replay_model: whether we need to replay model in\n        `postprocess_fn` to obtain `deterministic` sites.\n    :param bool dynamic_args: if `True`, the `potential_fn` and\n        `constraints_fn` are themselves dependent on model arguments.\n        When provided a `*model_args, **model_kwargs`, they return\n        `potential_fn` and `constraints_fn` callables, respectively.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :return: tuple of (`potential_fn`, `postprocess_fn`). The latter is used\n        to constrain unconstrained samples (e.g. those returned by HMC)\n        to values that lie within the site's support, and return values at\n        `deterministic` sites in the model.\n    \"\"\"\n    if dynamic_args:\n        potential_fn = partial(\n            _partial_args_kwargs, partial(potential_energy, model, enum=enum)\n        )\n        if replay_model:\n            # XXX: we seed to sample discrete sites (but not collect them)\n            model_ = seed(model.fn, 0) if enum else model\n            postprocess_fn = partial(\n                _partial_args_kwargs,\n                partial(constrain_fn, model, return_deterministic=True),\n            )\n        else:\n            postprocess_fn = partial(\n                _drop_args_kwargs, partial(transform_fn, inv_transforms)\n            )\n    else:\n        model_kwargs = {} if model_kwargs is None else model_kwargs\n        potential_fn = partial(\n            potential_energy, model, model_args, model_kwargs, enum=enum\n        )\n        if replay_model:\n            model_ = seed(model.fn, 0) if enum else model\n            postprocess_fn = partial(\n                constrain_fn,\n                model_,\n                model_args,\n                model_kwargs,\n                return_deterministic=True,\n            )\n        else:\n            postprocess_fn = partial(transform_fn, inv_transforms)\n\n    print(f\"potential_fn: {potential_fn}\")\n    return potential_fn, postprocess_fn"
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#potential_energy",
    "href": "posts/2023-03-26-numpyro-log-joint.html#potential_energy",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "potential_energy",
    "text": "potential_energy\nComputes potential energy (negative joint log density) of a model given unconstrained parameters. Under the hood, NumPyro will transform these unconstrained parameters to the values belonging to the supports of the corresponding priors in the model. To compute the potential energy, this function calls a log_density function that computes the log of joint density for the model given the latent values (parameters).\n\n\nCode\ndef potential_energy(model, model_args, model_kwargs, params, enum=False):\n    \"\"\"\n    (EXPERIMENTAL INTERFACE) Computes potential energy of a model given unconstrained params.\n    Under the hood, we will transform these unconstrained parameters to the values\n    belong to the supports of the corresponding priors in `model`.\n\n    :param model: a callable containing NumPyro primitives.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :param dict params: unconstrained parameters of `model`.\n    :param bool enum: whether to enumerate over discrete latent sites.\n    :return: potential energy given unconstrained parameters.\n    \"\"\"\n    if enum:\n        from numpyro.contrib.funsor import log_density as log_density_\n    else:\n        log_density_ = log_density\n\n    substituted_model = substitute(\n        model, substitute_fn=partial(_unconstrain_reparam, params)\n    )\n    # no param is needed for log_density computation because we already substitute\n    log_joint, model_trace = log_density_(\n        substituted_model, model_args, model_kwargs, {}\n    )\n    print(f\"-log_joint: {log_joint}\")\n    return -log_joint\n\n\nGiven our NumPyro model, data (model_args), and initialized parameters (using numpyro.sample), the potential energy (negative log joint density) is the following output:\n-log_joint: Traced&lt;ConcreteArray([ 15.773586   15.796449   15.68768    17.106882   16.307858   15.10714\n  16.277817   16.401972   17.409088   15.488239   17.44108    20.011204\n  15.796449   16.401972   17.409088   15.488239   32.46398    16.306961\n  18.321064   15.776845   16.432753   41.972443   16.90719    17.324219\n  15.487675   15.68768    15.10714    15.68768    16.96474    61.152782\n  16.319      32.46398    56.631355   16.733091   44.390583   15.796449\n  31.771408   15.9031725  17.999393   15.929144   15.796449   15.776845\n  25.234818   15.487675   32.46398    15.776845   22.136528   16.745249\n  15.796449   15.796449   61.152782   17.459694   15.776845   39.30664\n  31.771408   17.106882   15.796449   15.776845   16.836826   16.90372\n  15.565512   15.266311   15.796449   15.487675   25.503807   66.416145\n  42.01054    15.68768    16.438005   35.528217   16.401972  275.8356\n  15.488239   46.813717   18.31475    42.01054    15.929144   16.733091\n  15.929144   18.632296   16.553946   22.139755   16.879503   16.253452\n  15.929144   16.68712    16.90719    17.409088   16.306961   17.900412\n  72.883484   20.984446   17.080605   15.68768    15.266311   17.459694\n  15.487675   17.409088  221.04407    15.487675   15.68768    15.68768\n  15.903938   17.608683   17.233418   16.945618   17.102604   16.230682\n  16.401972   20.437622   16.307858   15.776845   66.416145   22.85551\n  17.459694   15.266729   18.263693   16.733091   15.68768    16.69443\n  17.767635   16.892221   16.277817   16.699389   15.796449   15.776845\n  15.68768    17.459694   18.93738    16.401972   41.471767   15.796449\n  82.16476    16.664154   15.68768    17.409088   17.106882   15.488239\n  15.266729   17.917303   26.629543   21.383934   18.279554   15.929144\n  16.90719    38.06461    16.673416   15.487675   16.253452   15.776845\n  16.306961   41.61213    15.9031725  15.488239   19.056816   30.152964\n  18.068584   15.796449   15.773586   16.216064   17.080605   16.798647\n  16.733091   16.307858   38.06461    15.487675   16.69443    66.416145\n  16.733091   39.110504   15.266729   15.796449   16.401972   18.379236\n  15.929144   16.276924   15.929144   16.306961  360.43808    15.929144\n  20.011204   15.776845   15.796449   15.487675   39.291206   15.68768\n  15.488239   30.152964   16.879503   15.929144   16.306961   16.69443\n  16.401972   16.553946   15.796449   16.673416   17.080605  379.3062\n  16.745249   17.896193   16.90719    15.266729   15.776845   15.776845 ], \n  dtype=float32)"
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#log_density",
    "href": "posts/2023-03-26-numpyro-log-joint.html#log_density",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "log_density",
    "text": "log_density\nThe log_density function first uses the effect handler substitute to return a callable which substitutes all primitive calls in fn with values from data whose key matches the site name. If the site name is not present in data, then there is no side effect. After substitute, another effect handler trace is used to record inputs, distributions, and outputs of numpyro.sample statements in the model, and NumPyro primitive calls, generally speaking.\n\n\nCode\ndef log_density(model, model_args, model_kwargs, params):\n    \"\"\"\n    (EXPERIMENTAL INTERFACE) Computes log of joint density for the model given\n    latent values ``params``.\n\n    :param model: Python callable containing NumPyro primitives.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :param dict params: dictionary of current parameter values keyed by site\n        name.\n    :return: log of joint density and a corresponding model trace\n    \"\"\"\n    \n    model = substitute(model, data=params)\n    model_trace = trace(model).get_trace(*model_args, **model_kwargs)\n\n\nThe effect handlers allow us to effectively loop through each site in the model trace to compute the joint log probability density. In the for loop, if the site type == sample grab that sites value(s) (the samples from the numpyro.sample statement) and evaluate the log probability of the value(s) for that sites fn (dist.Normal(), dist.MultivariateNormal(), etc.) with site['fn'].log_prob(&lt;some value&gt;) The output snippet below shows the site b1 defined in the model and the value sampled using the numpyro.sample statement.\nsite: {'type': 'sample', 'name': 'b1', 'fn': &lt;numpyro.distributions.distribution.Independent object at 0x13a9ceb20&gt;, 'args': (), 'kwargs': {'rng_key': None, 'sample_shape': ()}, 'value': Traced&lt;ConcreteArray([ 1.2406311  -0.5222316  -1.2795658   1.800642   -0.43796206], dtype=float32)&gt;with&lt;JVPTrace(level=2/0)&gt; with\n  primal = Array([ 1.2406311 , -0.5222316 , -1.2795658 ,  1.800642  , -0.43796206],      dtype=float32)\n  tangent = Traced&lt;ShapedArray(float32[5])&gt;with&lt;JaxprTrace(level=1/0)&gt; with\n    pval = (ShapedArray(float32[5]), None)\n    recipe = LambdaBinding(), 'scale': None, 'is_observed': False, 'intermediates': [], 'cond_indep_stack': [], 'infer': {}} \n\nvalue: Traced&lt;ConcreteArray([ 1.2406311  -0.5222316  -1.2795658   1.800642   -0.43796206], dtype=float32)&gt;with&lt;JVPTrace(level=2/0)&gt; with\n  primal = Array([ 1.2406311 , -0.5222316 , -1.2795658 ,  1.800642  , -0.43796206],      dtype=float32)\n  tangent = Traced&lt;ShapedArray(float32[5])&gt;with&lt;JaxprTrace(level=1/0)&gt; with\n    pval = (ShapedArray(float32[5]), None)\n    recipe = LambdaBinding(), \nSubsequently, we can also see the fn of this sample site defined in our model:\nsite fn: &lt;numpyro.distributions.distribution.Independent object at 0x13a9ceb20&gt;\nwhere the fn is an Independent Normal distribution because we called the .to_event(1) method in our model. Next, the log probability for the sample site value is computed by calling the .log_prob() method. For example, the log probability of the sampled values for site b1 is:\nlog prob.    = Traced&lt;ConcreteArray(-8.036343574523926, dtype=float32)\nSubsequently, we sum over the log probability for that site. Lastly, the variable log_joint is created for the log joint probability density, and the current site log probability is added to the log joint probability. After looping through each sample site, the log joint then represents the log joint probability density for the model given the latent values (parameters).\nlog joint         = Traced&lt;ConcreteArray([ -15.773586   -15.796449   -15.68768    -17.106882   -16.307858\n  -15.10714    -16.277817   -16.401972   -17.409088   -15.488239\n  -17.44108    -20.011204   -15.796449   -16.401972   -17.409088\n  -15.488239   -32.46398    -16.306961   -18.321064   -15.776845\n  -16.432753   -41.972443   -16.90719    -17.324219   -15.487675\n  -15.68768    -15.10714    -15.68768    -16.96474    -61.152782\n  -16.319      -32.46398    -56.631355   -16.733091   -44.390583\n  -15.796449   -31.771408   -15.9031725  -17.999393   -15.929144\n  -15.796449   -15.776845   -25.234818   -15.487675   -32.46398\n  -15.776845   -22.136528   -16.745249   -15.796449   -15.796449\n  -61.152782   -17.459694   -15.776845   -39.30664    -31.771408\n  -17.106882   -15.796449   -15.776845   -16.836826   -16.90372\n  -15.565512   -15.266311   -15.796449   -15.487675   -25.503807\n  -66.416145   -42.01054    -15.68768    -16.438005   -35.528217\n  -16.401972  -275.8356     -15.488239   -46.813717   -18.31475\n  -42.01054    -15.929144   -16.733091   -15.929144   -18.632296\n  -16.553946   -22.139755   -16.879503   -16.253452   -15.929144\n  -16.68712    -16.90719    -17.409088   -16.306961   -17.900412\n  -72.883484   -20.984446   -17.080605   -15.68768    -15.266311\n  -17.459694   -15.487675   -17.409088  -221.04407    -15.487675\n  -15.68768    -15.68768    -15.903938   -17.608683   -17.233418\n  -16.945618   -17.102604   -16.230682   -16.401972   -20.437622\n  -16.307858   -15.776845   -66.416145   -22.85551    -17.459694\n  -15.266729   -18.263693   -16.733091   -15.68768    -16.69443\n  -17.767635   -16.892221   -16.277817   -16.699389   -15.796449\n  -15.776845   -15.68768    -17.459694   -18.93738    -16.401972\n  -41.471767   -15.796449   -82.16476    -16.664154   -15.68768\n  -17.409088   -17.106882   -15.488239   -15.266729   -17.917303\n  -26.629543   -21.383934   -18.279554   -15.929144   -16.90719\n  -38.06461    -16.673416   -15.487675   -16.253452   -15.776845\n  -16.306961   -41.61213    -15.9031725  -15.488239   -19.056816\n  -30.152964   -18.068584   -15.796449   -15.773586   -16.216064\n  -17.080605   -16.798647   -16.733091   -16.307858   -38.06461\n  -15.487675   -16.69443    -66.416145   -16.733091   -39.110504\n  -15.266729   -15.796449   -16.401972   -18.379236   -15.929144\n  -16.276924   -15.929144   -16.306961  -360.43808    -15.929144\n  -20.011204   -15.776845   -15.796449   -15.487675   -39.291206\n  -15.68768    -15.488239   -30.152964   -16.879503   -15.929144\n  -16.306961   -16.69443    -16.401972   -16.553946   -15.796449\n  -16.673416   -17.080605  -379.3062     -16.745249   -17.896193\n  -16.90719    -15.266729   -15.776845   -15.776845 ], dtype=float32)\nAnd voila, this output is the log joint probability density (and placing a negative sign in front of this array gives you the potential energy) for the model given the latent values. The values in the output above represent the log joint probability of the initialized latent valus, i.e., no inference has been ran yet.\n\n\nCode\ndef log_density(model, model_args, model_kwargs, params):\n    \"\"\"\n    (EXPERIMENTAL INTERFACE) Computes log of joint density for the model given\n    latent values ``params``.\n\n    :param model: Python callable containing NumPyro primitives.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :param dict params: dictionary of current parameter values keyed by site\n        name.\n    :return: log of joint density and a corresponding model trace\n    \"\"\"\n    \n    model = substitute(model, data=params)\n    model_trace = trace(model).get_trace(*model_args, **model_kwargs)\n    log_joint = jnp.zeros(())\n    print(\"---- inside log_density -----\")\n    print('\\n')\n    for site in model_trace.values():\n        print(f\"site: {site}\", '\\n')\n        if site[\"type\"] == \"sample\":\n            value = site[\"value\"]\n            print(f\"value: {value}, \\n\")\n            intermediates = site[\"intermediates\"]\n            print(f\"intermediates: {intermediates}, \\n\")\n            scale = site[\"scale\"]\n            print(f\"site fn: {site['fn']}, \\n\")\n            if intermediates:\n                log_prob = site[\"fn\"].log_prob(value, intermediates)\n            else:\n                guide_shape = jnp.shape(value)\n                model_shape = tuple(\n                    site[\"fn\"].shape()\n                )  # TensorShape from tfp needs casting to tuple\n                try:\n                    broadcast_shapes(guide_shape, model_shape)\n                except ValueError:\n                    raise ValueError(\n                        \"Model and guide shapes disagree at site: '{}': {} vs {}\".format(\n                            site[\"name\"], model_shape, guide_shape\n                        )\n                    )\n                log_prob = site[\"fn\"].log_prob(value)\n\n            if (scale is not None) and (not is_identically_one(scale)):\n                log_prob = scale * log_prob\n\n            # print(f\"before sum log prob.    = {log_prob}, \\n\")\n            # log_prob = jnp.sum(log_prob)\n            # print(f\"after sum log prob.     = {log_prob}, \\n\")\n            log_joint = log_joint + log_prob\n            print(f\"log joint               = {log_joint}, \\n\")\n    return log_joint, model_trace"
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#returning-to-get_potential_fn",
    "href": "posts/2023-03-26-numpyro-log-joint.html#returning-to-get_potential_fn",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "Returning to get_potential_fn",
    "text": "Returning to get_potential_fn\nHowever, log_density and potential_energy compute the log probability of the current latent value, not a function. Therefore, we return to the Python function get_potential_fn that returns the log joint probability as a function potential_fn, i.e., a function that will evaluate the potential energy given the model args defined by our model, i.e., X_train and y_train and the latent values using the log_density function described above. The potential_fn is then “fed” into the HMC algorithm to perform inference.\nWhat’s great about get_potential_fn is that the log joint probability density function can be accessed externally given our NumPyro model, and passed into other sampling libraries such as Blackjax."
  },
  {
    "objectID": "posts/2022-10-07-inference-mc-approximation.html",
    "href": "posts/2022-10-07-inference-mc-approximation.html",
    "title": "Inference - Monte Carlo Approximation",
    "section": "",
    "text": "In the probabilistic approach to machine learning, all unknown quantities—predictions about the future, hidden states of a system, or parameters of a model—are treated as random variables, and endowed with probability distributions. The process of inference corresponds to computing the posterior distribution over these quantities, conditioning on whatever data is available. Given that the posterior is a probability distribution, we can draw samples from it. The samples in this case are parameter values. The Bayesian formalism treats parameter distributions as the degrees of relative plausibility, i.e., if this parameter is chosen, how likely is the data to have arisen? We use Bayes’ rule for this process of inference. Let \\(h\\) represent the uknown variables and \\(D\\) the known variables, i.e., the data. Given a likelihood \\(p(D|h)\\) and a prior \\(p(h)\\), we can compute the posterior \\(p(h|D)\\) using Bayes’ rule:\n\\[p(h|D) = \\frac{p(D|h)p(h)}{p(D)}\\]\nThe main problem is the \\(p(D)\\) in the demoninator. \\(p(D)\\) is a normalization constant and ensures the probability distribution sums to 1. When the number of unknown variables \\(h\\) is large, computing \\(p(D)\\) requires a high dimensional integral of the form:\n\\[p(D) = \\int p(D|h)p(h)dh\\]\nThe integral is needed to convert the unnormalized joint probability of some parameter value \\(p(h, D)\\) to a normalized probability \\(p(h|D)\\). This also allows us to take into account all the other plausible values of \\(h\\) that could have generated the data. There are three ways for computing the posterior:\n\nAnalytical Solution\nGrid Approximation\nApproximate Inference\n\nMany problems are complex and require a model where computing the posterior distribution using a grid of parameters or in exact mathematical form is not feasible (or possible). Therefore, you adopt the approximate inference / sampling approach. The sampling approach has a major benefit. Working with samples transforms a problem in calculus \\(\\rightarrow\\) into a problem of data summary \\(\\rightarrow\\) into a frequency format problem. An integral in a typical Bayesian context is just the total probability in some interval. Once you have samples from the probability distribution, it’s just a matter of counting values in the interval. Therefore, once you fit a model to the data using some sampling algorithm, then interpreting the model is a matter of interpreting the frequency of parameter samples (though this is easier said than done).\nTo gain a better conceptual understanding of algorithmic techniques for computing (approximate) posteriors, I will be diving deeper into the main inference algorithms over the next couple of posts.\n\n\nAs discussed above, it is often difficult to compute the posterior distribution analytically. In this example, suppose \\(x\\) is a random variable, and \\(y = f(x)\\) is some function of \\(x\\). Here, \\(y\\) is our target distribution (think the posterior). Instead of computing \\(p(y)\\) analytically, it is possible to draw a large number of samples from \\(p(x)\\), and then use these samples to approximate \\(p(y)\\).\nIf \\(x\\) is distributed uniformly in an interval between \\(-1, 1\\) and \\(y = f(x) = x^2\\), we can approximate \\(p(y)\\) by drawing samples from \\(p(x)\\). By using a large number of samples, a good approximation can be computed.\n\n\nCode\ndef plot_mc(x_samples, probs_x, true_y, pdf_y, approx_y):\n\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 4))\n\n    ax[0].set_title('True Uniform Distribution')\n    ax[0].plot(x_samples, probs_x)\n    ax[0].set_xlabel('$x$')\n    ax[0].set_ylabel('$p(x)$')\n\n    ax[1].set_title('True $y$ PDF')\n    ax[1].plot(true_y, pdf_y)\n    ax[1].set_xlabel('$y$')\n    ax[1].set_ylabel('$p(y)$')\n\n    ax[2].set_title('Approximated $y$ PDF')\n    ax[2].hist(approx_y, bins=30, density=True)\n    ax[2].set_xlabel('$y$')\n    ax[2].set_ylabel('$p(y)$')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\ndef main():\n    \n    square_func = lambda x: x**2\n\n    # True p(x) \n    lower, upper = -1, 1\n    x_samples = np.linspace(lower, upper, 200)\n    \n    # Analytical solution\n    probs_x = 1 / (upper - lower) * np.ones(len(x_samples)) # p(X = x)\n    true_y = square_func(x_samples) # true output y\n    pdf_y = 1 / (2 * np.sqrt(true_y + 1e-2)) # true pdf of output y\n\n    # Approximation p(y) \n    uniform_samples = np.random.uniform(-1, 1, 1000) # sample from Uniform\n    approx_y = square_func(uniform_samples) # approx. output y\n\n    plot_mc(x_samples, probs_x, true_y, pdf_y, approx_y)\n\nmain()\n\n\n\n\n\n\n\n\nFirst, define the squaring function \\(f(x)\\) as square_func\nx_samples is an array of 200 samples in the interval \\([-1, 1]\\)\nThe probability of each element in x_samples: \\(p(X=x)\\) is computed\nCompute true_y using the known x_samples\nCompute the empirical probability density of the output true_y.\nDraw 1000 samples from a Uniform distribution\nUse these samples to approximate approx_y the empirical probability density"
  },
  {
    "objectID": "posts/2022-10-07-inference-mc-approximation.html#inference",
    "href": "posts/2022-10-07-inference-mc-approximation.html#inference",
    "title": "Inference - Monte Carlo Approximation",
    "section": "",
    "text": "In the probabilistic approach to machine learning, all unknown quantities—predictions about the future, hidden states of a system, or parameters of a model—are treated as random variables, and endowed with probability distributions. The process of inference corresponds to computing the posterior distribution over these quantities, conditioning on whatever data is available. Given that the posterior is a probability distribution, we can draw samples from it. The samples in this case are parameter values. The Bayesian formalism treats parameter distributions as the degrees of relative plausibility, i.e., if this parameter is chosen, how likely is the data to have arisen? We use Bayes’ rule for this process of inference. Let \\(h\\) represent the uknown variables and \\(D\\) the known variables, i.e., the data. Given a likelihood \\(p(D|h)\\) and a prior \\(p(h)\\), we can compute the posterior \\(p(h|D)\\) using Bayes’ rule:\n\\[p(h|D) = \\frac{p(D|h)p(h)}{p(D)}\\]\nThe main problem is the \\(p(D)\\) in the demoninator. \\(p(D)\\) is a normalization constant and ensures the probability distribution sums to 1. When the number of unknown variables \\(h\\) is large, computing \\(p(D)\\) requires a high dimensional integral of the form:\n\\[p(D) = \\int p(D|h)p(h)dh\\]\nThe integral is needed to convert the unnormalized joint probability of some parameter value \\(p(h, D)\\) to a normalized probability \\(p(h|D)\\). This also allows us to take into account all the other plausible values of \\(h\\) that could have generated the data. There are three ways for computing the posterior:\n\nAnalytical Solution\nGrid Approximation\nApproximate Inference\n\nMany problems are complex and require a model where computing the posterior distribution using a grid of parameters or in exact mathematical form is not feasible (or possible). Therefore, you adopt the approximate inference / sampling approach. The sampling approach has a major benefit. Working with samples transforms a problem in calculus \\(\\rightarrow\\) into a problem of data summary \\(\\rightarrow\\) into a frequency format problem. An integral in a typical Bayesian context is just the total probability in some interval. Once you have samples from the probability distribution, it’s just a matter of counting values in the interval. Therefore, once you fit a model to the data using some sampling algorithm, then interpreting the model is a matter of interpreting the frequency of parameter samples (though this is easier said than done).\nTo gain a better conceptual understanding of algorithmic techniques for computing (approximate) posteriors, I will be diving deeper into the main inference algorithms over the next couple of posts.\n\n\nAs discussed above, it is often difficult to compute the posterior distribution analytically. In this example, suppose \\(x\\) is a random variable, and \\(y = f(x)\\) is some function of \\(x\\). Here, \\(y\\) is our target distribution (think the posterior). Instead of computing \\(p(y)\\) analytically, it is possible to draw a large number of samples from \\(p(x)\\), and then use these samples to approximate \\(p(y)\\).\nIf \\(x\\) is distributed uniformly in an interval between \\(-1, 1\\) and \\(y = f(x) = x^2\\), we can approximate \\(p(y)\\) by drawing samples from \\(p(x)\\). By using a large number of samples, a good approximation can be computed.\n\n\nCode\ndef plot_mc(x_samples, probs_x, true_y, pdf_y, approx_y):\n\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 4))\n\n    ax[0].set_title('True Uniform Distribution')\n    ax[0].plot(x_samples, probs_x)\n    ax[0].set_xlabel('$x$')\n    ax[0].set_ylabel('$p(x)$')\n\n    ax[1].set_title('True $y$ PDF')\n    ax[1].plot(true_y, pdf_y)\n    ax[1].set_xlabel('$y$')\n    ax[1].set_ylabel('$p(y)$')\n\n    ax[2].set_title('Approximated $y$ PDF')\n    ax[2].hist(approx_y, bins=30, density=True)\n    ax[2].set_xlabel('$y$')\n    ax[2].set_ylabel('$p(y)$')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\ndef main():\n    \n    square_func = lambda x: x**2\n\n    # True p(x) \n    lower, upper = -1, 1\n    x_samples = np.linspace(lower, upper, 200)\n    \n    # Analytical solution\n    probs_x = 1 / (upper - lower) * np.ones(len(x_samples)) # p(X = x)\n    true_y = square_func(x_samples) # true output y\n    pdf_y = 1 / (2 * np.sqrt(true_y + 1e-2)) # true pdf of output y\n\n    # Approximation p(y) \n    uniform_samples = np.random.uniform(-1, 1, 1000) # sample from Uniform\n    approx_y = square_func(uniform_samples) # approx. output y\n\n    plot_mc(x_samples, probs_x, true_y, pdf_y, approx_y)\n\nmain()\n\n\n\n\n\n\n\n\nFirst, define the squaring function \\(f(x)\\) as square_func\nx_samples is an array of 200 samples in the interval \\([-1, 1]\\)\nThe probability of each element in x_samples: \\(p(X=x)\\) is computed\nCompute true_y using the known x_samples\nCompute the empirical probability density of the output true_y.\nDraw 1000 samples from a Uniform distribution\nUse these samples to approximate approx_y the empirical probability density"
  },
  {
    "objectID": "posts/2023-06-10-gsoc-week-2.html",
    "href": "posts/2023-06-10-gsoc-week-2.html",
    "title": "Google Summer of Code - Update 1",
    "section": "",
    "text": "I have been selected as a contributor under the organization NumFOCUS for the Google Summer of Code 2023 program. I will be working on developing better tools to interpret complex Bambi regression models. More details about the project can be found here. This blog post will be a summary of the community bonding period and the first two weeks of the program.\nTimeline of events:\n\nCommunity Bonding Period (May 4th - 28th)\nWeek 1 (May 29th - June 4th)\nWeek 2 (June 5th - June 11th)"
  },
  {
    "objectID": "posts/2023-06-10-gsoc-week-2.html#introduction",
    "href": "posts/2023-06-10-gsoc-week-2.html#introduction",
    "title": "Google Summer of Code - Update 1",
    "section": "",
    "text": "I have been selected as a contributor under the organization NumFOCUS for the Google Summer of Code 2023 program. I will be working on developing better tools to interpret complex Bambi regression models. More details about the project can be found here. This blog post will be a summary of the community bonding period and the first two weeks of the program.\nTimeline of events:\n\nCommunity Bonding Period (May 4th - 28th)\nWeek 1 (May 29th - June 4th)\nWeek 2 (June 5th - June 11th)"
  },
  {
    "objectID": "posts/2023-06-10-gsoc-week-2.html#community-bonding-period",
    "href": "posts/2023-06-10-gsoc-week-2.html#community-bonding-period",
    "title": "Google Summer of Code - Update 1",
    "section": "Community Bonding Period",
    "text": "Community Bonding Period\nThe community bonding period is a time for students to get to know their mentors and organizations, and to change / finalize project details. I have been in contact with my mentors Tomás Capretto and Osvaldo Martin via Slack and GitHub. A goal of mine throughout the community bonding period was not only to get to know my mentors, but also to familiarize myself with the Bambi codebase before coding officially began. To achieve this, I read through the codebase attempting to understand how the different modules interact with one another. Additionally, I read through the Bambi documentation to understand how the different models are implemented, and any additional functionality provided the library. After familarizing myself with the codebase at a high level, I decided to get a jumpstart on the project deliverables for week 1 through 4. Below is a table of the deliverables for the entire project timeline.\n\n\n\n\n\n\n\nWeek\nDeliverable\n\n\n\n\n1\nReview plot_cap() design and relevant Bambi codebase. Discuss grammar of graphics libraries with mentors. Identify any open GitHub issues regarding plot_cap() that would restrict development.\n\n\n2\nImplement predictions at the observational level for plot_cap(), and add tests, and documentation.\n\n\n3\nDesign of how plot_comparisons() and plot_slopes() can re-utilize the plot_cap() code and be incorporated into the Bambi codebase.\n\n\n4\nBegin plot_comparisons() implementation\n\n\n5\nDeliver working example of plot_comparisons() for a simple Bambi model, and open draft PR.\n\n\n6\nContinue plot_comparisons() implementation. Deliver example on a complex model.\n\n\n7\nWrite tests, examples, and documentation for plot_comparisons(), and open PR.\n\n\n8\nBegin plot_slopes() implementation.\n\n\n9\nDeliver working example of plot_slopes() for a simple Bambi model, and open draft PR.\n\n\n10\nContinue plot_slopes() implementation. Deliver example on a complex model.\n\n\n11\nWrite tests, examples, and documentation for plot_slopes(), and open PR.\n\n\n12\nReview plot_cap(), plot_comparisons(), and plot_slopes() code. Ensure documentation and examples are correct, and tests pass.\n\n\n13\nFinal submission. This week is a buffer period for code review, unexpected difficulties, and ensuring documentation and added examples are accurate and comprehensive."
  },
  {
    "objectID": "posts/2023-06-10-gsoc-week-2.html#progress",
    "href": "posts/2023-06-10-gsoc-week-2.html#progress",
    "title": "Google Summer of Code - Update 1",
    "section": "Progress",
    "text": "Progress\nPredictions at the observation level for plot_cap were implemented in PR 668. By default, plot_cap uses the posterior distribution to visualize some mean outcome parameter of a a GLM. However, the posterior predictive distribution can also be plotted by specifying pps=True where pps stands for posterior predictive samples of the response variable. Upon completion of the PR above, an example showing the functionality of plot_cap was added to the Bambi documentation in PR 670.\nCompleting these two deliveribles early on in the program allowed me to spend more time on the design of the plots sub-package and implementation of plot_comparisons (which is ending up to be more complex than initially thought). For the design of the plots sub-package, I decided to create five different modules, each with a specific purpose. The modules are as follows:\n\ncreate_data.py - creates the data called by the functions in the effects.py and comparisons.py modules.\neffects.py - contains the functions (predictions, comparisons, slopes) that are used to create the dataframe that is passed to the plotting functions and or user when called standalone.\nutils.py - contains commonly used functions that are called by multiple modules in the plots sub-package.\nplotting.py - contains the plotting functions (plot_predictions, plot_comparisons, plot_slopes) that are used to generate the plots for the user.\nplot_types.py - determines the plot types (numeric or categorical) for the the plotting module.\n\nThe modularization of the plots sub-package will allow for easier maintenance and testing of the codebase and offers re-usability of code using OOP principles. The modularization of code happened during the initial development stages of the plot_comparisons function.\nCurrently, plot_comparisons only works when a user defines the covariate and its contrast, and the covariates they would like to condition on. For example:\n# user provided contrast for `Age` and default grid \n# calculated for the conditional variable `SexCode`\nfig, ax = plot_comparison(\n    model=titanic_model,\n    idata=titanic_idata,\n    contrast_predictor={\"Age\": [50, 70]},\n    conditional=\"SexCode\",\n)\nor\n# user can also pass a dict into conditional to define the \n# values used for the conditional variables\nfig, ax = plot_comparison(\n    model=titanic_model,\n    idata=titanic_idata,\n    contrast_predictor={\"PClass\": [1, 3]},\n    conditional={\"Age\": [50], \"SexCode\": [0, 1]}\n)\nNext week, default contrast values will be computed for numeric and categorical conrast predictors. Additionally, default values for both numeric and categorical conditinoal variables will be added. This will allow the user to call plot_comparisons without having to specify contrast and conditional variable values."
  },
  {
    "objectID": "posts/2022-05-31-Probabilistic-Predicion-Problems-Part-1.html",
    "href": "posts/2022-05-31-Probabilistic-Predicion-Problems-Part-1.html",
    "title": "Probabilistic Prediction Problems - Part 1",
    "section": "",
    "text": "Part one is a collection of notes on the problems with probabilistic predictions."
  },
  {
    "objectID": "posts/2022-05-31-Probabilistic-Predicion-Problems-Part-1.html#distributions-over-actions",
    "href": "posts/2022-05-31-Probabilistic-Predicion-Problems-Part-1.html#distributions-over-actions",
    "title": "Probabilistic Prediction Problems - Part 1",
    "section": "Distributions over Actions",
    "text": "Distributions over Actions\nIt is possible to assume the set of possible actions is to pick a class label (or “reject” option) in classification or a real valued scalar as in a point estimate of a parameter. However, it is also possible to assume the set of possible actions is to pick a probability distribution over some value of interest (paramter or class label). That is, we want to perform probabilistic prediction or probabilistic forecasting rather than predicting a scalar. From a decision theoretic approach, we assume the true state of nature is a distribution, \\(h = p(Y \\vert x)\\), with the action being another distribution, \\(a = q(Y \\vert x)\\). The goal is to be as close to the true state of nature \\(p\\) with our approximation \\(q\\). Therefore, we want to pick \\(q\\) to minimize \\(\\mathbb{E}\\ell(p, q)\\) for a given \\(x\\).\nWhen making a prediction, the accuracy depends upon the definition of the target, and there is no universal best target. Rather, a decision-theoretic approach should be taken, in particular because of the following two dimensions:\n\nCost benefit analysis - What’s the risk (or cost) we face when we are wrong? What’s the payoff when we are right?\nContext - Some prediction tasks are inheritently harder than others. Therefore, how can we judge how much a model can possibly improve prediction?"
  },
  {
    "objectID": "posts/2022-05-31-Probabilistic-Predicion-Problems-Part-1.html#kl-divergence-cross-entropy-and-log-loss",
    "href": "posts/2022-05-31-Probabilistic-Predicion-Problems-Part-1.html#kl-divergence-cross-entropy-and-log-loss",
    "title": "Probabilistic Prediction Problems - Part 1",
    "section": "KL Divergence, Cross-Entropy, and Log-Loss",
    "text": "KL Divergence, Cross-Entropy, and Log-Loss\nIf we want to compare two distributions, a common loss function is Kullback-Leibler Divergence (KL Divergence).\n\nProper Scoring Rules\nThe key property of proper scoring rules is that the loss function is minimized i.f.f the decision maker picks the distribution \\(q\\) that matches the true distribution \\(p\\). Such a loss function \\(\\ell\\) is a proper scoring rule.\nMaximizing a proper scoring rule will force the model to match the true probabilities, i.e., the probabilities are calibrated. For example, when the weather person states the probability of rain is \\(70\\%\\), then it should rain about \\(70\\%\\) of the time.\nThe following are proper scoring “loss functions”:\n\nNegative log-likelihood - negative of the log-likelihood because maximization is for losers\n\nlog scoring rule - log of the joint probability\n\nCross entropy -\n\nlog-loss\n\nBrier score -\n\n\n\nInformation and Uncertainty\nWe want to use the log probability of the data to score the accuracy of competing models"
  },
  {
    "objectID": "posts/2022-10-08-inference-mh.html",
    "href": "posts/2022-10-08-inference-mh.html",
    "title": "Inference - Metropolis Hastings from Scratch",
    "section": "",
    "text": "Main Idea\nMetropolis-Hastings (MH) is one of the simplest kinds of MCMC algorithms. The idea with MH is that at each step, we propose to move from the current state \\(x\\) to a new state \\(x'\\) with probability \\(q(x'|x)\\), where \\(q\\) is the proposal distribution. The user is free to choose the proposal distribution and the choice of the proposal is dependent on the form of the target distribution. Once a proposal has been made to move to \\(x'\\), we then decide whether to accept or reject the proposal according to some rule. If the proposal is accepted, the new state is \\(x'\\), else the new state is the same as the current state \\(x\\).\nProposals can be symmetric and asymmetric. In the case of symmetric proposals \\(q(x'|x) = q(x|x')\\), the acceptance probability is given by the rule:\n\\[A = min(1, \\frac{p^*(x')}{p^*(x)})\\]\nThe fraction is a ratio between the probabilities of the proposed state \\(x'\\) and current state \\(x\\). If \\(x'\\) is more probable than \\(x\\), the ratio is \\(&gt; 1\\), and we move to the proposed state. However, if \\(x'\\) is less probable, we may still move there, depending on the relative probabilities. If the relative probabilities are similar, we may code exploration into the algorithm such that they go in the opposite direction. This helps with the greediness of the original algorithm—only moving to more probable states.\n\n\nThe Algorithm\n\nInitialize \\(x^0\\)\nfor \\(s = 0, 1, 2, 3, ...\\) do:\n\nDefine \\(x = x^s\\)\nSample \\(x' \\sim q(x'|x)\\) where \\(q\\) is the user’s proposal distribution\nCompute the acceptance probability given by:\n\\(p_a(x_{t+1}|x_i) = min(1, \\frac{p(x_{i+1})q(x_i | x_{i+1})}{p(x_i)q(x_{i+1}|x_I)})\\)\n\nCompute \\(A = min(1, \\alpha)\\)\nSample \\(u \\sim U(0, 1)\\)\nSet new sample to: \\(x^{s+1} = \\left\\{ \\begin{array}{ll}  x' & \\quad \\text{if} \\quad u \\leq A(\\text{accept}) \\\\  x & \\quad \\text{if} \\quad x &gt; A(\\text{reject}) \\\\ \\end{array} \\right.\\)\n\n\n\nRandom Walk Metropolis-Hastings\nThe random walk metropolis-hastings (RWMH) corresponds to MH with a Gaussian propsal distribution of the form:\n\\[q(x'|x) = \\mathcal{N}(x'|x, \\tau^2 I)\\]\nBelow, I implement the RWMH for sampling from a 1-dimenensional mixture of Gaussians (implemented using the MixtureSameFamily PyTorch class) with the following parameters:\n\n\\(\\mu = -20, 20\\)\nMixture component probability \\(= 0.3, 0.7\\)\n\\(\\sum = 10, 10\\)\n\n\n\nCode\ndef plot(distribution, trace_history, xmin, xmax, n_iterations, n_evals=500):\n\n    x_evals = torch.linspace(xmin, xmax, n_evals)\n    evals = torch.exp(distribution.log_prob(x_evals))\n    \n    fig = plt.figure(figsize=(12, 4))\n\n    ax = fig.add_subplot(1, 2, 1)\n    ax.plot(torch.arange(n_iterations), trace_history)\n    ax.set_xlabel('Iterations')\n    ax.set_ylabel('Sampled Value')\n\n    ax = fig.add_subplot(1, 2, 2, projection='3d')\n    ax.plot(torch.arange(n_iterations), trace_history)\n    ax.plot(torch.zeros(n_evals), x_evals, evals)\n    ax.set_xlabel('Iterations')\n    ax.set_ylabel('Sampled Value')\n\n    fig.suptitle('Random Walk Metropolis-Hastings')\n    plt.show()\n\n\ndef true_distribution(mixture_probs, mus, scales):\n\n    return dist.MixtureSameFamily(\n        mixture_distribution=dist.Categorical(probs=mixture_probs),\n        component_distribution=dist.Normal(loc=mus, scale=scales)\n        )\n\n\n\ndef metropolis_hasting(x0, tau, mixture, n_iterations, rng_key=None):\n    \"\"\"\n    implements the random walk metropolis-hasting algorithm\n    \"\"\"\n\n    x_current = x0\n    x_samples = torch.zeros(n_iterations)\n    x_samples[0] = x_current\n    cnt_acceptance = 0\n     \n    for n in range(1, n_iterations):\n        \n        # datum of proposed state x'\n        x_candidate = x_current + tau * dist.Normal(loc=0, scale=1).sample()\n        # probs. of proposed state x'\n        p_candidate = torch.exp(mixture.log_prob(x_candidate))\n        # probs. of current state x\n        p_current = torch.exp(mixture.log_prob(x_current))\n        \n        # acceptance formula\n        alpha = p_candidate / p_current\n        probs_accept = min(1, alpha)\n        \n        # sample u ~ U(0, 1)\n        u = dist.Uniform(0, 1).sample()\n\n        if u &gt;= probs_accept:\n            x_current = x_current\n        else:\n            x_current = x_candidate\n            cnt_acceptance += 1\n\n        x_samples[n] = x_current\n\n    acceptence_ratio = cnt_acceptance / n_iterations\n    print('---- statistics ----')\n    print(f'acceptance rate = {acceptence_ratio}')\n\n    return x_samples\n\n\ndef main(args):\n\n    # initial parameter value\n    x0 = torch.tensor(20.)\n\n    # mixture dist. parameters\n    mixture_probs = torch.tensor([0.3, 0.7])\n    mus = torch.tensor([-20., 20.])\n    scales = torch.tensor([10., 10.])\n\n    n_iters = args.iters\n    tau = torch.tensor(args.tau)\n\n    mixture_distribution = true_distribution(mixture_probs, mus, scales)\n    x_samples = metropolis_hasting(x0, tau, mixture_distribution, n_iters)\n\n    plot(mixture_distribution, x_samples, -100, 100, n_iters)\n\n\nparser = argparse.ArgumentParser(description='rw-mh')\nparser.add_argument('--iters', type=int, default=1000)\nparser.add_argument('--tau', type=float, default=8.)\nargs = parser.parse_args(\"\")\n\nmain(args)\n\n---- statistics ----\nacceptance rate = 0.803\n\n\n\n\n\n\n\nResults\nThe mixture distribution can be tricky to sample from as it is has more than one model, i.e., it is a bimodal distribution. However, we can see that the RWMH spends time sampling from both component distributions, albeit, the distribution with the higher probability more. Due to the random search based perturbations (random walk), the sampler seems to randomly jump from component to component, showing that the chain is not sticky. Additionally, the acceptance rate is \\(0.803\\) indicating that about \\(80\\%\\) of new proposals were accepted."
  },
  {
    "objectID": "posts/2022-06-23-ELBO.html",
    "href": "posts/2022-06-23-ELBO.html",
    "title": "Variational Inference - ELBO",
    "section": "",
    "text": "We don’t know the real posterior so we are going to choose a distribution \\(Q(\\theta)\\) from a family of distributions \\(Q^*\\) that are easy to work with and parameterized by \\(\\theta\\). The approximate distribution should be as close as possible to the true posterior. This closeness is measured using KL-Divergence. If we have the joint \\(p(x, z)\\) where \\(x\\) is some observed data, the goal is to perform inference: given what we have observed, what can we infer about the latent states?, i.e , we want the posterior.\nRecall Bayes theorem:\n\\[p(z | x) = \\frac{p(x|z)p(z)}{p(x)}\\]\nThe problem is the marginal \\(p(x = D)\\) as this could require a hundred, thousand, . . .dimensional integral:\n\\[p(x) = \\int_{z_0},...,\\int_{z_{D-1}}p(x, z)dz_0,...,d_{z_D{-1}}\\]\nIf we want the full posterior and can’t compute the marginal, then what’s the solution? Surrogate posterior. We want to approximate the true posterior using some known distribution:\n\\[q(z) \\approx p(z|X=D)\\]\nwhere \\(\\approx\\) can mean you want the approximated posterior to be “as good as possible”. Using variational inference, the objective is to minimize the distance between the surrogate \\(q(z)\\) and the true posterior \\(p(x)\\) using KL-Divergence:\n\\[q^*(z) = argmin_{q(z) \\in Q} (KL(q(z) || p(z|x=D)))\\]\nwhere \\(Q\\) is a more “simple” distribution. We can restate the KL-divergence as the expectation:\n\\[KL(q(z) || p(z|D)) = \\mathbb{E_{z \\sim q(z)}}[log \\frac{q(z)}{p(z|D)}]\\]\nwhich, taking the expectation over \\(z\\), is equivalent to integration:\n\\[\\int_{z_0}, . . .,\\int_{z_{D-1}}q(z)log\\frac{q(z)}{p(z|D)}d_{z_0},...,d_{z_{D-1}}\\]\nBut, sadly we don’t have \\(p(z \\vert D)\\) as this is the posterior! We only have the joint. Solution? Recall our KL-divergence:\n\\[KL(q(z) || p(z|D))\\]\nWe can rearrange the terms inside the \\(log\\) so that we can actually compute something:\n\\[\\int_{z}q(z)log(\\frac{q(z)p(D)}{p(z, D)})dz\\]\nWe only have the joint. Not the posterior; nor the marginal. We know from Bayes rule that we can express the posterior in terms of the joint \\(p(z, D)\\) divided by the marginal \\(p(x=D)\\):\n\\[p(z|D) = \\frac{p(Z, D)}{p(D)}\\]\nWe plug this inside of the \\(log\\):\n\\[\\int_{z}q(z)log(\\frac{q(z)p(D)}{p(z, D)})dz\\]\nHowever, the problem now is that we have reformulated our problem into another quantity that we don’t have, i.e., the marginal \\(p(D)\\). But we can put the quantity that we don’t have outside of the \\(log\\) to form two separate integrals.\n\\[\\int_z q(z)log(\\frac{q(z)}{p(z, D)})dz + \\int_zq(z)log(p(D)dz\\]\nThis is a valid rearrangement because of the properties of logarithms. In this case, the numerator is a product, so this turns into a sum of the second integral. What do we see in these two terms? We see an expectation over the quantity \\(\\frac{q(z)}{p(z, D)}\\) and another expectation over \\(p(D)\\). Rewriting in terms of expectation:\n\\[\\mathbb{E_{z{\\sim q(z)}}}[log(\\frac{q(z)}{p(z, D)})] + \\mathbb{E_{z \\sim q(z)}}[log(p(D))]\\]\nThe right term contains information we know—the functional form of the surrogate \\(q(z)\\) and the joint \\(p(z, D)\\) (in the form of a directed graphical model). We still don’t have access to \\(p(D)\\) on the right side, but this is a constant quantity. The expectation of a quantity that does not contain \\(z\\) is just whatever the expectation was taken over. Because of this, we can again rearrange:\n\\[-\\mathbb{E_{z \\sim q(z)}}[log \\frac{p(z, D)}{q(z)}]+log (p(D))\\]\nThe minus sign is a result of the “swapping” of the numerator and denominator and is required to make it a valid change. Looking at this, the left side is a function dependent on \\(q\\). In shorthand form, we can call this \\(\\mathcal{L(q)}\\). Our KL-divergence is:\n\\[KL = \\mathcal{-L(q)} + \\underbrace{log(p(D))}_\\textrm{evidence}\\]\nwhere \\(p(D)\\) is a value between \\([0, 1]\\) and this value is called the evidence which is the log probability of the data. If we apply the \\(log\\) to something between \\([0, 1]\\) then this value will be negative. This value is also constant since we have observed the dataset and thus does not change.\n\\(KL\\) is the distance (between the posterior and the surrogate) so it must be something positive. If the \\(KL\\) is positive and the evidence is negative, then in order to fulfill this equation, \\(\\mathcal{L}\\) must also be negative (negative times a negative is a positive). The \\(\\mathcal{L}\\) should be smaller than the evidence, and thus it is called the lower bound of the evidence \\(\\rightarrow\\) Evidence Lower Bound (ELBO).\nAgain, ELBO is defined as: \\(\\mathcal{L} = \\mathbb{E_{z \\sim q(z)}}[log(\\frac{p(z, D)}{q(z)})]\\) and is important to note that the ELBO is equal to the evidence if and only if the KL-divergence between the surrogate and the true posterior is \\(0\\):\n\\[\\mathcal{L(q)} = log(p(D)) \\textrm{ i.f.f. } KL(q(z)||p(z|D))=0\\]"
  },
  {
    "objectID": "posts/2022-07-23-bmcp-ch-5.html",
    "href": "posts/2022-07-23-bmcp-ch-5.html",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 5",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport arviz as az\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions import constraints, transforms\nfrom pyro.infer import Predictive, TracePredictive, NUTS, MCMC\nfrom pyro.infer.autoguide import AutoLaplaceApproximation\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\nfrom patsy import dmatrix\nplt.style.use('ggplot')\nplt.rcParams[\"figure.figsize\"] = (9, 4)"
  },
  {
    "objectID": "posts/2022-07-23-bmcp-ch-5.html#fitting-splines-in-pyro",
    "href": "posts/2022-07-23-bmcp-ch-5.html#fitting-splines-in-pyro",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 5",
    "section": "5.5 - Fitting splines in Pyro",
    "text": "5.5 - Fitting splines in Pyro\n\nday = pd.read_csv('./data/Bike-Sharing-Dataset/day.csv')\nhour = pd.read_csv('./data/Bike-Sharing-Dataset/hour.csv')\nhour['cnt_std'] = hour['cnt'] / hour['cnt'].max()\n\n\nsns.scatterplot(x=hour['hr'], y=hour['cnt_std'], alpha=0.1, color='grey')\nplt.ylabel('Count')\nplt.xlabel('Hour of Day (0-23)')\nplt.title('Actual Bike Demand');\n\n\n\n\n\nnum_knots = 6\nknot_list = torch.linspace(0, 23, num_knots + 2)[1:-1]\n\nB = dmatrix(\n    \"bs(cnt_std, knots=knots, degree=3, include_intercept=True) - 1\",\n    {'cnt_std': hour.hr.values, 'knots': knot_list[1:-1]}\n)\n\nB = torch.tensor(np.asarray(B)).float()\ncnt_bikes = torch.tensor(hour['cnt_std'].values).float()\nhour_bikes = torch.tensor(hour['hr'].values).reshape(-1, 1).float()\n\n\nSplines Model - MCMC\n\ndef splines(design_matrix, count_bikes=None):\n\n    N, P = design_matrix.shape\n\n    tau = pyro.sample('tau', dist.HalfCauchy(1.))\n    sigma = pyro.sample('sigma', dist.HalfNormal(1.))\n\n    with pyro.plate('knot_list', P):\n        beta = pyro.sample('beta', dist.Normal(0., tau))\n\n    mu = pyro.deterministic('mu', torch.matmul(beta, design_matrix.T))\n\n    with pyro.plate('output', N):\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=count_bikes)\n\n\npyro.render_model(\n    splines, (B, cnt_bikes), render_distributions=True\n)\n\n\n\n\n\nkernel = NUTS(splines)\nmcmc_splines = MCMC(kernel, 500, 300)\nmcmc_splines.run(B, cnt_bikes)\n\nSample: 100%|██████████| 800/800 [00:12, 62.91it/s, step size=2.93e-01, acc. prob=0.887]\n\n\n\nprior_predictive = Predictive(splines, num_samples=500)(B, None)\nspline_samples = mcmc_splines.get_samples(500)\nsplines_predictive = Predictive(splines, spline_samples)(B, None)\n\naz_splines_pred = az.from_pyro(\n    prior=prior_predictive,\n    posterior=mcmc_splines, \n    posterior_predictive=splines_predictive\n    )\n\n\nsns.lineplot(\n    x=hour_bikes.flatten(), y=splines_predictive['mu'].mean(axis=0).T.flatten(),\n    color='black'\n    )\nsns.lineplot(\n    x=hour_bikes.flatten(), y=(B * spline_samples['beta'].mean(axis=0))[:, 5],\n    linestyle='--'\n    );\n\n\n\n\n\ncnt_mu = splines_predictive['y'].mean(axis=0).T.flatten()\ncnt_std = splines_predictive['y'].std(axis=0).T.flatten()\n\ndf = pd.DataFrame({\n    'hr': hour['hr'].values,\n    'cnt_scaled': hour['cnt_std'].values,\n    'cnt_mu': cnt_mu,\n    'cnt_std': cnt_std,\n    'cnt_high': cnt_mu + cnt_std,\n    'cnt_low': cnt_mu - cnt_std\n})\n\ndf = df.sort_values(by=['hr'])\n\n\n\nFigure 5.9\n\nsns.lineplot(\n    x=df['hr'], y=df['cnt_mu'], color='blue')\nsns.scatterplot(\n    x=hour['hr'], y=hour['cnt_std'], color='grey', alpha=0.3\n    )\nplt.fill_between(\n    x=df['hr'], y1=df['cnt_high'], y2=df['cnt_low'], color='grey',\n    alpha=0.3\n    )\nplt.scatter(knot_list, np.zeros_like(knot_list), color='black')\nplt.title('Cubic Splines using 6 Knots');"
  },
  {
    "objectID": "posts/2022-07-23-bmcp-ch-5.html#choosing-knots-and-priors-for-splines",
    "href": "posts/2022-07-23-bmcp-ch-5.html#choosing-knots-and-priors-for-splines",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 5",
    "section": "5.6 - Choosing knots and priors for splines",
    "text": "5.6 - Choosing knots and priors for splines\n\nBs = []\nnum_knots = [3, 6, 9, 12, 18]\nfor nk in num_knots:\n    knot_list = torch.linspace(0, 24, nk+2)[1:-1]\n    B = dmatrix(\n        'bs(cnt, knots=knots, degree=3, include_intercept=True) - 1',\n        {'cnt': hour.hr.values, 'knots': knot_list[1:-1]}\n    )\n    B = torch.tensor(np.asarray(B)).float()\n    Bs.append(B)\n\n\ninf_data = []\nfor B in Bs:\n\n    mcmc_obj = MCMC(NUTS(splines), 500, 300)\n    mcmc_obj.run(B, cnt_bikes)\n\n    post_samples = mcmc_obj.get_samples(500)\n    post_pred = Predictive(\n        splines, post_samples\n    )(B, None)\n\n    az_obj = az.from_pyro(\n        posterior=mcmc_obj,\n        posterior_predictive=post_pred\n    )\n\n    inf_data.append(az_obj)\n\nSample: 100%|██████████| 800/800 [00:16, 48.58it/s, step size=2.62e-01, acc. prob=0.924]\nSample: 100%|██████████| 800/800 [00:13, 58.92it/s, step size=2.90e-01, acc. prob=0.888]\nSample: 100%|██████████| 800/800 [00:14, 55.82it/s, step size=2.67e-01, acc. prob=0.910]\nSample: 100%|██████████| 800/800 [00:13, 58.91it/s, step size=3.04e-01, acc. prob=0.886]\nSample: 100%|██████████| 800/800 [00:17, 45.97it/s, step size=2.59e-01, acc. prob=0.890]\n\n\n\n# something is not right here\ndict_cmp = {f\"m_{k}k\": v for k, v in zip(num_knots, inf_data)}\ncmp = az.compare(dict_cmp, ic='loo', var_name='y')\ncmp\n\n['m_18k', 'm_12k', 'm_9k', 'm_6k', 'm_3k']\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nm_3k\n0\n10575.715589\n20.874493\n0.000000\n0.850828\n129.781140\n0.000000\nFalse\nlog\n\n\nm_6k\n1\n10423.693963\n14.518533\n152.021626\n0.000000\n131.808422\n19.387344\nFalse\nlog\n\n\nm_9k\n2\n10094.535427\n12.458113\n481.180162\n0.000000\n133.635244\n36.071652\nFalse\nlog\n\n\nm_12k\n3\n9580.695289\n8.562018\n995.020300\n0.000000\n136.447458\n53.955052\nFalse\nlog\n\n\nm_18k\n4\n8600.222467\n6.347949\n1975.493122\n0.149172\n142.746908\n81.838699\nFalse\nlog\n\n\n\n\n\n\n\n\ncolors = ['black', 'blue', 'grey', 'grey', 'black']\nlinestyle = [\"-\",\"-\",\"--\",\"--\",\"-\"]\nlinewidth = [1.5, 3, 1.5, 1.5, 3]\n\nfor ob, col, knots, ls, lw in zip(\n    inf_data, colors, sorted(num_knots, reverse=True), linestyle, linewidth\n    ):\n\n    sns.lineplot(\n    x=hour['hr'], y=ob['posterior_predictive']['y'][0].mean(axis=0), color=col,\n    label=f'knots={knots}', linestyle=ls, linewidth=lw\n    )\n    sns.scatterplot(\n    x=hour['hr'], y=hour['cnt_std'].values, color='lightgrey', alpha=0.5, edgecolor='grey'\n    )\n    plt.title('Model fit with different number of knots');\n\n\n\n\n\n5.6.1 Regularizing priors for splines\n\nclass GaussianRandomWalk(dist.TorchDistribution):\n    has_rsample = True\n    arg_constraints = {'scale': constraints.positive}\n    support = constraints.real\n\n    def __init__(self, scale, num_steps=1):\n        self.scale = scale\n        batch_shape, event_shape = scale.shape, torch.Size([num_steps])\n        super(GaussianRandomWalk, self).__init__(batch_shape, event_shape)\n    \n    def rsample(self, sample_shape=torch.Size()):\n        shape = sample_shape + self.batch_shape + self.event_shape\n        walks = self.scale.new_empty(shape).normal_()\n        return walks.cumsum(-1) * self.scale.unsqueeze(-1)\n    \n    def log_prob(self, x):\n        init_prob = dist.Normal(self.scale.new_tensor(0.), self.scale).log_prob(x[..., 0])\n        step_probs = dist.Normal(x[..., :-1], self.scale).log_prob(x[..., 1:])\n        return init_prob + step_probs.sum(-1)\n\n\ndef splines_grw(design_matrix, count_bikes=None):\n\n    N, P = design_matrix.shape\n\n    tau = pyro.sample('tau', dist.HalfCauchy(1.))\n    sigma = pyro.sample('sigma', dist.HalfNormal(1.))\n\n    with pyro.plate('knot_list', P):\n        beta = pyro.sample('beta', GaussianRandomWalk(scale=tau, num_steps=14))\n\n    mu = pyro.deterministic('mu', torch.matmul(beta, design_matrix.T))\n\n    with pyro.plate('output', N):\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=count_bikes)\n\n\nnum_knots = 12\nknot_list = torch.linspace(0, 23, num_knots + 2)[1:-1]\n\nB = dmatrix(\n    \"bs(cnt_std, knots=knots, degree=3, include_intercept=True) - 1\",\n    {'cnt_std': hour.hr.values, 'knots': knot_list[1:-1]}\n)\n\nB = torch.tensor(np.asarray(B)).float()\ncnt_bikes = torch.tensor(hour['cnt_std'].values).float()\nhour_bikes = torch.tensor(hour['hr'].values).reshape(-1, 1).float()\n\n\nsplines_grw_mcmc = MCMC(NUTS(splines_grw), 500, 300)\nsplines_grw_mcmc.run(B, cnt_bikes)\n\nSample: 100%|██████████| 800/800 [01:48,  7.40it/s, step size=1.51e-01, acc. prob=0.878]\n\n\n\nsplines_grw_mcmc.summary()\n\n\n                 mean       std    median      5.0%     95.0%     n_eff     r_hat\n  beta[0,0]      0.06      0.00      0.06      0.05      0.06    793.18      1.00\n  beta[0,1]     -0.00      0.01     -0.00     -0.02      0.02    543.60      1.00\n  beta[0,2]      0.07      0.01      0.07      0.05      0.08    539.25      1.00\n  beta[0,3]     -0.07      0.01     -0.07     -0.08     -0.05    593.26      1.00\n  beta[0,4]      0.32      0.01      0.32      0.31      0.34    598.96      1.00\n  beta[0,5]      0.30      0.01      0.30      0.29      0.32    647.04      1.00\n  beta[0,6]      0.10      0.01      0.10      0.09      0.12    580.51      1.00\n  beta[0,7]      0.34      0.01      0.34      0.33      0.36    588.65      1.00\n  beta[0,8]      0.19      0.01      0.19      0.18      0.20    556.40      1.00\n  beta[0,9]      0.31      0.01      0.31      0.30      0.32    542.28      1.00\n beta[0,10]      0.56      0.01      0.56      0.55      0.58    614.87      1.00\n beta[0,11]      0.11      0.01      0.11      0.09      0.13    651.78      1.00\n beta[0,12]      0.19      0.01      0.19      0.17      0.20    619.13      1.00\n beta[0,13]      0.09      0.01      0.09      0.08      0.10    684.85      1.00\n  beta[1,0]      0.06      0.01      0.06      0.05      0.07    684.00      1.00\n  beta[1,1]     -0.00      0.01     -0.00     -0.02      0.01    546.69      1.00\n  beta[1,2]      0.07      0.01      0.07      0.05      0.09    424.07      1.01\n  beta[1,3]     -0.07      0.01     -0.07     -0.08     -0.05    269.08      1.02\n  beta[1,4]      0.32      0.01      0.32      0.31      0.33    414.38      1.02\n  beta[1,5]      0.30      0.01      0.30      0.29      0.32    618.06      1.01\n  beta[1,6]      0.10      0.01      0.10      0.09      0.12    563.03      1.00\n  beta[1,7]      0.34      0.01      0.34      0.33      0.36    422.95      1.00\n  beta[1,8]      0.19      0.01      0.19      0.18      0.20    553.61      1.00\n  beta[1,9]      0.31      0.01      0.31      0.29      0.32    468.49      1.00\n beta[1,10]      0.56      0.01      0.56      0.55      0.58    446.50      1.00\n beta[1,11]      0.11      0.01      0.11      0.10      0.13    373.58      1.00\n beta[1,12]      0.19      0.01      0.19      0.17      0.20    500.06      1.00\n beta[1,13]      0.09      0.01      0.09      0.08      0.10    610.37      1.00\n  beta[2,0]      0.06      0.01      0.06      0.05      0.07    564.38      1.00\n  beta[2,1]     -0.00      0.01     -0.00     -0.02      0.01    599.84      1.00\n  beta[2,2]      0.07      0.01      0.07      0.05      0.09    452.10      1.00\n  beta[2,3]     -0.07      0.01     -0.07     -0.08     -0.05    550.86      1.00\n  beta[2,4]      0.32      0.01      0.32      0.31      0.34    599.10      1.00\n  beta[2,5]      0.30      0.01      0.30      0.29      0.32    688.41      1.00\n  beta[2,6]      0.10      0.01      0.10      0.09      0.11    497.99      1.00\n  beta[2,7]      0.34      0.01      0.34      0.33      0.36    439.54      1.00\n  beta[2,8]      0.19      0.01      0.19      0.17      0.20    695.93      1.00\n  beta[2,9]      0.31      0.01      0.31      0.29      0.32    728.54      1.00\n beta[2,10]      0.56      0.01      0.56      0.55      0.58    574.70      1.00\n beta[2,11]      0.11      0.01      0.11      0.09      0.13    543.40      1.00\n beta[2,12]      0.19      0.01      0.19      0.17      0.20    512.70      1.00\n beta[2,13]      0.09      0.01      0.09      0.08      0.10    635.21      1.00\n  beta[3,0]      0.06      0.01      0.06      0.05      0.07    598.90      1.00\n  beta[3,1]     -0.00      0.01     -0.00     -0.02      0.01    480.85      1.00\n  beta[3,2]      0.07      0.01      0.07      0.05      0.08    540.00      1.00\n  beta[3,3]     -0.07      0.01     -0.07     -0.08     -0.05    461.03      1.00\n  beta[3,4]      0.32      0.01      0.32      0.31      0.33    489.37      1.00\n  beta[3,5]      0.30      0.01      0.30      0.29      0.32    484.36      1.00\n  beta[3,6]      0.10      0.01      0.10      0.09      0.12    591.71      1.00\n  beta[3,7]      0.34      0.01      0.34      0.33      0.36    468.37      1.00\n  beta[3,8]      0.19      0.01      0.19      0.18      0.20    325.26      1.00\n  beta[3,9]      0.31      0.01      0.31      0.30      0.32    618.83      1.00\n beta[3,10]      0.56      0.01      0.56      0.55      0.58    502.47      1.00\n beta[3,11]      0.11      0.01      0.11      0.09      0.12    542.12      1.00\n beta[3,12]      0.19      0.01      0.19      0.17      0.20    576.58      1.00\n beta[3,13]      0.09      0.01      0.09      0.08      0.09    709.30      1.00\n  beta[4,0]      0.06      0.00      0.06      0.05      0.07    785.06      1.00\n  beta[4,1]     -0.00      0.01     -0.00     -0.02      0.01    604.06      1.00\n  beta[4,2]      0.07      0.01      0.07      0.05      0.09    493.62      1.00\n  beta[4,3]     -0.07      0.01     -0.07     -0.08     -0.05    511.71      1.00\n  beta[4,4]      0.32      0.01      0.32      0.31      0.34    497.03      1.00\n  beta[4,5]      0.30      0.01      0.30      0.29      0.31    657.27      1.00\n  beta[4,6]      0.10      0.01      0.10      0.09      0.12    577.98      1.00\n  beta[4,7]      0.34      0.01      0.34      0.33      0.36    514.37      1.00\n  beta[4,8]      0.19      0.01      0.19      0.18      0.20    470.99      1.00\n  beta[4,9]      0.31      0.01      0.31      0.29      0.32    669.33      1.00\n beta[4,10]      0.56      0.01      0.56      0.55      0.58    540.58      1.00\n beta[4,11]      0.11      0.01      0.11      0.09      0.12    502.99      1.00\n beta[4,12]      0.19      0.01      0.19      0.17      0.20    417.16      1.00\n beta[4,13]      0.09      0.00      0.09      0.08      0.09    592.79      1.00\n  beta[5,0]      0.06      0.01      0.06      0.05      0.06    841.34      1.00\n  beta[5,1]     -0.00      0.01     -0.00     -0.02      0.01    598.37      1.00\n  beta[5,2]      0.07      0.01      0.07      0.05      0.08    393.97      1.00\n  beta[5,3]     -0.07      0.01     -0.07     -0.08     -0.05    470.53      1.00\n  beta[5,4]      0.32      0.01      0.32      0.31      0.33    519.00      1.00\n  beta[5,5]      0.30      0.01      0.30      0.29      0.32    454.97      1.00\n  beta[5,6]      0.10      0.01      0.10      0.09      0.11    467.47      1.00\n  beta[5,7]      0.34      0.01      0.34      0.33      0.36    453.50      1.00\n  beta[5,8]      0.19      0.01      0.19      0.17      0.20    476.61      1.00\n  beta[5,9]      0.31      0.01      0.31      0.29      0.32    468.98      1.00\n beta[5,10]      0.56      0.01      0.56      0.55      0.58    412.85      1.00\n beta[5,11]      0.11      0.01      0.11      0.09      0.13    417.32      1.00\n beta[5,12]      0.19      0.01      0.19      0.17      0.20    487.35      1.01\n beta[5,13]      0.09      0.00      0.09      0.08      0.10    855.13      1.00\n  beta[6,0]      0.06      0.00      0.06      0.05      0.06    469.53      1.00\n  beta[6,1]     -0.00      0.01     -0.00     -0.02      0.01    490.03      1.00\n  beta[6,2]      0.07      0.01      0.07      0.05      0.08    412.12      1.00\n  beta[6,3]     -0.07      0.01     -0.07     -0.08     -0.05    547.10      1.00\n  beta[6,4]      0.32      0.01      0.32      0.31      0.34    513.47      1.00\n  beta[6,5]      0.30      0.01      0.30      0.29      0.32    527.11      1.00\n  beta[6,6]      0.10      0.01      0.10      0.09      0.11    568.13      1.00\n  beta[6,7]      0.34      0.01      0.34      0.33      0.36    621.95      1.00\n  beta[6,8]      0.19      0.01      0.19      0.18      0.20    611.84      1.00\n  beta[6,9]      0.31      0.01      0.31      0.30      0.32    591.34      1.00\n beta[6,10]      0.56      0.01      0.56      0.55      0.58    485.72      1.00\n beta[6,11]      0.11      0.01      0.11      0.10      0.13    454.76      1.00\n beta[6,12]      0.19      0.01      0.19      0.17      0.20    418.77      1.00\n beta[6,13]      0.09      0.00      0.09      0.08      0.10    635.51      1.00\n  beta[7,0]      0.06      0.00      0.06      0.05      0.07    633.33      1.00\n  beta[7,1]     -0.00      0.01     -0.00     -0.02      0.01    574.07      1.00\n  beta[7,2]      0.07      0.01      0.07      0.05      0.08    519.25      1.00\n  beta[7,3]     -0.07      0.01     -0.07     -0.08     -0.05    619.73      1.00\n  beta[7,4]      0.32      0.01      0.32      0.31      0.34    623.32      1.00\n  beta[7,5]      0.30      0.01      0.30      0.29      0.32    487.51      1.00\n  beta[7,6]      0.10      0.01      0.10      0.09      0.12    521.75      1.00\n  beta[7,7]      0.34      0.01      0.34      0.33      0.36    528.14      1.00\n  beta[7,8]      0.19      0.01      0.19      0.18      0.20    536.29      1.00\n  beta[7,9]      0.31      0.01      0.31      0.30      0.32    474.66      1.01\n beta[7,10]      0.56      0.01      0.56      0.55      0.58    444.83      1.01\n beta[7,11]      0.11      0.01      0.11      0.10      0.13    578.87      1.00\n beta[7,12]      0.18      0.01      0.19      0.17      0.20    636.52      1.00\n beta[7,13]      0.09      0.00      0.09      0.08      0.09    726.01      1.00\n  beta[8,0]      0.06      0.00      0.06      0.05      0.06    759.45      1.00\n  beta[8,1]     -0.00      0.01     -0.00     -0.02      0.02    451.96      1.00\n  beta[8,2]      0.07      0.01      0.07      0.05      0.09    556.69      1.00\n  beta[8,3]     -0.07      0.01     -0.07     -0.08     -0.05    587.00      1.00\n  beta[8,4]      0.32      0.01      0.32      0.31      0.34    432.25      1.01\n  beta[8,5]      0.30      0.01      0.30      0.29      0.32    497.88      1.00\n  beta[8,6]      0.10      0.01      0.10      0.09      0.12    493.79      1.00\n  beta[8,7]      0.34      0.01      0.34      0.33      0.36    473.27      1.00\n  beta[8,8]      0.19      0.01      0.19      0.18      0.20    511.87      1.00\n  beta[8,9]      0.31      0.01      0.31      0.30      0.32    560.39      1.00\n beta[8,10]      0.56      0.01      0.56      0.55      0.57    643.56      1.00\n beta[8,11]      0.11      0.01      0.11      0.09      0.13    615.96      1.00\n beta[8,12]      0.19      0.01      0.19      0.17      0.20    789.36      1.00\n beta[8,13]      0.09      0.00      0.09      0.08      0.10    722.99      1.00\n  beta[9,0]      0.06      0.01      0.06      0.05      0.07    705.95      1.00\n  beta[9,1]     -0.00      0.01     -0.00     -0.02      0.02    539.48      1.01\n  beta[9,2]      0.07      0.01      0.07      0.05      0.09    729.97      1.01\n  beta[9,3]     -0.07      0.01     -0.07     -0.08     -0.05    606.84      1.01\n  beta[9,4]      0.32      0.01      0.32      0.31      0.33    533.78      1.00\n  beta[9,5]      0.30      0.01      0.30      0.29      0.32    770.46      1.00\n  beta[9,6]      0.10      0.01      0.10      0.09      0.11    672.34      1.00\n  beta[9,7]      0.34      0.01      0.34      0.33      0.36    590.69      1.00\n  beta[9,8]      0.19      0.01      0.19      0.17      0.20    701.83      1.00\n  beta[9,9]      0.31      0.01      0.31      0.30      0.32    491.28      1.00\n beta[9,10]      0.56      0.01      0.56      0.55      0.58    501.94      1.00\n beta[9,11]      0.11      0.01      0.11      0.10      0.13    458.62      1.00\n beta[9,12]      0.19      0.01      0.19      0.17      0.20    491.62      1.01\n beta[9,13]      0.09      0.00      0.09      0.08      0.09    694.76      1.00\n beta[10,0]      0.06      0.01      0.06      0.05      0.07    661.86      1.00\n beta[10,1]     -0.00      0.01     -0.00     -0.02      0.01    560.89      1.00\n beta[10,2]      0.07      0.01      0.07      0.05      0.09    487.57      1.00\n beta[10,3]     -0.07      0.01     -0.07     -0.08     -0.05    538.75      1.00\n beta[10,4]      0.32      0.01      0.32      0.31      0.34    582.82      1.00\n beta[10,5]      0.30      0.01      0.30      0.29      0.32    441.16      1.00\n beta[10,6]      0.10      0.01      0.10      0.09      0.12    413.37      1.01\n beta[10,7]      0.34      0.01      0.34      0.33      0.36    529.81      1.02\n beta[10,8]      0.19      0.01      0.19      0.18      0.20    615.98      1.01\n beta[10,9]      0.31      0.01      0.31      0.29      0.32    518.03      1.02\nbeta[10,10]      0.56      0.01      0.56      0.55      0.57    549.82      1.00\nbeta[10,11]      0.11      0.01      0.11      0.10      0.13    526.15      1.00\nbeta[10,12]      0.19      0.01      0.19      0.17      0.20    506.01      1.00\nbeta[10,13]      0.09      0.00      0.09      0.08      0.09    626.06      1.00\n beta[11,0]      0.06      0.01      0.06      0.05      0.07    656.89      1.00\n beta[11,1]     -0.00      0.01     -0.00     -0.02      0.01    501.48      1.01\n beta[11,2]      0.07      0.01      0.07      0.05      0.08    434.22      1.01\n beta[11,3]     -0.07      0.01     -0.07     -0.08     -0.06    455.18      1.01\n beta[11,4]      0.32      0.01      0.32      0.31      0.34    609.90      1.00\n beta[11,5]      0.30      0.01      0.30      0.29      0.31    742.15      1.00\n beta[11,6]      0.10      0.01      0.10      0.09      0.12    811.31      1.00\n beta[11,7]      0.34      0.01      0.34      0.33      0.36    572.83      1.00\n beta[11,8]      0.19      0.01      0.19      0.17      0.20    642.85      1.00\n beta[11,9]      0.31      0.01      0.31      0.30      0.32    541.95      1.00\nbeta[11,10]      0.56      0.01      0.56      0.55      0.58    488.09      1.00\nbeta[11,11]      0.11      0.01      0.11      0.10      0.13    400.39      1.00\nbeta[11,12]      0.19      0.01      0.19      0.17      0.20    502.89      1.00\nbeta[11,13]      0.09      0.00      0.09      0.08      0.09    770.16      1.00\n beta[12,0]      0.06      0.00      0.06      0.05      0.06    835.10      1.00\n beta[12,1]     -0.00      0.01     -0.00     -0.02      0.01    525.41      1.00\n beta[12,2]      0.07      0.01      0.07      0.05      0.09    461.99      1.00\n beta[12,3]     -0.07      0.01     -0.07     -0.08     -0.05    402.14      1.00\n beta[12,4]      0.32      0.01      0.32      0.31      0.33    536.10      1.00\n beta[12,5]      0.30      0.01      0.30      0.29      0.32    508.00      1.00\n beta[12,6]      0.10      0.01      0.10      0.09      0.12    432.68      1.00\n beta[12,7]      0.34      0.01      0.34      0.33      0.36    456.73      1.00\n beta[12,8]      0.19      0.01      0.19      0.18      0.20    454.93      1.00\n beta[12,9]      0.31      0.01      0.31      0.29      0.32    429.57      1.00\nbeta[12,10]      0.56      0.01      0.56      0.55      0.58    428.65      1.00\nbeta[12,11]      0.11      0.01      0.11      0.09      0.13    467.15      1.00\nbeta[12,12]      0.19      0.01      0.19      0.17      0.20    514.43      1.00\nbeta[12,13]      0.09      0.01      0.09      0.08      0.09    648.53      1.00\n beta[13,0]      0.06      0.00      0.06      0.05      0.06    693.73      1.00\n beta[13,1]     -0.00      0.01     -0.00     -0.02      0.01    571.39      1.00\n beta[13,2]      0.07      0.01      0.07      0.05      0.08    705.45      1.00\n beta[13,3]     -0.07      0.01     -0.07     -0.08     -0.05    672.76      1.00\n beta[13,4]      0.32      0.01      0.32      0.31      0.34    675.63      1.00\n beta[13,5]      0.30      0.01      0.30      0.29      0.32    508.98      1.00\n beta[13,6]      0.10      0.01      0.10      0.09      0.12    562.72      1.00\n beta[13,7]      0.34      0.01      0.34      0.33      0.36    554.37      1.00\n beta[13,8]      0.19      0.01      0.19      0.18      0.20    516.77      1.00\n beta[13,9]      0.31      0.01      0.31      0.29      0.32    493.20      1.00\nbeta[13,10]      0.56      0.01      0.56      0.55      0.58    494.85      1.00\nbeta[13,11]      0.11      0.01      0.11      0.09      0.13    510.48      1.00\nbeta[13,12]      0.19      0.01      0.19      0.17      0.20    676.12      1.00\nbeta[13,13]      0.09      0.00      0.09      0.08      0.10    843.17      1.00\n      sigma      0.13      0.00      0.13      0.13      0.13    562.53      1.00\n        tau      0.21      0.01      0.21      0.19      0.23    968.57      1.00\n\nNumber of divergences: 0\n\n\n\nsplines_grw_samples = splines_grw_mcmc.get_samples(1000)\nsplines_grw_post_pred = Predictive(\n    splines_grw,\n    splines_grw_samples\n)(B, None)\n\n\n# mean -&gt; mean b/c I take mean of GRW first, then mean of\n# 1000 posterior samples\nsns.lineplot(\n    x=hour['hr'], y=splines_grw_post_pred['y'].mean(axis=1).mean(axis=0), \n    color='blue', lw=2, label='splines_grw mean function'\n    )\nsns.scatterplot(\n    x=hour['hr'], y=hour['cnt_std'].values, \n    color='lightgrey', alpha=0.5, edgecolor='grey'\n    )\nplt.title('Gaussian Random Walk Prior');"
  },
  {
    "objectID": "posts/2022-07-23-bmcp-ch-5.html#modeling-textco_2-uptake-with-splines",
    "href": "posts/2022-07-23-bmcp-ch-5.html#modeling-textco_2-uptake-with-splines",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 5",
    "section": "Modeling \\(\\text{CO}_2\\) Uptake with Splines",
    "text": "Modeling \\(\\text{CO}_2\\) Uptake with Splines\n\nplants_CO2 = pd.read_csv(\"./data/CO2_uptake.csv\")\nplant_names = plants_CO2.Plant.unique()\nCO2_conc = plants_CO2.conc.values[:7]\nCO2_concs = plants_CO2.conc.values\nuptake = plants_CO2.uptake.values\nindex = range(12)\ngroups = len(index)\n\n\nnum_knots = 2\nknot_list = np.linspace(CO2_conc[0], CO2_conc[-1], num_knots+2)[1:-1]\n\nBg = dmatrix(\n    \"bs(conc, knots=knots, degree=3, include_intercept=True) - 1\",\n    {\"conc\": CO2_concs, \"knots\": knot_list},\n)\n\nBg = torch.tensor(np.asarray(Bg)).float()\nuptake = torch.tensor(uptake).float()\n\n\nPooled Model - MCMC\n\ndef single_response(design_matrix, obs=None):\n\n    N, P = design_matrix.shape\n    \n    tau = pyro.sample('tau', dist.HalfCauchy(1.))\n    sigma = pyro.sample('sigma', dist.HalfNormal(1.))\n\n    with pyro.plate('coef', P):\n        beta = pyro.sample('beta', dist.Normal(0., tau))\n\n    ug = pyro.deterministic('ug', torch.matmul(beta, design_matrix.T))\n\n    with pyro.plate('obs', N):\n        up = pyro.sample('uptake', dist.Normal(ug, sigma), obs=obs)\n\n\npyro.render_model(\n    single_response,\n    (Bg, uptake), \n    render_distributions=True\n)\n\n\n\n\n\nsr_mcmc = MCMC(NUTS(single_response), 500, 300)\nsr_mcmc.run(Bg, uptake)\n\nSample: 100%|██████████| 800/800 [00:38, 20.99it/s, step size=1.98e-01, acc. prob=0.934]\n\n\n\nsr_mcmc.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n   beta[0]     12.21      1.82     12.27      9.20     15.06    261.26      1.00\n   beta[1]     30.06      3.52     30.11     24.50     35.95    251.13      1.00\n   beta[2]     30.06      6.05     29.81     20.25     39.69    193.86      1.00\n   beta[3]     33.86      9.16     33.72     20.00     49.44    171.13      1.00\n   beta[4]     25.27     22.92     24.96    -14.30     60.07    186.48      1.01\n   beta[5]     33.41      2.03     33.29     30.06     36.65    395.37      1.00\n     sigma      6.77      0.38      6.75      6.24      7.48    596.67      1.00\n       tau     31.24     10.56     28.94     17.24     47.38    226.15      1.00\n\nNumber of divergences: 0\n\n\n\nsr_samples = sr_mcmc.get_samples(1000)\nsr_post_pred = Predictive(\n    single_response, \n    sr_samples\n)(Bg, None)\n\n\nfig, axes = plt.subplots(4, 3, figsize=(10, 6), sharey=True, sharex=True)\n\nfor count, (idx, ax) in enumerate(zip(range(0, 84, 7), axes.ravel())):\n    ax.plot(CO2_conc, uptake[idx:idx+7], '.', lw=1, color='black')\n    ax.plot(CO2_conc, sr_post_pred['uptake'].mean(axis=0)[idx:idx+7], \"k\", alpha=0.5);\n    az.plot_hdi(CO2_conc, sr_post_pred['uptake'][:,idx:idx+7], color=\"C2\", smooth=False, ax=ax)\n    ax.set_title(plant_names[count])\n\nplt.tight_layout()\nfig.text(0.4, -0.05, \"CO2 concentration\", size=18)\nfig.text(-0.03, 0.4, \"CO2 uptake\", size=18, rotation=90);\n\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:156: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n\n\n\n\n\n\nnum_knots = 2\nknot_list = np.linspace(CO2_conc[0], CO2_conc[-1], num_knots+2)[1:-1]\n\nBi = dmatrix(\n    \"bs(conc, knots=knots, degree=3, include_intercept=True) - 1\",\n    {\"conc\": CO2_conc, \"knots\": knot_list},\n)\n\nBi = torch.tensor(np.asarray(Bi)).float()\n\n\n\nMixed Effects Model - MCMC\n\ndef individual_response(design_matrix, groups, obs=None):\n\n    N, P = design_matrix.size()\n    \n    tau = pyro.sample('tau', dist.HalfCauchy(1.))\n    sigma = pyro.sample('sigma', dist.HalfNormal(1.))\n    beta = pyro.sample('beta', dist.Normal(0., tau).expand([P, groups]))\n    ug = pyro.deterministic('ug', torch.matmul(design_matrix, beta))\n    ug = ug[:, index].T.ravel()\n\n    with pyro.plate('obs', ug.size(0)):\n        up = pyro.sample('uptake', dist.Normal(ug, sigma), obs=obs)\n\n\nir_mcmc = MCMC(NUTS(individual_response), 500, 300)\nir_mcmc.run(Bi, groups, uptake)\n\nSample: 100%|██████████| 800/800 [01:03, 12.67it/s, step size=1.36e-01, acc. prob=0.895]\n\n\n\nir_mcmc.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n beta[0,0]     15.89      2.02     15.80     12.85     19.17    749.09      1.00\n beta[0,1]     13.24      1.97     13.24     10.27     16.83    818.95      1.00\n beta[0,2]     16.11      2.09     16.17     13.20     20.12    861.96      1.00\n beta[0,3]     14.01      2.04     13.96     10.65     17.05   1070.16      1.00\n beta[0,4]      9.39      2.04      9.29      6.12     12.53    646.55      1.00\n beta[0,5]     13.72      2.06     13.76     10.31     17.12    797.97      1.00\n beta[0,6]     10.36      1.84     10.24      7.20     13.16    733.61      1.00\n beta[0,7]     11.61      1.96     11.58      8.36     14.59    499.54      1.00\n beta[0,8]     11.02      1.85     10.98      7.65     13.60    414.56      1.00\n beta[0,9]     10.31      1.97     10.33      6.76     13.22    907.83      1.01\nbeta[0,10]      7.64      2.00      7.59      4.22     10.82    602.67      1.00\nbeta[0,11]     10.94      2.01     11.01      7.74     14.01    803.46      1.00\n beta[1,0]     41.13      3.72     40.87     35.36     47.45    294.46      1.00\n beta[1,1]     37.47      4.01     37.73     30.86     43.60    274.05      1.00\n beta[1,2]     44.81      3.81     44.62     39.36     51.19    373.35      1.00\n beta[1,3]     31.45      3.68     31.47     26.03     38.26    499.04      1.00\n beta[1,4]     39.25      3.74     39.25     33.99     45.84    327.53      1.00\n beta[1,5]     33.63      3.66     33.62     27.48     39.48    535.10      1.00\n beta[1,6]     25.75      3.68     25.54     20.20     31.82    485.46      1.00\n beta[1,7]     30.41      3.55     30.30     25.06     36.66    319.34      1.00\n beta[1,8]     25.89      3.73     25.82     19.97     31.73    307.24      1.00\n beta[1,9]     19.24      3.86     19.20     12.78     24.82    514.52      1.01\nbeta[1,10]     14.15      3.80     14.05      8.83     20.76    286.51      1.00\nbeta[1,11]     22.33      3.90     22.39     16.56     28.69    250.93      1.00\n beta[2,0]     30.70      5.89     30.87     21.00     39.83    218.51      1.00\n beta[2,1]     43.32      6.25     43.17     31.48     52.51    365.20      1.00\n beta[2,2]     38.19      6.03     38.38     28.38     47.50    315.04      1.00\n beta[2,3]     34.33      6.01     33.96     25.13     44.26    376.64      1.00\n beta[2,4]     36.97      5.78     36.70     27.90     46.70    287.45      1.00\n beta[2,5]     36.23      5.68     36.39     26.48     45.10    335.15      1.00\n beta[2,6]     31.01      5.85     31.13     20.69     39.98    387.93      1.00\n beta[2,7]     33.15      5.64     33.24     24.02     41.86    238.67      1.00\n beta[2,8]     28.14      5.80     28.03     18.82     37.71    262.62      1.01\n beta[2,9]     16.99      6.29     16.97      5.62     26.27    375.90      1.00\nbeta[2,10]     10.97      6.47     10.75      0.52     20.80    246.43      1.00\nbeta[2,11]     13.48      6.39     13.07      3.82     23.94    166.02      1.00\n beta[3,0]     43.46      9.05     43.38     30.07     58.98    160.78      1.01\n beta[3,1]     40.72      9.80     40.54     23.31     55.08    375.00      1.00\n beta[3,2]     51.19      9.04     51.03     36.54     65.81    279.54      1.00\n beta[3,3]     33.55      9.70     33.58     16.81     48.18    380.20      1.00\n beta[3,4]     42.53      8.86     42.41     29.50     57.63    276.04      1.00\n beta[3,5]     44.49      8.99     44.56     29.90     59.10    320.28      1.00\n beta[3,6]     34.04      9.10     33.86     18.79     48.64    388.05      1.00\n beta[3,7]     32.81      9.16     32.71     18.91     47.81    245.83      1.00\n beta[3,8]     31.09      9.19     31.39     13.62     43.80    248.20      1.00\n beta[3,9]     24.42      9.38     24.31     10.53     40.01    389.07      1.00\nbeta[3,10]     15.10      9.82     15.47     -1.38     30.21    228.23      1.00\nbeta[3,11]     23.38      9.72     23.52      8.32     39.32    160.46      1.00\n beta[4,0]     32.31     22.64     31.24     -3.10     70.85    241.13      1.00\n beta[4,1]     37.32     24.16     38.20      3.44     80.53    407.01      1.00\n beta[4,2]     24.10     22.41     23.79    -11.63     62.44    347.81      1.00\n beta[4,3]     37.11     24.25     38.23     -3.03     70.66    349.09      1.00\n beta[4,4]     20.83     22.29     21.67    -13.15     58.67    329.19      1.00\n beta[4,5]     25.82     23.11     26.21     -9.84     68.61    371.96      1.00\n beta[4,6]     26.63     23.04     25.68    -16.67     60.41    422.46      1.00\n beta[4,7]     21.17     23.67     22.25    -16.18     60.12    273.25      1.00\n beta[4,8]     17.28     23.27     17.68    -21.30     52.39    324.63      1.00\n beta[4,9]     18.38     23.02     18.19    -13.46     55.14    419.31      1.00\nbeta[4,10]     11.79     23.83     11.44    -25.03     48.55    253.85      1.00\nbeta[4,11]      9.50     23.80      9.69    -26.23     49.25    195.24      1.00\n beta[5,0]     39.49      1.95     39.52     36.04     42.50   1449.89      1.00\n beta[5,1]     44.12      1.93     44.08     40.64     46.90   1125.14      1.00\n beta[5,2]     45.41      2.21     45.38     41.92     48.87    468.95      1.00\n beta[5,3]     38.56      2.18     38.59     34.91     42.08    661.23      1.00\n beta[5,4]     42.21      2.11     42.32     38.65     45.23    615.67      1.00\n beta[5,5]     41.19      1.98     41.11     38.21     44.66    756.89      1.00\n beta[5,6]     35.32      2.11     35.43     31.69     38.64   1030.06      1.00\n beta[5,7]     31.34      2.05     31.39     27.53     34.23    928.45      1.00\n beta[5,8]     27.55      1.94     27.57     24.71     31.08   1275.81      1.00\n beta[5,9]     21.57      1.89     21.64     18.33     24.45    982.11      1.00\nbeta[5,10]     14.30      2.08     14.28     10.81     17.45    958.17      1.00\nbeta[5,11]     19.76      1.96     19.83     16.75     23.04    805.23      1.00\n     sigma      2.04      0.29      2.02      1.56      2.48     86.44      1.00\n       tau     31.47      2.90     31.15     26.90     36.18    586.94      1.00\n\nNumber of divergences: 0\n\n\n\nir_samples = ir_mcmc.get_samples(1000)\nir_post_pred = Predictive(\n    individual_response, \n    ir_samples\n)(Bi, groups, None)\n\n\nfig, axes = plt.subplots(4, 3, figsize=(10, 6), sharey=True, sharex=True)\n\nfor count, (idx, ax) in enumerate(zip(range(0, 84, 7), axes.ravel())):\n    ax.plot(CO2_conc, uptake[idx:idx+7], '.', lw=1, color='black')\n    ax.plot(CO2_conc, ir_post_pred['uptake'].mean(axis=0)[idx:idx+7], \"k\", alpha=0.5);\n    az.plot_hdi(CO2_conc, ir_post_pred['uptake'][:,idx:idx+7], color=\"C2\", smooth=False, ax=ax)\n    ax.set_title(plant_names[count])\n\nplt.tight_layout()\nfig.text(0.4, -0.05, \"CO2 concentration\", size=18)\nfig.text(-0.03, 0.4, \"CO2 uptake\", size=18, rotation=90);\n\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:156: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)"
  },
  {
    "objectID": "posts/2021-09-14-Textbooks-Have-Gotten-Good,-Like-Really-Good.html",
    "href": "posts/2021-09-14-Textbooks-Have-Gotten-Good,-Like-Really-Good.html",
    "title": "Textbooks Have Gotten Good, Like Really Good",
    "section": "",
    "text": "In a recent interview, they presented the problem they were facing and asked about the methods I would use to go about solving it. I started off with the heuristics behind the method, Bayesian inference, and continued into some of the more detailed advantages and disadvantages behind why I would choose this method. I think they were a bit surprised about this as my original background is in economics, which is traditionally a domain that employs frequentist based methodologies. This led the interviewers to asking, “It seems you are quite familiar with Bayesian statistics and given your background, how did you come to using Bayesian methods within your projects?”. To which I responded, “I first read about the concept in Thinking Fast and Slow by Daniel Kahneman and became intrigued, so I read a few textbooks about Bayesian statistics and Bayesian methods in machine learning.” The interviewers responses, “You read textbooks?”.\nInitially, I was a little confused as their response inclined that reading technical textbooks is something that is not normal and or only the advanced have the capabilities to do. So I started to ask myself “How am I able to learn from such technical textbooks?” and started to swipe through the pdf textbooks from various authors and publishing dates. I came to the realization that the textbooks I most often read or refer back to are the ones that were recently published. This observation lead to the conclusion that modern textbooks are, well, very modern and interactive—directional linking throughout sections, hyperlinks to GitHub repositories and Google Colab notebooks for reproducible code, and excellent visualizations of the concepts presented throughout that chapter.\nThe improvement in technical textbooks is really a result of three ingredients:\n\n1.) Utilizing Jupyter Notebooks / Google Colab\n2.) Incorporating GitHub public repositories\n3.) The author(s) and or community providing code for the chapter’s content\n\nKevin Murphy’s latest release “Probabilistic Machine Learning - An Introduction” has it’s own GitHub repository with Google Colab notebooks for generating figures for each chapter presented in the book. Not only that, but also supplementary material links for each chapter with additional information, typically Google Colab notebooks, showcasing the software libraries used to generate the figures.\nWhen these three ingredients are brought together, you now have an interactive textbook in which some of the barriers to entry, in the form of prior knowledge and skillsets, are broken down and allow for a wider audience to gain said new knowledge and skillsets. Furthermore, and where traditional non-interactive textbooks lack, an interactive textbook’s main advantages are in the form of trial and error, math formalisms \\(\\rightarrow\\) code, and indirect efficient programming principles.\nThe mysterious power of trial and error could be the biggest advantage. Simply playing around with the code, inputing different values here and there to see how the outputs are affected allows one to gain a level of intuition that is not as easily accessible with textbooks without code or visualizations. Try something. It might not work and if it doesn’t, try something else—an error is just another opportunity to run another trial.\nAfter having read the definitions and formalisms, going from math \\(\\rightarrow\\) code can be a difficult task and often renders the question, “where do I even start?”.\nBuilding off of math \\(\\rightarrow\\) code, interactive textbooks allow one to see how others have programmed the algorithm. For me, as a self taught programmer, this is a huge value add as it allows me to learn more efficient and better programming principles (assuming the open source code is “efficient”). As well, reading code produced by others allows one to continually develop their skillsets and knowledge within programming—another valuable effect.\nThese three ingredients is where the bulk of the value add comes. A couple other points worth pointing out are (1) familiarizing yourself with the formalisms pays dividends, and (2) you should read the textbooks where the author resonates with you. Becoming knowledgeable with the formalisms pays dividends as you read other textbooks, research papers, and even venture outside of the field you are in. Their may be some subtleties in notations, but you can recognize this and still manage your way through the technical content you are reading. Secondly, it is often the case I will have three to four books about the same “material”, but I will only “connect” with one or two of the author’s way of describing the concepts. So, don’t be afraid to download several books on the same material (which is easily doable from pdf drive)."
  },
  {
    "objectID": "posts/2022-06-30-Probabilistic-Prediction-Problems-Part-2.html",
    "href": "posts/2022-06-30-Probabilistic-Prediction-Problems-Part-2.html",
    "title": "Probabilistic Prediction Problems - Part 2",
    "section": "",
    "text": "Part two deals with model evaluation and selection using the metrics and scoring rules defined in part one."
  },
  {
    "objectID": "posts/2022-06-30-Probabilistic-Prediction-Problems-Part-2.html#the-problem-with-parameters",
    "href": "posts/2022-06-30-Probabilistic-Prediction-Problems-Part-2.html#the-problem-with-parameters",
    "title": "Probabilistic Prediction Problems - Part 2",
    "section": "The Problem with Parameters",
    "text": "The Problem with Parameters\nThree main monsters when it comes to “modeling”: 1. Overfitting 2. Underfitting 3. Con-founders\nThe goal of the model should be stated before you choose your methods to tame these monsters: - Is the goal predictive power? - Is the goal to understand causes?\nRegarding monster (1), adding variables and parameters to a model can help to reveal hidden effects and improve estimates. However, more parameters always results in a better model “fit”. While more complex models fit the data better, they often predict new data worse. Models that have many parameters tend to overfit more than simpler models. Generally, fit is measured by how well the model can retrodict the data used to fit the model. A common metric for this is “variance explained”, \\(R^2\\). Monster (2) hurts, too as underfitting produces models that are inaccurate both within and out of sample. Underfit models have learned too little, whether that be from uninformative features or too simple a model.\nSo, how to navigate overfitting and underfitting? First, pick a criterion of model performance as the target—what do you want the model to be good at? Methods based on information theory can provide a common and useful target."
  },
  {
    "objectID": "posts/2022-06-30-Probabilistic-Prediction-Problems-Part-2.html#evaluating-generative-models",
    "href": "posts/2022-06-30-Probabilistic-Prediction-Problems-Part-2.html#evaluating-generative-models",
    "title": "Probabilistic Prediction Problems - Part 2",
    "section": "Evaluating Generative Models",
    "text": "Evaluating Generative Models\nGenerative models aim to model the underlying generative process of the data, typically using Bayes theorem from which we can also generate new samples:\n\\[p(z | x) = \\frac{p(x|z)p(z)}{p(x)}\\]\nGenerally speaking, when evaluating generative models, we want the metrics to capture: - sample quality - are the samples generated by the model part of the data distribution? - sample diversity - are the samples from the model distribution capturing all modes of the data? - generalization - is the model generalizing beyond the training data?"
  },
  {
    "objectID": "posts/2022-06-30-Probabilistic-Prediction-Problems-Part-2.html#model-selection-and-evaluation",
    "href": "posts/2022-06-30-Probabilistic-Prediction-Problems-Part-2.html#model-selection-and-evaluation",
    "title": "Probabilistic Prediction Problems - Part 2",
    "section": "Model Selection and Evaluation",
    "text": "Model Selection and Evaluation\nTo check the results of modeling and inference, we would like to know how well a model fits observed data \\(x\\), which we can quantify with the evidence or marginal likelihood."
  },
  {
    "objectID": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html",
    "href": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html",
    "title": "No Code, Dependency, and Building Technology",
    "section": "",
    "text": "‘Programmers’, loosely speaking, in some form or another have always been developing software to automate tedious and repetitive tasks. Rightly so, as this is one of the tasks computers are designed to perform. As science and technology progresses, and gets more technological, there is a growing seperation between the maker and the user. This is one of the negative externalities of modernism - we enjoy the benefits of a more advanced and technologically adept society, but fewer and fewer people understand the inner workings. Andrej Karpathy has a jokingly short paragraph in his blog on the matter, “A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are now familiar with and expect”.\nEver since the rise of machine learning and data science, the industry was bound to develop no and or low code products for everything between data cleaning, processing, and annotation, to the implementation of models. This is an example of one of the highest forms of abstraction - you don’t even need to be able to code. Knowledge of the problem at hand and some simple intuition into which model may provide a low validation error is almost all you need. A lower level form of abstraction is through the use of application programming interfaces (APIs) such as scikit-learn, PyTorch, etc. Using these APIs requires more technical skillsets, knowledge of first principles, and a deeper understanding of the problem than the no-code substitutes stated above. Lastly, we have the ‘bare bones’ implementation of algorithms - what most people usually call, “programming things from scratch”. But even then, good luck writing your favorite model without using numpy, scipy, or JAX.\nAs teams of couragous developers and organizations seek to create products to help make everyday routines a commodity and more productive, it can be harder to learn technical methods from first principles as they have been abstracted away. There’s no need, nor the time, to start and go writing everything from scratch, but having a solid intuition into what is going on under the hood can allow you to uniquely solve complex problems, debug more efficiently, contribute to open source software, etc."
  },
  {
    "objectID": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#modernity-and-abstraction",
    "href": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#modernity-and-abstraction",
    "title": "No Code, Dependency, and Building Technology",
    "section": "",
    "text": "‘Programmers’, loosely speaking, in some form or another have always been developing software to automate tedious and repetitive tasks. Rightly so, as this is one of the tasks computers are designed to perform. As science and technology progresses, and gets more technological, there is a growing seperation between the maker and the user. This is one of the negative externalities of modernism - we enjoy the benefits of a more advanced and technologically adept society, but fewer and fewer people understand the inner workings. Andrej Karpathy has a jokingly short paragraph in his blog on the matter, “A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are now familiar with and expect”.\nEver since the rise of machine learning and data science, the industry was bound to develop no and or low code products for everything between data cleaning, processing, and annotation, to the implementation of models. This is an example of one of the highest forms of abstraction - you don’t even need to be able to code. Knowledge of the problem at hand and some simple intuition into which model may provide a low validation error is almost all you need. A lower level form of abstraction is through the use of application programming interfaces (APIs) such as scikit-learn, PyTorch, etc. Using these APIs requires more technical skillsets, knowledge of first principles, and a deeper understanding of the problem than the no-code substitutes stated above. Lastly, we have the ‘bare bones’ implementation of algorithms - what most people usually call, “programming things from scratch”. But even then, good luck writing your favorite model without using numpy, scipy, or JAX.\nAs teams of couragous developers and organizations seek to create products to help make everyday routines a commodity and more productive, it can be harder to learn technical methods from first principles as they have been abstracted away. There’s no need, nor the time, to start and go writing everything from scratch, but having a solid intuition into what is going on under the hood can allow you to uniquely solve complex problems, debug more efficiently, contribute to open source software, etc."
  },
  {
    "objectID": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#switching-costs-dependency-and-open-source-goods",
    "href": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#switching-costs-dependency-and-open-source-goods",
    "title": "No Code, Dependency, and Building Technology",
    "section": "Switching Costs, Dependency and Open Source Goods",
    "text": "Switching Costs, Dependency and Open Source Goods\nAt least the two lower levels of abstractions usually rely on open-source software, whereas the no-code alternative is typically (at the moment) proprietary technology in the form of machine learning as a service and or platform. [Edit 14.09.2021: H20.ai, HuggingFace, MakeML, CreateML, Google Cloud AutoML are all open-source services or platforms in the low or no code ML space] Open-source gives you the biggest flexibility that, if the space again changes, you can move things. Otherwise, you find yourself locked into a technology stack the way you were locked in to technologies from the ’80s and ’90s and 2000s.\nBeing locked into IT components can have serious opportunity costs. For example, switching from Mac to a Windows based PC involves not only the hardware costs of the computer itself, but also involves purchasing of a whole new library of software, and even more importantly, learning how to use a brand new system. When you, or an organization, decides to go the propriety route, these switching costs can be very high, and users may find themselves experiencing lock-in; a situation where the cost of changing to a different system is so high that switching is virtually inconcievable.\nOf course, the producers of this hardware / sofware love the fact that you have an inelastic demand curve - a rise in prices won’t affect demand much as switching costs are high. In summary, as machine learning platforms didn’t really exist ~15 years ago, they will more than likely change quite a bit and you are dependent on the producer of continually adapting to industry trends and technological advancements while at the same time, staying flexible to cater to your edge cases when you need it."
  },
  {
    "objectID": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#leverage-and-building-technology",
    "href": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#leverage-and-building-technology",
    "title": "No Code, Dependency, and Building Technology",
    "section": "Leverage and Building Technology",
    "text": "Leverage and Building Technology\nIt would be a full time job to stay present and up to date on everything being released in the space of “machine learning”, but also to be knowledgeable of first princples and have skillsets to use the technologies is in another class of its own. Before AWS, APIs, open source, etc., as an organization or startup, it was likely the question, “we need amazing technical people who can build it all”. Now, with the increase and rise of PaaS, SaaS, open source librarys and tooling, the question shifts from a question of “building”, to a question of “leveraging existing tools”. How long before no / low code gets good enough before we are asking, “why did we not build this with no-code tools?”.\nThe highest form of leverage for a company is to develop and build out difficult and new technology. No-code, if developed right (and is ideally open-source), can still provide leverage and value-add, but any advantage just becomes table stakes. This is what will distinguish great teams vs. good teams; non-linear returns will continue to be through building out proprietery and difficult technology."
  },
  {
    "objectID": "posts/2023-06-17-gsoc-update-2.html",
    "href": "posts/2023-06-17-gsoc-update-2.html",
    "title": "Google Summer of Code - Update 2",
    "section": "",
    "text": "It is currently the end of week five of Google Summer of Code 2023. According to the original deliverables table outlined in my proposal, the goal was to have opened a draft PR for the core functionality of the plot_comparisons. Subsequently, week six and seven were to be spent further developing the plot_comparisons function, and writing tests and a demo notebook for the documentation, respectively. However, at the end of week five, I have a PR open with the majority of the functionality that marginaleffects has. In addition, I also exposed the comparisons function, added tests (which can and will be improved), and have started on documentation."
  },
  {
    "objectID": "posts/2023-06-17-gsoc-update-2.html#methodology",
    "href": "posts/2023-06-17-gsoc-update-2.html#methodology",
    "title": "Google Summer of Code - Update 2",
    "section": "Methodology",
    "text": "Methodology\nHere, I adopt the notation from Chapter 14.4 of Regression and Other Stories to describe the methodology of average predicted comparisons.\nAssume we have fit a Bambi model predicting an outcome \\(Y\\) based on inputs \\(X\\) and parameters \\(\\theta\\). Consider the following scalar inputs:\n\\[u: \\text{the input of interest}\\] \\[v: \\text{all the other inputs}\\] \\[X = (u, v)\\]\nSuppose for the input of interest, we are interested in comparing \\(u = u^{\\text{high}}\\) to \\(u = u^{\\text{low}}\\) with all other inputs \\(v\\) held constant. The predictive difference in the outcome changing only \\(u\\) from is:\n\\[\\text{predictive difference (comparison)} = \\mathbb{E}(y|u^{\\text{high}}, v, \\theta) - \\mathbb{E}(y|u^{\\text{low}}, v, \\theta)\\]\nSelecting the maximum and minimum values of \\(u\\) and averaging over all other inputs \\(v\\) in the data gives you a new “hypothetical” dataset and corresponds to counting all pairs of transitions of \\((u^\\text{low})\\) to \\((u^\\text{high})\\), i.e., differences in \\(u\\) with \\(v\\) held constant."
  },
  {
    "objectID": "posts/2023-06-17-gsoc-update-2.html#implementation",
    "href": "posts/2023-06-17-gsoc-update-2.html#implementation",
    "title": "Google Summer of Code - Update 2",
    "section": "Implementation",
    "text": "Implementation\nTo demonstrase how to compute and plot average predictive comparisons with comparisons and plot_comparions, we model and predict how many fish are caught by visitors to a state park. Many visitors catch zero fish, either because they did not fish at all, or because they were unlucky. We would like to explicitly model this bimodal behavior (zero versus non-zero) using a Zero Inflated Poisson model, and to compare how different inputs of interest \\(u\\) and other covariate values \\(v\\) are associated with the number of fish caught.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport bambi as bmb\nfrom bambi.plots import plot_comparison, comparisons\n\n\n\nfish_data = pd.read_stata(\"http://www.stata-press.com/data/r11/fish.dta\")\ncols = [\"count\", \"livebait\", \"camper\", \"persons\", \"child\"]\nfish_data = fish_data[cols]\nfish_data[\"livebait\"] = fish_data[\"livebait\"].astype(\"category\")\nfish_data[\"camper\"] = fish_data[\"camper\"].astype(\"category\")\n\n\nlikelihood = bmb.Likelihood(\"ZeroInflatedPoisson\", params=[\"mu\", \"psi\"], parent=\"mu\")\nlinks = {\"mu\": \"log\", \"psi\": \"logit\"}\nzip_family = bmb.Family(\"zip\", likelihood, links)\npriors = {\"psi\": bmb.Prior(\"Beta\", alpha=3, beta=3)}\n\n\nfish_model = bmb.Model(\n    \"count ~ livebait + camper + persons + child\", \n    fish_data, \n    priors=priors,\n    family=zip_family\n)\n\nfish_idata = fish_model.fit(\n    draws=1000, \n    target_accept=0.95, \n    random_seed=1234, \n    chains=4\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [count_psi, Intercept, livebait, camper, persons, child]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:03&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\nDefault contrast and conditional values\nIn both plot_comparisons and comparisons, \\(u\\) and \\(v\\) are represented by contrast and conditional, respectively. Lets say we are interested in comparing the number of fish caught for livebait = [0, 1] conditional on the number of people.\n\nfig, ax = plot_comparison(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=\"persons\",\n) \nfig.set_size_inches(7, 3)\n\n\n\n\nBy default, if no value is passed with contrast, then the mean or mode is computed (depending on the data type used to fit the model) as the contrast value. As live bait is category data type and no value was passed, the contrast value is [0, 1]. By default, comparisons compares \\(u^\\text{high}\\) to \\(u^\\text{low}\\). Thus, in this example, \\(u^\\text{high}\\) = 1 and \\(u^\\text{low}\\) = 0. The plot shows us that comparing a group of two people who used live bait and no live bait, the expected difference in the number of fish caught is about \\(2\\). As the number of people in the group increases, the expected difference also increases.\nWe can call comparisons directly to view a summary dataframe that includes the term \\(u\\) and its contrast, the specified conditional covariate, and the expected difference in the outcome with the uncertainty interval (by default the 94% highest density interval is computed). We see that camper and child are also in the summary dataframe. This is because for unspecified covariates, comparisons and plot_comparisons computes a default value (mean or mode based on the data type of the covariate). Thus, \\(v\\) = persons, camper, child.\n\ncomparisons_df = comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=\"persons\",\n)\ncomparisons_df.head(10)\n\n\n\n\n\n\n\n\nterm\ncontrast\npersons\ncamper\nchild\nestimate\nhdi_0.03%\nhdi_0.97%\n\n\n\n\n0\nlivebait\n(0.0, 1.0)\n1.000000\n1.0\n0.684\n0.656590\n0.483500\n0.833816\n\n\n1\nlivebait\n(0.0, 1.0)\n1.061224\n1.0\n0.684\n0.692487\n0.509677\n0.873858\n\n\n2\nlivebait\n(0.0, 1.0)\n1.122449\n1.0\n0.684\n0.730352\n0.545617\n0.924122\n\n\n3\nlivebait\n(0.0, 1.0)\n1.183674\n1.0\n0.684\n0.770294\n0.578521\n0.971953\n\n\n4\nlivebait\n(0.0, 1.0)\n1.244898\n1.0\n0.684\n0.812425\n0.616945\n1.024377\n\n\n5\nlivebait\n(0.0, 1.0)\n1.306122\n1.0\n0.684\n0.856867\n0.647213\n1.069846\n\n\n6\nlivebait\n(0.0, 1.0)\n1.367347\n1.0\n0.684\n0.903747\n0.687662\n1.127065\n\n\n7\nlivebait\n(0.0, 1.0)\n1.428571\n1.0\n0.684\n0.953198\n0.730820\n1.186828\n\n\n8\nlivebait\n(0.0, 1.0)\n1.489796\n1.0\n0.684\n1.005363\n0.770465\n1.244912\n\n\n9\nlivebait\n(0.0, 1.0)\n1.551020\n1.0\n0.684\n1.060390\n0.813576\n1.306117\n\n\n\n\n\n\n\nFurthermore, conditional is not limited to one covariate. We can pass a list of covariates to condition on. For example, we can also include camper.\n\nfig, ax = plot_comparison(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=[\"persons\", \"camper\"],\n) \nfig.set_size_inches(7, 3)\n\n\n\n\n\nUnit level contrasts\nEvaluating average predictive comparisons at central values for the conditional covariates \\(v\\) can be problematic when the inputs have a large variance since no single central value (mean, median, etc.) is representative of the covariate. This is especially true when \\(v\\) exhibits bi or multimodality. Thus, it may be desireable to use the empirical distribution of \\(v\\) to compute the predictive comparisons, and then average over a specific or set of covariates to obtain the average predictive comparisons. To achieve unit level contrasts, do not pass a parameter into conditional and or specify None.\n\nunit_level = comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=None,\n)\n\n# empirical distribution\nprint(unit_level.shape[0] == fish_model.data.shape[0])\nunit_level.head(10)\n\nTrue\n\n\n\n\n\n\n\n\n\nterm\ncontrast\ncamper\nchild\npersons\nestimate\nhdi_0.03%\nhdi_0.97%\n\n\n\n\n0\nlivebait\n(0.0, 1.0)\n0.0\n0.0\n1.0\n0.868743\n0.634292\n1.121583\n\n\n1\nlivebait\n(0.0, 1.0)\n1.0\n0.0\n1.0\n1.700244\n1.307142\n2.120623\n\n\n2\nlivebait\n(0.0, 1.0)\n0.0\n0.0\n1.0\n0.868743\n0.634292\n1.121583\n\n\n3\nlivebait\n(0.0, 1.0)\n1.0\n1.0\n2.0\n1.011327\n0.760836\n1.249259\n\n\n4\nlivebait\n(0.0, 1.0)\n0.0\n0.0\n1.0\n0.868743\n0.634292\n1.121583\n\n\n5\nlivebait\n(0.0, 1.0)\n1.0\n2.0\n4.0\n1.454320\n0.961899\n1.947773\n\n\n6\nlivebait\n(0.0, 1.0)\n0.0\n1.0\n3.0\n1.236932\n0.899355\n1.543456\n\n\n7\nlivebait\n(0.0, 1.0)\n0.0\n3.0\n4.0\n0.188411\n0.090118\n0.288605\n\n\n8\nlivebait\n(0.0, 1.0)\n1.0\n2.0\n3.0\n0.607264\n0.395854\n0.826614\n\n\n9\nlivebait\n(0.0, 1.0)\n1.0\n0.0\n1.0\n1.700244\n1.307142\n2.120623\n\n\n\n\n\n\n\nSince the empirical distrubution is used for computing the average predictive comparisons, the same number of rows (250) is returned as the data used to fit the model. To average over a covariate, use the average_by argument. If True is passed, then comparisons averages over all covariates. Else, if a single or list of covariates are passed, then comparisons averages by the covariates passed.\n\n# marginalize over all covariates\ncomparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=None,\n    average_by=True\n)\n\n\n\n\n\n\n\n\nterm\ncontrast\nestimate\nhdi_0.03%\nhdi_0.97%\n\n\n\n\n0\nlivebait\n(0.0, 1.0)\n3.657341\n2.99057\n4.344358\n\n\n\n\n\n\n\n\n# average by number of persons\ncomparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=None,\n    average_by=[\"persons\"]\n)\n\n\n\n\n\n\n\n\nterm\ncontrast\npersons\nestimate\nhdi_0.03%\nhdi_0.97%\n\n\n\n\n0\nlivebait\n(0.0, 1.0)\n1.0\n1.379314\n1.047445\n1.735029\n\n\n1\nlivebait\n(0.0, 1.0)\n2.0\n1.969410\n1.559249\n2.374562\n\n\n2\nlivebait\n(0.0, 1.0)\n3.0\n3.709952\n3.076369\n4.364713\n\n\n3\nlivebait\n(0.0, 1.0)\n4.0\n7.369520\n6.112693\n8.669469\n\n\n\n\n\n\n\n\n# average by number of persons and camper\ncomparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=None,\n    average_by=[\"persons\", \"camper\"]\n)\n\n\n\n\n\n\n\n\nterm\ncontrast\npersons\ncamper\nestimate\nhdi_0.03%\nhdi_0.97%\n\n\n\n\n0\nlivebait\n(0.0, 1.0)\n1.0\n0.0\n0.868743\n0.634292\n1.121583\n\n\n1\nlivebait\n(0.0, 1.0)\n1.0\n1.0\n1.700244\n1.307142\n2.120623\n\n\n2\nlivebait\n(0.0, 1.0)\n2.0\n0.0\n1.430712\n1.095632\n1.784995\n\n\n3\nlivebait\n(0.0, 1.0)\n2.0\n1.0\n2.350440\n1.887173\n2.791573\n\n\n4\nlivebait\n(0.0, 1.0)\n3.0\n0.0\n2.438080\n1.902601\n2.976076\n\n\n5\nlivebait\n(0.0, 1.0)\n3.0\n1.0\n4.451878\n3.761067\n5.174751\n\n\n6\nlivebait\n(0.0, 1.0)\n4.0\n0.0\n3.551448\n2.698365\n4.379869\n\n\n7\nlivebait\n(0.0, 1.0)\n4.0\n1.0\n10.751241\n9.136812\n12.468829\n\n\n\n\n\n\n\n\n\n\nUser specified contrast and conditional values\nThe modeller can also pass their own values for contrast and conditional by using a dictionary where the key, value pairs are the covariate and value(s) to use. For example, if we wanted to compare the number of fish caught for \\(4\\) versus \\(1\\) persons conditional on a range of child and livebait values, we would pass the following dictionary.\n\nfig, ax = plot_comparison(\n    model=fish_model,\n    idata=fish_idata,\n    contrast={\"persons\": [1, 4]},\n    conditional={\"child\": [0, 1, 2], \"livebait\": [0, 1]},\n) \nfig.set_size_inches(7, 3)\n\n\n\n\nNotably, comparing \\(4\\) to \\(1\\) persons given \\(0\\) children and using livebait, the expected difference is about \\(26\\) fish. When not using livebait, the expected difference decreases substantially to about \\(5\\) fish. Using livebait with a group of people is associated with a much larger expected difference in the number of fish caught. Oh, and don’t bring children fishing if you have plans to catch dinner.\n\n\nConclusion\n…"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html",
    "href": "posts/2022-07-22-bmcp-ch-4.html",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\nimport arviz as az\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom torch.distributions import constraints, transforms\nfrom pyro.distributions import constraints\nfrom pyro.infer import Predictive, TracePredictive, NUTS, MCMC\nfrom pyro.infer.autoguide import AutoLaplaceApproximation\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\nfrom pyro.infer.mcmc.util import summary\nimport os\nplt.style.use('ggplot')\nplt.rcParams[\"figure.figsize\"] = (7, 4)"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#transforming-covariates",
    "href": "posts/2022-07-22-bmcp-ch-4.html#transforming-covariates",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "4.1 Transforming Covariates",
    "text": "4.1 Transforming Covariates\n\nFigure 4.2\n\nX_ = torch.from_numpy(babies['Month'].values.reshape(-1, 1)).to(torch.float)\n#X_ = torch.tensor(X_).float()\n\ny = torch.from_numpy(babies['Length'].values).to(torch.float)\n#y = torch.tensor(y).float()\n\n\ndef linear_babies(month, length=None):\n\n    N, P = month.shape\n    \n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 10.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0, 10.).expand([P]))\n    sigma = pyro.sample('sigma', dist.HalfNormal(10.))\n    mu = beta_0 + torch.matmul(beta_1, month.T)\n    \n    with pyro.plate('plate', size=N):\n        y = pyro.sample('y', dist.Normal(mu, sigma), obs=length)\n\n\npyro.render_model(\n    linear_babies, \n    model_args=(X_, y),\n    render_distributions=True\n    )\n\n\n\n\n\nkernel = NUTS(linear_babies, adapt_step_size=True)\nmcmc_linear_babies = MCMC(kernel, 500, 300)\nmcmc_linear_babies.run(X_, y)\n\nSample: 100%|██████████| 800/800 [00:05, 144.48it/s, step size=3.89e-01, acc. prob=0.917]\n\n\n\nmcmc_babie_samples = mcmc_linear_babies.get_samples(1000)\npredictive = Predictive(linear_babies, mcmc_babie_samples)(X_, None)\n\naz_linear_babies = az.from_pyro(\n    posterior=mcmc_linear_babies, posterior_predictive=predictive)\n\nposterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented.\n\n\n\ny_mu = predictive['y'].mean(axis=0)\n\n\nsns.scatterplot(x=babies['Month'], y=babies['Length'], color='grey', alpha=0.75)\nsns.lineplot(x=babies['Month'], y=y_mu, color='blue')\naz.plot_hdi(x=babies['Month'], y=predictive['y'].numpy());\n\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n\n\n\n\n\n\n\nFigure 4.3\n\ndef sqrt_babies(month, length=None):\n\n    N, P = month.shape\n    \n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 10.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0, 10.).expand([P]))\n    sigma = pyro.sample('sigma', dist.HalfNormal(10.))\n    mu = beta_0 + torch.matmul(beta_1, torch.sqrt(month.T))\n    \n    with pyro.plate('plate', size=N):\n        y = pyro.sample('y', dist.Normal(mu, sigma), obs=length)\n\n\nkernel = NUTS(sqrt_babies, adapt_step_size=True)\nmcmc_sqrt = MCMC(kernel, 500, 300)\nmcmc_sqrt.run(X_, y)\n\nSample: 100%|██████████| 800/800 [00:06, 119.31it/s, step size=2.91e-01, acc. prob=0.943]\n\n\n\nmcmc_sqrt_babie_samples = mcmc_sqrt.get_samples(1000)\npredictive_sqrt = Predictive(sqrt_babies, mcmc_sqrt_babie_samples)(X_, None)\n\naz_sqrt_babies = az.from_pyro(\n    posterior=mcmc_sqrt, posterior_predictive=predictive_sqrt)\n\nposterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented.\n\n\n\ny_mu = predictive_sqrt['y'].mean(axis=0)\n\n\nplt.scatter(babies['Month'], babies['Length'], color='grey', alpha=0.75)\nplt.plot(babies['Month'], y_mu, color='blue')\naz.plot_hdi(x=babies['Month'], y=az_sqrt_babies['posterior_predictive']['y'], hdi_prob=.50, color='grey')\naz.plot_hdi(x=babies['Month'], y=az_sqrt_babies['posterior_predictive']['y'], hdi_prob=.94, color='darkgrey')\nplt.title('Linear model with square root transformation on months');"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#varying-uncertainty",
    "href": "posts/2022-07-22-bmcp-ch-4.html#varying-uncertainty",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "4.2 - Varying Uncertainty",
    "text": "4.2 - Varying Uncertainty\n\ndef varying_uncertainty(month, length=None):\n\n    N, P = month.shape\n    \n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 10.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0, 10.).expand([P]))\n    delta = pyro.sample('delta', dist.HalfNormal(10.).expand([2]))\n    sigma = pyro.deterministic(\n        'sigma', \n        delta[0].unsqueeze(-1) + torch.matmul(delta[1].unsqueeze(-1), month.T)\n        )\n    mu = beta_0 + torch.matmul(beta_1, torch.sqrt(month.T))\n    \n    with pyro.plate('plate', size=N):\n        y = pyro.sample('y', dist.Normal(mu, sigma), obs=length)\n\n\npyro.render_model(\n    varying_uncertainty,\n    (X_, y),\n    render_distributions=True\n)\n\n\n\n\n\nvarying_sigma_mcmc = MCMC(NUTS(varying_uncertainty), num_samples=500, warmup_steps=300)\nvarying_sigma_mcmc.run(X_, y)\n\nSample: 100%|██████████| 800/800 [00:08, 95.74it/s, step size=3.06e-01, acc. prob=0.912] \n\n\n\nvarying_sigma_samples = varying_sigma_mcmc.get_samples(1000)\nvarying_sigma_post_pred = Predictive(\n    varying_uncertainty, varying_sigma_samples)(X_, None)\n\n\nfig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(7, 5))\n\nax[0].scatter(babies['Month'], babies['Length'], color='grey', alpha=0.75)\nax[0].plot(babies['Month'], varying_sigma_post_pred['y'].mean(axis=0), color='blue')\naz.plot_hdi(x=babies['Month'], y=varying_sigma_post_pred['y'], hdi_prob=.50, color='grey', ax=ax[0])\naz.plot_hdi(x=babies['Month'], y=varying_sigma_post_pred['y'], hdi_prob=.94, color='darkgrey', ax=ax[0])\nax[0].set_ylabel('length')\nax[0].set_title('Linear model with square root transformation on months')\n\nax[1].plot(babies['Month'], varying_sigma_post_pred['sigma'].mean(axis=0)[0], color='blue', alpha=0.75)\nax[1].set_ylabel('$\\sigma$')\nax[1].set_title('Varying $\\sigma$');"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#interaction-effects",
    "href": "posts/2022-07-22-bmcp-ch-4.html#interaction-effects",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "4.3 - Interaction Effects",
    "text": "4.3 - Interaction Effects\n\ntips_df = pd.read_csv(os.path.abspath('.') + '/data/tips.csv')\ntips_df\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns\n\n\n\n\ntotal_bill_centered = torch.tensor((tips_df[\"total_bill\"] - tips_df[\"total_bill\"].mean()).values, dtype=torch.float32)\ntips = torch.tensor(tips_df[\"tip\"].values, dtype=torch.float)\nsmoker = torch.tensor(pd.Categorical(tips_df[\"smoker\"]).codes, dtype=torch.float)\n\n\ndef interaction_model(bill, smoker, tips=None):\n\n    beta = pyro.sample('beta', dist.Normal(0., 1.).expand([4]))\n    sigma = pyro.sample('sigma', dist.HalfNormal(1.))\n    mu = beta[0] + beta[1] * bill + beta[2] * smoker + beta[3] * smoker * bill\n\n    with pyro.plate('plate', len(bill)):\n        y = pyro.sample('y', dist.Normal(mu, sigma), obs=tips)\n\n\npyro.render_model(\n    interaction_model,\n    (total_bill_centered, smoker, tips), render_distributions=True)\n\n\n\n\n\ninteraction_mcmc = MCMC(NUTS(interaction_model), num_samples=500, warmup_steps=300)\ninteraction_mcmc.run(total_bill_centered, smoker, tips)\n\nSample: 100%|██████████| 800/800 [00:05, 144.46it/s, step size=4.95e-01, acc. prob=0.895]\n\n\n\nmcmc_interaction_samples = interaction_mcmc.get_samples(1000)\ninteraction_predictive = Predictive(interaction_model, mcmc_interaction_samples)\nposterior_predictive = interaction_predictive(total_bill_centered, smoker, None)\n\naz_inference_interaction = az.from_pyro(\n    posterior=interaction_mcmc, posterior_predictive=posterior_predictive)\n\n\ntip_mu = posterior_predictive['y'].mean(axis=0)\ntip_std = posterior_predictive['y'].std(axis=0)\n\npredictions = pd.DataFrame({\n    'bill': total_bill_centered,\n    'smoker': smoker,\n    'tip': tips, \n    'tip_mu': tip_mu,\n    'tip_std': tip_std,\n    'tip_high': tip_mu + tip_std,\n    'tip_low': tip_mu - tip_std\n})\n\npredictions = predictions.sort_values(by=['bill'])\n\n\nsmoker_df = predictions[predictions['smoker'] == 1]\nnonsmoker_df = predictions[predictions['smoker'] == 0]\n\n\n# colors are terrible - TO DO\nsns.lineplot(\n    x=smoker_df['bill'], y=smoker_df['tip_mu'], color='blue', label='smoker'\n    )\nplt.fill_between(\n    smoker_df['bill'], smoker_df['tip_low'], smoker_df['tip_high'], \n    color='lightblue', alpha=0.5\n    )\nsns.lineplot(\n    x=nonsmoker_df['bill'], y=nonsmoker_df['tip_mu'], color='red', label='non-smoker'\n    )\nplt.fill_between(\n    nonsmoker_df['bill'], nonsmoker_df['tip_low'], nonsmoker_df['tip_high'],\n    color='lightgrey', alpha=0.5\n    )\nsns.scatterplot(\n    x=predictions['bill'], y=predictions['tip'], hue=predictions['smoker']\n    )\nplt.legend()\nplt.title('Interaction Effect on Tip')\nplt.show()"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#robust-regression",
    "href": "posts/2022-07-22-bmcp-ch-4.html#robust-regression",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "4.4 - Robust Regression",
    "text": "4.4 - Robust Regression\n\ndef generate_sales(*, days, mean, std, label):\n    \"\"\"code taken from the authors / book\"\"\"\n    np.random.seed(0)\n    df = pd.DataFrame(index=range(1, days+1), columns=[\"customers\", \"sales\"])\n    for day in range(1, days+1):\n        num_customers = stats.randint(30, 100).rvs()+1\n        \n        # This is correct as there is an independent draw for each customers orders\n        dollar_sales = stats.norm(mean, std).rvs(num_customers).sum()\n        \n        df.loc[day, \"customers\"] = num_customers\n        df.loc[day, \"sales\"] = dollar_sales\n        \n    # Fix the types as not to cause Theano errors\n    df = df.astype({'customers': 'int32', 'sales': 'float32'})\n    \n    # Sorting will make plotting the posterior predictive easier later\n    df[\"Food_Category\"] = label\n    df = df.sort_values(\"customers\")\n    \n    return df\n\n\nempanadas = generate_sales(days=200, mean=180, std=30, label=\"Empanada\")\n\nempanadas.iloc[0] = [50, 92000, \"Empanada\"]\nempanadas.iloc[1] = [60, 90000, \"Empanada\"]\nempanadas.iloc[2] = [70, 96000, \"Empanada\"]\nempanadas.iloc[3] = [80, 91000, \"Empanada\"]\nempanadas.iloc[4] = [90, 99000, \"Empanada\"]\n\nempanadas = empanadas.sort_values(\"customers\")\n\nfig, ax = plt.subplots()\nempanadas.sort_values(\"sales\")[:-5].plot(x=\"customers\", y=\"sales\", kind=\"scatter\", ax=ax);\nempanadas.sort_values(\"sales\")[-5:].plot(x=\"customers\", y=\"sales\", kind=\"scatter\", c=\"black\", ax=ax);\n\nax.set_ylabel(\"Argentine Peso\")\nax.set_xlabel(\"Customer Count\")\nax.set_title(\"Empanada Sales\");\n\n\n\n\n\ncustomer_count = torch.tensor(empanadas['customers'].values, dtype=torch.float)\nsales = torch.tensor(empanadas['sales'].values, dtype=torch.float)\n\n\ndef robust_regression(customers, peso=None):\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(50.))\n    beta = pyro.sample('beta', dist.Normal(150., 20.))\n    v = pyro.sample('dof', dist.HalfNormal(20.))\n\n    mu = pyro.deterministic('mu', beta * customers)\n\n    with pyro.plate('plate', len(customers)):\n        sales = pyro.sample('sales', dist.StudentT(loc=mu, scale=sigma, df=v), obs=peso)\n\n\npyro.render_model(\n    robust_regression, (customer_count, sales), render_distributions=True)\n\n\n\n\n\nkernel = NUTS(robust_regression)\nmcmc_robust = MCMC(kernel, 500, 300)\nmcmc_robust.run(customer_count, sales)\n\nSample: 100%|██████████| 800/800 [00:09, 88.07it/s, step size=6.43e-01, acc. prob=0.925] \n\n\n\nmcmc_robust.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n      beta    179.60      0.25    179.61    179.18    179.99    333.32      1.00\n       dof      1.27      0.15      1.27      1.00      1.50    509.08      1.00\n     sigma    150.67     12.30    150.52    128.41    168.89    479.97      1.00\n\nNumber of divergences: 0\n\n\n\nmcmc_robust_samples = mcmc_robust.get_samples(1000)\nrobust_predictive = Predictive(robust_regression, mcmc_robust_samples)(customer_count, None)\naz_robust_inf = az.from_pyro(posterior=mcmc_robust, posterior_predictive=robust_predictive)\n\n\nmu = az_robust_inf['posterior_predictive']['mu'].values.reshape(-1, len(customer_count)).mean(axis=0)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(7, 5), sharex=True)\n\nax[0].scatter(customer_count, sales)\nax[0].plot(customer_count, mu, c='blue')\naz.plot_hdi(\n    customer_count, az_robust_inf['posterior_predictive']['sales'], \n    color='grey', ax=ax[0])\nax[0].set_ylabel('sales')\n\n\nax[1].scatter(customer_count, sales)\nax[1].plot(customer_count, mu, c='blue')\naz.plot_hdi(\n    customer_count, az_robust_inf['posterior_predictive']['sales'], \n    color='grey', ax=ax[1]\n    )\nax[1].set_ylim(bottom=2500, top=21000)\nax[1].set_ylabel('sales')\n\nplt.suptitle('Robust Regression using Student-t likelihood');"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#pooling-multilevel-models-mixed-effects",
    "href": "posts/2022-07-22-bmcp-ch-4.html#pooling-multilevel-models-mixed-effects",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "4.5 - Pooling, Multilevel Models, Mixed Effects",
    "text": "4.5 - Pooling, Multilevel Models, Mixed Effects\n\ndef generate_sales(*, days, mean, std, label):\n    \"\"\"code taken from authors / book\"\"\"\n    np.random.seed(0)\n    df = pd.DataFrame(index=range(1, days+1), columns=[\"customers\", \"sales\"])\n    for day in range(1, days+1):\n        num_customers = stats.randint(30, 100).rvs()+1\n        \n        # This is correct as there is an independent draw for each customers orders\n        dollar_sales = stats.norm(mean, std).rvs(num_customers).sum()\n        \n        df.loc[day, \"customers\"] = num_customers\n        df.loc[day, \"sales\"] = dollar_sales\n        \n    # Fix the types as not to cause Theano errors\n    df = df.astype({'customers': 'int32', 'sales': 'float32'})\n    \n    # Sorting will make plotting the posterior predictive easier later\n    df[\"Food_Category\"] = label\n    df = df.sort_values(\"customers\")\n    \n    return df\n\n\npizza_df = generate_sales(days=365, mean=13, std=5, label=\"Pizza\")\nsandwich_df = generate_sales(days=100, mean=6, std=5, label=\"Sandwich\")\nsalad_days = 3\nsalad_df = generate_sales(days=salad_days, mean=8 ,std=3, label=\"Salad\")\n\n\nfig, ax = plt.subplots()\npizza_df.plot(x=\"customers\", y=\"sales\", kind=\"scatter\", ax=ax, c=\"grey\", label=\"Pizza\", marker=\"^\", s=60);\nsandwich_df.plot(x=\"customers\", y=\"sales\", kind=\"scatter\", ax=ax, c='black', label=\"Sandwich\", marker=\"s\");\nsalad_df.plot(x=\"customers\", y=\"sales\", kind=\"scatter\", ax=ax, label=\"Salad\", c=\"blue\");\n\nax.set_xlabel(\"Number of Customers\")\nax.set_ylabel(\"Daily Sales Dollars\")\nax.set_title(\"Aggregated Sales Dollars\")\nax.legend()\nplt.show()\n\n\n\n\n\nsales_df = pd.concat([pizza_df, sandwich_df, salad_df]).reset_index(drop=True)\nsales_df[\"Food_Category\"] = pd.Categorical(sales_df[\"Food_Category\"])\n\n\ncustomers = torch.tensor(sales_df['customers'].values, dtype=torch.float)\nsales = torch.tensor(sales_df['sales'].values, dtype=torch.float)\nfood_category = torch.tensor(sales_df['Food_Category'].cat.codes.values, dtype=torch.long)\n\nNotes:\n\nextend shape to 3 because of the 3 food categories\nuse dtype = torch.long when using a tensor as indices\nif you use the pyro.plate() primitive, it seems you do not need to specify the .expand() method on distributions, i.e., to make the batch size &gt; 1 in the case of a multidimensional design matrix\n\n\nUnpooled - MCMC\n\ndef unpooled_model(food_cat, customers, sales=None):\n\n    P = 3\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', len(np.unique(food_cat))):\n        sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n        beta = pyro.sample('beta', dist.Normal(10., 10.))\n    \n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma[food_cat]), obs=sales)\n\n\n# should be: beta -&gt; mu -&gt; y\npyro.render_model(\n    unpooled_model, (food_category, customers, sales),\n    render_distributions=True\n)\n\n\n\n\n\nkernel = NUTS(unpooled_model)\nmcmc_unpooled = MCMC(kernel, 500, 300)\nmcmc_unpooled.run(food_category, customers, sales)\n\nSample: 100%|██████████| 800/800 [00:04, 171.15it/s, step size=5.88e-01, acc. prob=0.889]\n\n\n\nmcmc_unpooled.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n   beta[0]     13.02      0.03     13.02     12.97     13.07    678.99      1.00\n   beta[1]      8.13      0.20      8.13      7.82      8.45    395.54      1.00\n   beta[2]      6.11      0.05      6.11      6.03      6.19    574.60      1.00\n  sigma[0]     40.09      1.39     39.99     37.95     42.38    778.36      1.00\n  sigma[1]     21.57      8.38     19.84     11.07     35.72    507.41      1.01\n  sigma[2]     36.13      2.58     36.10     32.26     40.33    560.15      1.00\n\nNumber of divergences: 0\n\n\n\nunpooled_posterior_samples = mcmc_unpooled.get_samples(1000)\nunpooled_predictive = \\\n    Predictive(unpooled_model, unpooled_posterior_samples)(food_category, customers, None)\n\naz_unpooled_inf = az.from_pyro(\n    posterior=mcmc_unpooled, posterior_predictive=unpooled_predictive)\n\n\naz.plot_trace(az_unpooled_inf, var_names=[\"beta\", \"sigma\"], compact=False)\nplt.tight_layout()\n\n\n\n\n\naz.plot_forest(az_unpooled_inf, var_names=['beta'])\nplt.show()\n\n\n\n\n\naz.plot_forest(az_unpooled_inf, var_names=['sigma'])\nplt.show()\n\n\n\n\n\n\nUnpooled - SVI\nNOT FINISHED\n\ndef unpooled_model(food_cat, customers, sales=None):\n\n    P = 3\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', len(np.unique(food_cat))):\n        sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n        beta = pyro.sample('beta', dist.Normal(10., 10.))\n    \n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma[food_cat]), obs=sales)\n\n\ndef unpooled_guide(food_cat, customers, sales=None): \n\n    with pyro.plate('food_cat_i', len(np.unique(food_cat))):\n        sigma_scale = pyro.param(\n            'sigma_scale', torch.tensor(1.), constraint=constraints.positive\n            )\n        sigma = pyro.sample('sigma', dist.HalfNormal(sigma_scale))\n\n        beta_loc = pyro.param('beta_loc', torch.tensor(10.))\n        beta_scale = pyro.param(\n            'beta_scale', torch.tensor(1.), constraint=constraints.positive)\n        beta = pyro.sample('beta', dist.Normal(beta_loc, beta_scale))\n\n\npyro.render_model(\n    unpooled_guide, (food_category, customers, sales), render_params=True)\n\n\n\n\n\nauto_guide = AutoLaplaceApproximation(unpooled_model)\n\n\npyro.clear_param_store()\n\nadam_params = {'lr': 0.002}\noptim = Adam(adam_params)\nsvi = SVI(unpooled_model, auto_guide, optim, Trace_ELBO())\n\niter = 1000\nelbo_loss = []\nfor i in range(iter):\n    loss = svi.step(food_category, customers, sales)\n    elbo_loss.append(loss)\n\nplt.figure(figsize=(10, 3))\nplt.plot(np.arange(1, iter+1), elbo_loss)\nplt.ylabel('ELBO Loss')\nplt.xlabel('Iterations')\nplt.title(f'iter {len(elbo_loss)}, loss: {elbo_loss[-1]:.4f}')\nplt.show()\n\n\n\n\n\nfor name, value in pyro.get_param_store().items():\n    print(name, pyro.param(name).data.cpu().numpy())\n\nAutoLaplaceApproximation.loc [ 4.113231   3.4546025  3.8238153 12.153096   8.615855   6.703431 ]\n\n\n\npredictive = Predictive(unpooled_model, guide=auto_guide, num_samples=1000)\nposterior_svi_samples = predictive(food_category, customers, None)\n\n\n\nPooled - MCMC\n\ndef pooled_model(customers, sales=None):\n\n    P = 3\n    N = len(customers)\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n    beta = pyro.sample('beta', dist.Normal(10., 10.))\n    \n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=sales)\n\n\npyro.render_model(\n    pooled_model, (customers, sales),\n    render_distributions=True\n)\n\n\n\n\n\nkernel = NUTS(pooled_model)\nmcmc_pooled = MCMC(kernel, 500, 300)\nmcmc_pooled.run(customers, sales)\n\nSample: 100%|██████████| 800/800 [00:02, 289.74it/s, step size=7.09e-01, acc. prob=0.931]\n\n\n\nmcmc_pooled.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n      beta     11.50      0.12     11.50     11.32     11.70    530.37      1.00\n     sigma    186.28      4.92    186.04    178.49    194.12    262.65      1.00\n\nNumber of divergences: 0\n\n\n\npooled_samples = mcmc_pooled.get_samples(1000)\npooled_predictive = Predictive(pooled_model, pooled_samples)(customers, None)\naz_pooled_inf = az.from_pyro(\n    posterior=mcmc_pooled, posterior_predictive=pooled_predictive)\n\n\nsales_mu = pooled_predictive['y'].mean(axis=0)\nsales_std = pooled_predictive['y'].std(axis=0)\n\npredictions = pd.DataFrame({\n    'customers': customers,\n    'category': food_category,\n    'sales': sales, \n    'sales_mu': sales_mu,\n    'sales_std': sales_std,\n    'sales_high': sales_mu + sales_std,\n    'sales_low': sales_mu - sales_std\n})\n\npredictions = predictions.sort_values(by=['customers'])\n\n\nsns.scatterplot(\n    x=predictions['customers'], y=predictions['sales'], \n    hue=predictions['category'], palette='tab10')\nsns.lineplot(\n    x=predictions['customers'], y=predictions['sales_mu'],\n    color='black')\nplt.fill_between(\n    x=predictions['customers'], \n    y1=predictions['sales_low'], \n    y2=predictions['sales_high'],\n    color='grey',\n    alpha=0.25)\nplt.title('Pooled Parameters - MCMC')\nplt.show()\n\n\n\n\n\n\nPooled - SVI\nUsing LaPlace Approximation doesn’t make the most sense since. . .\npooled_guide learns beta, but does not learn sigma. This could be related to parameter initialization\n\nsigma = dist.TransformedDistribution(\n        dist.Normal(torch.tensor(0.), 0.1 * torch.rand(1)), transforms=transforms.AbsTransform()\n    )\n\nsns.kdeplot(x=sigma.sample((1000,)).reshape(1, -1)[0]);\n\n\n\n\n\ndef pooled_model(customers, sales=None):\n\n    P = 3\n    N = len(customers)\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n    beta = pyro.sample('beta', dist.Normal(10., 10.))\n    \n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=sales)\n\n\ndef pooled_guide(customers, sales=None): \n\n    sigma_scale = pyro.param(\n        'sigma_scale', 0.1 * torch.rand(1), constraint=constraints.positive\n        )\n\n    sigma_loc = pyro.param(\n        'sigma_loc', torch.tensor(0.))\n    \n    sigma = pyro.sample('sigma', dist.TransformedDistribution(\n        dist.Normal(sigma_loc, sigma_scale), transforms=transforms.AbsTransform()\n    ))\n\n    beta_loc = pyro.param('beta_loc', torch.tensor(1.))\n    beta_scale = pyro.param(\n        'beta_scale', 0.1 * torch.rand(1), constraint=constraints.positive)\n    beta = pyro.sample('beta', dist.Normal(beta_loc, beta_scale))\n\n\npyro.render_model(\n    pooled_guide (customers, sales), render_params=True)\n\n\nauto_guide = pyro.infer.autoguide.AutoDiagonalNormal(pooled_model)\npyro.render_model(auto_guide, (customers, sales, ), render_params=True)\n\n\n\n\n\npyro.clear_param_store()\n\n#adam_params = {'lr': 0.005, 'betas': (0.95, 0.99)}\nadam_params = {'lr': 0.01}\noptim = Adam(adam_params)\nsvi = SVI(pooled_model, auto_guide, optim, Trace_ELBO())\n\niter = 2000\nelbo_loss = []\nfor i in range(iter):\n    loss = svi.step(customers, sales)\n    #loss = svi.step(food_category, customers, sales)\n    elbo_loss.append(loss)\n\nplt.plot(np.arange(1, iter+1), elbo_loss)\nplt.ylabel('ELBO Loss')\nplt.xlabel('Iterations')\nplt.title(f'iter {len(elbo_loss)}, loss: {elbo_loss[-1]:.4f}')\nplt.show()\n\n\n\n\n\npredictive = Predictive(pooled_model, guide=auto_guide, num_samples=1000)\nposterior_svi_samples = predictive(customers, None)\n\n\nsales_mu = posterior_svi_samples['y'].mean(axis=0)\nsales_std = posterior_svi_samples['y'].std(axis=0)\n\npredictions_svi = pd.DataFrame({\n    'customers': customers,\n    'category': food_category,\n    'sales': sales, \n    'sales_mu': sales_mu,\n    'sales_std': sales_std,\n    'sales_high': sales_mu + sales_std,\n    'sales_low': sales_mu - sales_std\n})\n\npredictions_svi = predictions_svi.sort_values(by=['customers'])\n\n\nsns.scatterplot(\n    x=predictions_svi['customers'], y=predictions_svi['sales'], \n    hue=predictions_svi['category'], palette='tab10')\nsns.lineplot(\n    x=predictions_svi['customers'], y=predictions_svi['sales_mu'],\n    color='black')\nplt.fill_between(\n    x=predictions_svi['customers'], \n    y1=predictions_svi['sales_low'], \n    y2=predictions_svi['sales_high'],\n    color='grey',\n    alpha=0.25)\nplt.title('Pooled Parameters - SVI')\nplt.show()\n\n\n\n\n\n\nMixing Group and Common Parameters - MCMC\n\ncustomers_z = (customers - customers.mean()) / (customers.std())\nsales_std = sales / sales.max()\n\n\ndef pooled_sigma_model(food_cat, customers, sales=None):\n\n    P = len(np.unique(food_cat))\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', P):\n        beta = pyro.sample('beta', dist.Normal(10., 20.))\n    \n    sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n\n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=sales)\n\n\npyro.render_model(\n    pooled_sigma_model, (food_category, customers, sales),\n    render_distributions=True\n)\n\n\n\n\n\nkernel = NUTS(pooled_sigma_model)\nmcmc_pooled_sigma = MCMC(kernel, 500, 300)\nmcmc_pooled_sigma.run(food_category, customers, sales)\n\nSample: 100%|██████████| 800/800 [00:03, 210.80it/s, step size=7.51e-01, acc. prob=0.906]\n\n\n\nmcmc_pooled_sigma.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n   beta[0]     13.02      0.03     13.02     12.97     13.06    673.42      1.00\n   beta[1]      8.15      0.33      8.14      7.62      8.72    579.90      1.00\n   beta[2]      6.11      0.06      6.11      6.02      6.21    396.32      1.00\n     sigma     39.19      1.24     39.21     36.88     40.96    545.10      1.00\n\nNumber of divergences: 0\n\n\n\npooled_sigma_samples = mcmc_pooled_sigma.get_samples(1000)\npooled_sigma_predictive = Predictive(pooled_sigma_model, pooled_sigma_samples)(food_category, customers, None)\naz_pooled_sigma_inf = az.from_pyro(\n    posterior=mcmc_pooled_sigma, posterior_predictive=pooled_sigma_predictive)\n\n\nsales_df['food_cat_encode'] = food_category\nsales_df['sales_std'] = sales_std\nsales_df['customers_z'] = customers_z\n\nfig, ax = plt.subplots()\n\nfor i in range(3):\n    category_mask = sales_df['food_cat_encode'] == i\n    mu_cat = pooled_sigma_predictive['y'][:, category_mask].mean(axis=0)\n\n    customers = sales_df.loc[category_mask, ['customers']].values.flatten()\n    sales = sales_df.loc[category_mask, ['sales']].values.flatten()\n\n    ax.plot(customers, mu_cat, c='black')\n    ax.scatter(customers, sales)\n    az.plot_hdi(\n        x=customers,\n        y=pooled_sigma_predictive['y'][:, category_mask],\n        color='grey'\n    )\n    \nax.set_xlabel('Customers')\nax.set_ylabel('Sales')\nax.set_title('Pooled Sigma');\n\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n\n\n\n\n\n\n\nMixing Group and Common Parameters - SVI\nUsing LaPlace approximation\n\ncustomers_z = (customers - customers.mean()) / (customers.std())\nsales_std = sales / sales.max()\n\n\ndef pooled_sigma_model(food_cat, customers, sales=None):\n\n    P = len(np.unique(food_cat))\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', P):\n        beta = pyro.sample('beta', dist.Normal(10., 20.))\n    \n    sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n\n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=sales)\n\n\nfrom torch.distributions import constraints, transforms\n\ndef pool_sigma_guide(food_cat, customers, sales=None):\n\n    P = len(np.unique(food_cat))\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', P):\n        \n        beta_scale = pyro.param(\n            'beta_scale', torch.tensor(1.), constraint=constraints.positive)\n        beta_loc = pyro.param('beta_loc', torch.randn(1))\n        beta = pyro.sample('beta', dist.Normal(beta_loc, beta_scale))\n    \n    sigma_loc = pyro.param(\n        'sigma_loc', torch.randn(1))\n    \n    sigma_scale = pyro.param(\n        'sigma_scale', 0.1 * torch.rand(1), constraint=constraints.positive)\n    \n    sigma = pyro.sample('sigma', dist.TransformedDistribution(\n        dist.Normal(sigma_loc, sigma_scale), transforms=transforms.ExpTransform()\n    ))\n    #sigma = pyro.sample('sigma', dist.HalfCauchy(sigma_scale))\n\n\npyro.render_model(\n    pool_sigma_guide, (food_category, customers, sales), render_params=True)\n\n\nauto_guide = AutoLaplaceApproximation(pooled_sigma_model)\n\n\ncustomers = torch.tensor(sales_df['customers'].values, dtype=torch.float64)\nsales = torch.tensor(sales_df['sales'].values, dtype=torch.float64)\nfood_category = torch.tensor(sales_df['Food_Category'].cat.codes.values, dtype=torch.long)\n\npyro.clear_param_store()\n\nadam_params = {'lr': 0.002}\noptim = Adam(adam_params)\nsvi = SVI(pooled_sigma_model, auto_guide, optim, Trace_ELBO())\n#svi = SVI(pooled_sigma_model, pool_sigma_guide, optim, Trace_ELBO())\n\niter = 2000\nelbo_loss = []\nfor i in range(iter):\n    loss = svi.step(food_category, customers, sales)\n    elbo_loss.append(loss)\n\nplt.plot(np.arange(1, iter+1), elbo_loss)\nplt.ylabel('ELBO Loss')\nplt.xlabel('Iterations')\nplt.title(f'iter {len(elbo_loss)}, loss: {elbo_loss[-1]:.4f}')\nplt.show()\n\n\n\n\n\npredictive = Predictive(pooled_sigma_model, guide=auto_guide, num_samples=1000)\nposterior_svi_samples = predictive(food_category, customers, None)\n\n\nsales_df['food_cat_encode'] = food_category\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor i in range(3):\n    category_mask = sales_df['food_cat_encode'] == i\n    mu_cat = posterior_svi_samples['y'][:, category_mask].mean(axis=0)\n\n    customers = sales_df.loc[category_mask, ['customers']].values.flatten()\n    sales = sales_df.loc[category_mask, ['sales']].values.flatten()\n\n    ax.plot(customers, mu_cat, c='black')\n    ax.scatter(customers, sales)\n    az.plot_hdi(\n        x=customers,\n        y=posterior_svi_samples['y'][:, category_mask],\n        color='grey'\n    )\n    \nax.set_xlabel('Customers')\nax.set_ylabel('Sales')\nax.set_title('Pooled Sigma')\n\n/Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n\n\nText(0.5, 1.0, 'Pooled Sigma')"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#hierarchical-models",
    "href": "posts/2022-07-22-bmcp-ch-4.html#hierarchical-models",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\nIn the multi-level model above, the \\(\\sigma\\) is assumed to be the same for all 3 categories. Instead, we can say that \\(\\sigma\\) comes from the same underlying distribution, but is allowed to vary by category.\n\ndef unpooled_model(food_cat, customers, sales=None):\n\n    P = 3\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', len(np.unique(food_cat))):\n        sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n        beta = pyro.sample('beta', dist.Normal(10., 10.))\n    \n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma[food_cat]), obs=sales)\n\n\ndef hierarchical_model(food_cat, customers, sales=None):\n\n    N = len(customers)\n    P = len(np.unique(food_cat))\n\n    sigma_hyperprior = pyro.sample('sigma_hyperprior', dist.HalfNormal(20.))\n\n    with pyro.plate('food_cat_i', size=P):\n        beta = pyro.sample('beta', dist.Normal(10., 20.))\n        sigma = pyro.sample('sigma', dist.HalfNormal(sigma_hyperprior))\n    \n    with pyro.plate('output', size=N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma[food_cat]), obs=sales)\n\n\npyro.render_model(\n    hierarchical_model, (food_category, customers, sales),\n    render_distributions=True\n)\n\n\n\n\n\nkernel = NUTS(hierarchical_model)\nmcmc_hm = MCMC(kernel, 500, 300)\nmcmc_hm.run(food_category, customers, sales)\n\nSample: 100%|██████████| 800/800 [00:07, 112.51it/s, step size=4.78e-01, acc. prob=0.868]\n\n\n\nmcmc_hm.summary()\n\n\n                        mean       std    median      5.0%     95.0%     n_eff     r_hat\n           beta[0]     13.02      0.03     13.02     12.97     13.07    392.00      1.00\n           beta[1]      8.13      0.28      8.13      7.76      8.59    419.29      1.00\n           beta[2]      6.12      0.05      6.11      6.04      6.20    581.77      1.00\n          sigma[0]     40.28      1.44     40.20     38.08     42.58    347.26      1.00\n          sigma[1]     26.57     12.56     23.87      8.74     44.99    323.47      1.00\n          sigma[2]     36.24      2.63     36.11     32.35     40.71    503.11      1.00\n  sigma_hyperprior     31.52      9.10     30.20     16.75     45.43    542.93      1.00\n\nNumber of divergences: 2\n\n\n\nPosterior Geometry Matters\n\ndef salad_generator(hyperprior_beta_mean=5, hyperprior_beta_sigma=.2, \nsigma=50, days_per_location=[6, 4, 15, 10, 3, 5], \nsigma_per_location=[50,10,20,80,30,20]):\n    \"\"\"Generate noisy salad data\"\"\"\n\n    beta_hyperprior = stats.norm(hyperprior_beta_mean, hyperprior_beta_sigma)\n    \n    # Generate demands days per restaurant\n    df = pd.DataFrame()\n    for i, days in enumerate(days_per_location):\n        np.random.seed(0)\n\n        num_customers = stats.randint(30, 100).rvs(days)\n        sales_location = beta_hyperprior.rvs()*num_customers + stats.norm(0, sigma_per_location[i]).rvs(num_customers.shape)\n\n        location_df = pd.DataFrame({\"customers\":num_customers, \"sales\":sales_location})\n        location_df[\"location\"] = i\n        location_df.sort_values(by=\"customers\", ascending=True)\n        df = pd.concat([df, location_df])\n        \n    df.reset_index(inplace=True, drop=True)\n    \n    return df\n\nhierarchical_salad_df = salad_generator()\n\n\nfig, axes, = plt.subplots(2,3, sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.ravel()):\n    location_filter = (hierarchical_salad_df[\"location\"] == i)\n    hierarchical_salad_df[location_filter].plot(\n        kind=\"scatter\", x=\"customers\", y=\"sales\", ax=ax)\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"\")\n\naxes[1, 0].set_xlabel(\"Number of Customers\")\naxes[1, 0].set_ylabel(\"Sales\");\n\n\n\n\n\ncustomers = torch.tensor(\n    hierarchical_salad_df['customers'].values, dtype=torch.float)\nsales = torch.tensor(\n    hierarchical_salad_df['sales'].values, dtype=torch.float)\nlocation = torch.tensor(\n    hierarchical_salad_df['location'].values, dtype=torch.long)\n\n\ndef hierarchical_salad_model(customers, location, sales=None):\n\n    N = len(customers)\n    P = len(np.unique(location))\n\n    beta_loc_hyper = pyro.sample('beta_loc_hyper', dist.Normal(0., 10.))\n    beta_scale_hyper = pyro.sample('beta_scale_hyper', dist.HalfNormal(.1))\n    sigma_hyper = pyro.sample('sigma_hyper', dist.HalfNormal(30.))\n\n    with pyro.plate('location_i', size=P):\n        beta = pyro.sample('beta', dist.Normal(beta_loc_hyper, beta_scale_hyper))\n        sigma = pyro.sample('sigma', dist.HalfNormal(sigma_hyper))\n    \n    with pyro.plate('output', size=N):\n        mu = pyro.deterministic('mu', beta[location] * sigma[location])\n        output = pyro.sample('y', dist.Normal(mu, sigma[location]), obs=sales)\n\n\npyro.render_model(\n    hierarchical_salad_model, (customers, location, sales),\n    render_distributions=True\n)\n\n\n\n\n\nkernel = NUTS(hierarchical_salad_model)\nmcmc_salad = MCMC(kernel, 500, 300)\nmcmc_salad.run(customers, location, sales)\n\nSample: 100%|██████████| 800/800 [00:51, 15.55it/s, step size=8.40e-02, acc. prob=0.714] \n\n\n\nmcmc_salad.summary()\n\n\n                        mean       std    median      5.0%     95.0%     n_eff     r_hat\n           beta[0]      3.97      0.44      4.04      3.27      4.66      8.21      1.37\n           beta[1]      3.97      0.43      4.06      3.29      4.66      8.02      1.38\n           beta[2]      3.95      0.43      4.02      3.34      4.73      7.88      1.38\n           beta[3]      3.94      0.43      4.02      3.29      4.67      7.87      1.38\n           beta[4]      3.97      0.43      4.05      3.35      4.75      7.70      1.38\n           beta[5]      3.97      0.44      4.04      3.35      4.72      7.98      1.39\n    beta_loc_hyper      3.96      0.43      4.04      3.31      4.64      7.56      1.40\n  beta_scale_hyper      0.08      0.06      0.07      0.02      0.16     16.84      1.07\n          sigma[0]     95.53     13.35     94.09     71.52    112.00     31.79      1.11\n          sigma[1]    110.74     16.58    112.76     82.07    129.72     37.98      1.01\n          sigma[2]     97.10     11.26     95.03     77.21    113.18     11.57      1.25\n          sigma[3]    100.95     11.85     98.67     82.71    119.33     13.40      1.20\n          sigma[4]    113.05     18.59    110.91     85.69    139.79     53.15      1.07\n          sigma[5]    104.10     17.04    102.16     81.35    127.71      7.66      1.32\n       sigma_hyper     76.34     13.05     74.29     57.97     98.81     68.83      1.08\n\nNumber of divergences: 77\n\n\n\nsalad_samples = mcmc_salad.get_samples(1000)\nsalad_predictive_samples = Predictive(\n    hierarchical_salad_model, salad_samples)(customers, location, None)\n\naz_salad_inf = az.from_pyro(\n    posterior=mcmc_salad, posterior_predictive=salad_predictive_samples)\n\n\nslope_centered = salad_samples['beta'][..., 4].numpy().flatten()\nsigma_centered = salad_samples['beta_scale_hyper'].numpy().flatten()\ndivergences_centered = np.array(mcmc_salad.diagnostics()['divergences']['chain 0'])\n\n\ndivergent_samples = pd.DataFrame({\n    'slope': slope_centered,\n    'sigma': sigma_centered\n})\n\nmask = divergent_samples.index.isin(pd.Index(divergences_centered))\ndivergent_samples['divergence'] = [1 if booly == True else 0 for booly in mask]\n\nax = sns.jointplot(\n    data=divergent_samples, x='slope', y='sigma', \n    color='grey', hue='divergence', palette=\"muted\")\nax.set_axis_labels(xlabel='$param: \\\\beta_m}$', ylabel='hyperprior: $\\\\beta_{\\sigma}$');\n\n\n\n\n\ndef non_centered_hierarchical_salad_model(customers, location, sales=None):\n\n    N = len(customers)\n    P = len(np.unique(location))\n\n    beta_loc_hyper = pyro.sample('beta_loc_hyper', dist.Normal(0., 10.))\n    beta_scale_hyper = pyro.sample('beta_scale_hyper', dist.HalfNormal(.1))\n    sigma_hyper = pyro.sample('sigma_hyper', dist.HalfNormal(30.))\n\n    with pyro.plate('location_i', size=P):\n        beta_offset = pyro.sample('beta_offset', dist.Normal(0., 1.))\n        #beta = pyro.sample('beta', dist.Normal(beta_loc_hyper, beta_scale_hyper))\n        sigma = pyro.sample('sigma', dist.HalfNormal(sigma_hyper))\n        beta = pyro.deterministic('beta', beta_offset * beta_scale_hyper + beta_loc_hyper) \n    \n    with pyro.plate('output', size=N):\n        mu = pyro.deterministic('mu', beta[location] * sigma[location])\n        output = pyro.sample('y', dist.Normal(mu, sigma[location]), obs=sales)\n\n\npyro.render_model(\n    non_centered_hierarchical_salad_model, \n    (customers, location, sales),\n    render_distributions=True\n)\n\n\n\n\n\nkernel = NUTS(non_centered_hierarchical_salad_model)\nmcmc_non_centered_salad = MCMC(kernel, 500, 300)\nmcmc_non_centered_salad.run(customers, location, sales)\n\nSample: 100%|██████████| 800/800 [00:21, 37.13it/s, step size=3.39e-01, acc. prob=0.892]\n\n\n\nmcmc_non_centered_salad.summary()\n\n\n                        mean       std    median      5.0%     95.0%     n_eff     r_hat\n    beta_loc_hyper      3.96      0.44      3.94      3.16      4.56    191.41      1.01\n    beta_offset[0]     -0.01      1.05      0.03     -1.73      1.88    655.40      1.00\n    beta_offset[1]      0.10      1.05      0.11     -1.50      1.79    433.58      1.00\n    beta_offset[2]     -0.01      0.96     -0.03     -1.53      1.56    710.84      1.00\n    beta_offset[3]     -0.06      1.06     -0.04     -1.61      1.74    758.38      1.00\n    beta_offset[4]      0.11      0.97      0.12     -1.34      1.88    590.92      1.00\n    beta_offset[5]      0.16      0.89      0.11     -1.24      1.64    503.59      1.00\n  beta_scale_hyper      0.08      0.06      0.07      0.00      0.16    422.26      1.00\n          sigma[0]     94.50     13.61     92.37     74.43    116.52    293.44      1.00\n          sigma[1]    105.06     17.08    102.42     76.38    131.42    306.25      1.00\n          sigma[2]     96.19     10.89     95.25     77.80    112.88    233.29      1.02\n          sigma[3]    101.39     12.69    100.16     81.71    121.43    212.00      1.02\n          sigma[4]    110.76     18.83    107.95     83.05    138.54    277.04      1.00\n          sigma[5]    106.44     15.11    104.96     82.36    130.42    272.45      1.00\n       sigma_hyper     76.73     13.90     75.53     54.77    100.05    467.62      1.00\n\nNumber of divergences: 0\n\n\n\nnon_centered_salad_samples = mcmc_non_centered_salad.get_samples(1000)\nsalad_predictive_samples = Predictive(\n    non_centered_hierarchical_salad_model, non_centered_salad_samples)(customers, location, None)\n\naz_salad_inf = az.from_pyro(\n    posterior=mcmc_non_centered_salad, posterior_predictive=salad_predictive_samples)\n\n\nslope_un_centered = salad_predictive_samples['beta'][..., 4].numpy().flatten() ## index 4th beta param\nsigma_un_centered = non_centered_salad_samples['beta_scale_hyper'].numpy().flatten()\ndivergences_un_centered = np.array(mcmc_non_centered_salad.diagnostics()['divergences']['chain 0'])\n\n\nnon_centered = pd.DataFrame({\n    'beta_b4': slope_un_centered,\n    'beta_sigma_hyper': sigma_un_centered,\n    'parameterization': 'non_centered'\n})\n\nnon_centered_mask = non_centered.index.isin(pd.Index(divergences_un_centered))\nnon_centered['divergence'] = [1 if booly == True else 0 for booly in non_centered_mask]\n\ncentered = pd.DataFrame({\n    'beta_b4': slope_centered,\n    'beta_sigma_hyper': sigma_centered,\n    'parameterization': 'centered'\n})\n\ncentered_mask = centered.index.isin(pd.Index(divergences_centered))\ncentered['divergence'] = [1 if booly == True else 0 for booly in centered_mask]\n\ndf = pd.concat([non_centered, centered])\n\n\ng = sns.FacetGrid(df, col='parameterization', hue='divergence', height=5)\ng.map_dataframe(sns.scatterplot, 'beta_b4', 'beta_sigma_hyper')\ng.set_axis_labels(xlabel='$\\\\beta_[4]$', ylabel='$\\\\beta_{\\sigma h}$')\ng.add_legend()\nplt.show()\n\n\n\n\n\nsns.kdeplot(df[df['parameterization'] == 'centered']['beta_sigma_hyper'])\nsns.kdeplot(df[df['parameterization'] == 'non_centered']['beta_sigma_hyper'])\nplt.legend(['centered', 'non centered'])\nplt.title('Difference in $\\\\beta_{sigma}$ hyperprior estimates');\n\n\n\n\n\n\nPredictions at Multiple Levels\nUsing the fitted parameter estimates to make an out of sample prediction for the distribution of sales for 50 customers. - posterior predictive: learned parameters; the data the model expects to see - customer = data \\(x\\)\n\n# .reshape(-1, 1) to ensure dimensions remain as [1000, 6] when multiplying\n# 6 b/c of 6 locations\nbeta = (\n    non_centered_salad_samples['beta_offset'] * \n    non_centered_salad_samples['beta_scale_hyper'].reshape(-1, 1) + \n    non_centered_salad_samples['beta_loc_hyper'].reshape(-1, 1)\n    )\n\nbeta.size()\n\ntorch.Size([1000, 6])\n\n\n\nbeta_group = dist.Normal(\n    non_centered_salad_samples['beta_loc_hyper'],\n    non_centered_salad_samples['beta_scale_hyper']\n    ).sample((100,))\n\n# aggregate predictions\ngroup_level_sales_prediction = dist.Normal(\n    beta_group * 50, \n    non_centered_salad_samples['sigma_hyper']\n    ).sample((100,))\n\n# location 2 and 4\nlocation_two = dist.Normal(\n    beta[:, 2] * 50,\n    non_centered_salad_samples['sigma'][:, 2]\n    ).sample((100,))\n\nlocation_four = dist.Normal(\n    beta[:, 4] * 50,\n    non_centered_salad_samples['sigma'][:, 4]\n    ).sample((100,))\n\n\nsns.kdeplot(group_level_sales_prediction.flatten(), clip=[0, 600])\nsns.kdeplot(location_two.flatten(), clip=[0, 600])\nsns.kdeplot(location_four.flatten(), clip=[0, 600])\nplt.legend(['All Locations', 'Location 2', 'Location 4'])\nplt.xlabel('Salad Sales')\nplt.title('Predicted Revenue with 50 Customers');"
  },
  {
    "objectID": "posts/2022-07-25-mosquito-walk.html",
    "href": "posts/2022-07-25-mosquito-walk.html",
    "title": "A Fragment of the Sphinx",
    "section": "",
    "text": "This code complements the figures and methods described in Chapter 4: A Fragment of the Sphinx of the book Shape: The Hidden Geometry of Everything by Jordan Ellenberg. In 1897, Sir Ronald Ross had discovered that maleria was carried by the bite of the anopheles mosquito. In the late summer of 1904, he gave a lecture in Missouri, United States titled “The Logical Basis of Sanitary Policy of Mosquito Reduction”. This lecture contained the first glimmer into a new geometric theory that would explode into physics, finance, and many other areas of science and engineering: the random walk.\nRoss proposed you eliminate propogation of mosquitos in a circular region by draining the pools where they breed. That doesn’t eliminate all potentially malarial mosquitoes from the region, because others may be born outside the circle and fly in. But, the mosquitoes’ life is brief and lacks ambition; it won’t set a course straight for the center and stick to it. So, some region around the center of the circle would hopefully be malaria free, as long as the circle is large enough. How large is large enough? That depends on how far a mosquito is likely to travel.\n“Suppose that a mosquito is born at a given point, and that during its life it wanders about, to or fro, to left or to right, where it wills…After a time it will die. What are the probabilities that its dead body will be found at a given distance from its birthplace?”"
  },
  {
    "objectID": "posts/2022-07-25-mosquito-walk.html#one-dimensional",
    "href": "posts/2022-07-25-mosquito-walk.html#one-dimensional",
    "title": "A Fragment of the Sphinx",
    "section": "One-Dimensional",
    "text": "One-Dimensional\nRoss was only able to handle the simple case where the mosquito is fixed to a straight line, choosing merely whether to flit northeast or southwest. To deal with the one-dimensional case on pg.66, we need a notion of distance and space. The outcome, defined as a choice to fly either northeast or eouthwest over the course of 10 days, can be described using a Binomial distribution parameterized by \\(n =\\) trials and \\(p =\\) probability of success. Setting \\(n=1\\) and \\(p=0.5\\) indicates an equal probability of observing \\(1\\) or \\(0\\) and is equivalent to a Beroulli distribution. For the one-dimensional case, \\(1\\) is encoded as moving \\(1\\) unit northeast and \\(0\\) as moving \\(-1\\) units southwest.\n\n\nCode\ndef plot_walk_1d(direction):\n\n    direction_df = pd.DataFrame.from_dict(\n    direction, orient='index', columns=['move'])\n    direction_df.index = direction_df.index.set_names(['step'])\n\n    shift = direction_df['move'].shift(periods=1, fill_value=0)\n    direction_df['running_tally'] = direction_df['move'].cumsum()\n    direction_df = direction_df.reset_index()\n\n    x_min, x_max = direction_df['running_tally'].min(), \\\n    direction_df['running_tally'].max()\n\n    fig = px.scatter(\n            direction_df, \n            x='running_tally', y=np.zeros(len(direction_df)), \n            animation_frame='step',\n            range_x=[-10, 10],\n            labels={\n                'y': 'Northeast - Southwest'\n            },\n            title='Mosquito Distance Traveled'\n            )\n\n    fig.update_yaxes(showgrid=False, \n                    zeroline=True, zerolinecolor='grey', zerolinewidth=1,\n                    showticklabels=False)\n    fig.update_layout(\n        xaxis = dict(\n            tickmode='linear'\n        )\n    )\n    \n    fig.show()\n\n\n\noutcome = np.random.binomial(1, 0.5, 20)\n\ndirection = {}\ndirection[0] = 0\ncnt = 0\nfor step in outcome:\n    cnt += 1\n    if step == 0:\n        direction[cnt] = 1\n    else:\n        direction[cnt] = -1\n\nplot_walk_1d(direction)"
  },
  {
    "objectID": "posts/2022-07-25-mosquito-walk.html#two-dimensional",
    "href": "posts/2022-07-25-mosquito-walk.html#two-dimensional",
    "title": "A Fragment of the Sphinx",
    "section": "Two-Dimensional",
    "text": "Two-Dimensional\nOn pg.66, Ellenberg shows a two-dimensional representation of a hypothetical mosquito flitting about, to or fro, to left or to right, where it wills. The code below achieves a similar simulation. However, instead of explicitly using a Binomial distribution, the np.random.choice() function is used and is parameterized by ([-1, 1], n_steps). This is equivalent to using a Bernoulli distribution using the same parameters as in the 1d simulation, with the exception of either \\(-1\\) or \\(1\\) being returned instead of \\(1\\) or \\(0\\).\nFor the notion of space, the x-y plane is used. North and south are encoded as the y-axis whereas west and east are encoded as the x-axis. By calling the np.random.choice() function twice, one for each unit step on the 2d coordinate plane, it can take the following directions: - north and east: \\((+, +)\\) - north and west: \\((+, -)\\) - south and west: \\((-, -)\\) - south and east: \\((-, +)\\)\n\n\nCode\ndef plot_2d_walk(df):\n\n    fig = go.Figure(\n    layout=go.Layout(\n        xaxis=dict(range=[-80, 80], autorange=False),\n        yaxis=dict(range=[-80, 80], autorange=False),\n        width=1000, height=650,\n        xaxis_title='Units (East or West)',\n        yaxis_title='Units (North or South)',\n        title=\"Mosquito Random Walk (2d)\",\n        updatemenus=[dict(\n            type=\"buttons\",\n            buttons=[dict(label=\"Play\",\n                            method=\"animate\",\n                            args=[None, {\n                                'frame': {'duration': 0.5}, \n                                'transition': {'duration': 0}\n                                }])])]\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=df.x[:1],\n            y=df.y[:1]\n        )\n    )\n\n    fig.update(frames=[\n        go.Frame(data=[go.Scatter(x=df.x[:k], y=df.y[:k])])\n            for k in range(1, len(df) + 1)\n        ]\n    )\n\n    fig.show()\n\n\n\ndef mosquito_random_walk_2d(n_steps):\n\n    x_steps = np.random.choice([-1, 1], n_steps)\n    y_steps = np.random.choice([-1, 1], n_steps)\n    x_pos, y_pos = np.cumsum(x_steps), np.cumsum(y_steps)\n\n\n    df = pd.DataFrame({\n        'step': np.arange(0, n_steps),\n        'x': x_pos,\n        'y': y_pos\n    })\n\n    plot_2d_walk(df)\n\n\nmosquito_random_walk_2d(n_steps=1500)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Google Summer of Code - Update 2\n\n\n\n\n\n\n\nopen-source\n\n\npython\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n  \n\n\n\n\nGoogle Summer of Code - Update 1\n\n\n\n\n\n\n\nopen-source\n\n\npython\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTranslating a Model into a Log Joint Probability\n\n\n\n\n\n\n\nprobability\n\n\ninference\n\n\njax\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nInference - Hamiltonian Monte Carlo from Scratch\n\n\n\n\n\n\n\nprobability\n\n\nsampling\n\n\ninference\n\n\noptimization\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\nInference - Gibbs Sampling from Scratch\n\n\n\n\n\n\n\nprobability\n\n\nsampling\n\n\ninference\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\nInference - Metropolis Hastings from Scratch\n\n\n\n\n\n\n\nprobability\n\n\nsampling\n\n\ninference\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nInference - Monte Carlo Approximation\n\n\n\n\n\n\n\nprobability\n\n\nsampling\n\n\ninference\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nA Fragment of the Sphinx\n\n\n\n\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Modeling and Computation in Pyro - Chapter 5\n\n\n\n\n\n\n\nbayesian-statistics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Modeling and Computation in Pyro - Chapter 4\n\n\n\n\n\n\n\nbayesian-statistics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Modeling and Computation in Pyro - Chapter 3\n\n\n\n\n\n\n\nbayesian-statistics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\nProbabilistic Prediction Problems - Part 2\n\n\n\n\n\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVariational Inference - ELBO\n\n\n\n\n\n\n\nprobability\n\n\noptimization\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nProbabilistic Prediction Problems - Part 1\n\n\n\n\n\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTextbooks Have Gotten Good, Like Really Good\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\nNo Code, Dependency, and Building Technology\n\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\nEconomics of Open Source\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A home for recent thoughts and projects regarding probabilistic programming, microeconomics, and technology."
  }
]